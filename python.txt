to see whether command line editing is supported is typing "Control-P" to the first Python prompt you get. If it beeps, you have command line editing

python -c command [arg] ...


python -m module [arg] ...


to run the script and enter interactive mode afterwards. This can be done by passing "-i" before the script.

sys.argv[0] ----
When known to the interpreter, the script name and additional arguments thereafter are turned into a list of strings and assigned to the argv variable in the sys module. You can access this list by executing import sys. The length of the list is at least one; when no script and no arguments are given, sys.argv[0] is an empty string. When the script name is given as '-' (meaning standard input), sys.argv[0] is set to '-'. When -c command is used, sys.argv[0] is set to '-c'.

to have some standard commands executed every time the interpreter is started:
setting an environment variable named "PYTHONSTARTUP" to the name of a file containing your start-up commands.
You can also change the prompts sys.ps1 and sys.ps2 in this file.
This file is only read in interactive sessions.

If you want to read an additional start-up file from the current directory, you can program this in the global start-up file using code like "if os.path.isfile('.pythonrc.py'): execfile('.pythonrc.py')". If you want to use the startup file in a script, you must do this explicitly in the script:

import os
filename = os.environ.get('PYTHONSTARTUP')
if filename and os.path.isfile(filename):
    execfile(filename)


Python provides two hooks to let you customize it: "sitecustomize and usercustomize". To see how it works, you need first to find the location of your user site-packages directory. Start Python and run this code:

>>>
>>> import site
>>> site.getusersitepackages()
'/home/user/.local/lib/python3.2/site-packages'
Now you can create a file named "usercustomize.py" in that directory and put anything you want in it. It will affect every invocation of Python, unless it is started with the -s option to disable the automatic import.


raw string -
If we make the string literal a "raw" string, \n sequences are not converted to newlines, but the backslash at the end of the line, and the newline character in the source, are both included in the string as data. Thus, the example:

hello = r"This is a rather long string containing\n\  # check the r in front----
several lines of text much as you would do in C."

print hello
would print:

This is a rather long string containing\n\
several lines of text much as you would do in C.

Strings can be concatenated (glued together) with the + operator, and repeated with *: -

>>> word = 'Help' + 'A'
>>> word
'HelpA'
>>> '<' + word*5 + '>'
'<HelpAHelpAHelpAHelpAHelpA>'

>>> 'str' 'ing'                   #  <-  This is ok
'string'
>>> 'str'.strip() + 'ing'   #  <-  This is ok
'string'
>>> 'str'.strip() 'ing'     #  <-  This is invalid
  File "<stdin>", line 1, in ?
    'str'.strip() 'ing'
                      ^
SyntaxError: invalid syntax

>>> word[4]
'A'
>>> word[0:2]
'He'
>>> word[2:4]
'lp'
>>> word[:2]    # The first two characters
'He'
>>> word[2:]    # Everything except the first two characters
'lpA'

>>> 'x' + word[1:]
'xelpA'
>>> 'Splat' + word[4]
'SplatA'

Python strings cannot be changed. Assigning to an indexed position in the string results in an error:

>>>
>>> word[0] = 'x'
Traceback (most recent call last):
  File "<stdin>", line 1, in ?
TypeError: object does not support item assignment
>>> word[:1] = 'Splat'
Traceback (most recent call last):
  File "<stdin>", line 1, in ?
TypeError: object does not support slice assignment

>>> word[:2] + word[2:]
'HelpA'
>>> word[:3] + word[3:]
'HelpA'

>>> word[1:100]
'elpA'
>>> word[10:]
''
>>> word[2:1]    # upper bound smaller than lower bound ----
''

>>> word[-1]     # The last character
'A'
>>> word[-2]     # The last-but-one character
'p'
>>> word[-2:]    # The last two characters
'pA'
>>> word[:-2]    # Everything except the last two characters
'Hel'
>>> word[-0]     # (since -0 equals 0)
'H'
+---+---+---+---+---+
 | H | e | l | p | A |
 +---+---+---+---+---+
 0   1   2   3   4   5
-5  -4  -3  -2  -1

>>> s = 'supercalifragilisticexpialidocious'
>>> len(s)
34

Lists -
>>> a = ['spam', 'eggs', 100, 1234]
>>> a
['spam', 'eggs', 100, 1234]
>>> a[0]
'spam'
>>> a[3]
1234
>>> a[-2]
100
>>> a[1:-1]
['eggs', 100]
>>> a[:2] + ['bacon', 2*2]
['spam', 'eggs', 'bacon', 4]
>>> 3*a[:3] + ['Boo!']
['spam', 'eggs', 100, 'spam', 'eggs', 100, 'spam', 'eggs', 100, 'Boo!']
>>> a
['spam', 'eggs', 100, 1234]
>>> a[2] = a[2] + 23
>>> a
['spam', 'eggs', 123, 1234]

>>> shoplist = ['apple', 'mango', 'carrot', 'banana']
>>> shoplist[::-1]   # negative step go backward
['banana', 'carrot', 'mango', 'apple']

"str.isdigit(s)": To check if a string is digital
str.isdigit(s[4])
################################# -
Django

To isntall Django:
pip install Django

Testing the Django installation --
>>> import django
>>> django.VERSION
(1, 4, 2, 'final', 0)

mysite/:  ---
The outer mysite/ directory is just a container for your project. Its name doesn.t matter to Django; you can rename it to anything you like.
manage.py:  ---
A command-line utility that lets you interact with this Django project in various ways. Type python manage.py help to get a feel for what it can do. You should never have to edit this file; it.s created in this directory purely for convenience.
mysite/mysite/:  ---
The inner mysite/ directory is the actual Python package for your project. Its name is the Python package name you.ll need to use to import anything inside it (e.g. import mysite.settings).
__init__.py: ---
A file required for Python to treat the mysite directory as a package (i.e., a group of Python modules). It.s an empty file, and generally you won.t add anything to it.
settings.py: ---
Settings/configuration for this Django project. Take a look at it to get an idea of the types of settings available, along with their default values.
urls.py: ---
The URLs for this Django project. Think of this as the .table of contents. of your Django-powered site.
wsgi.py: ---
An entry-point for WSGI-compatible webservers to serve your project. See How to deploy with WSGI (https://docs.djangoproject.com/en/1.4/howto/deployment/wsgi/) for more details.

To start a project --
django-admin.py startproject mysite

To start Django server --
python manage.py runserver

python manage.py runserver 8080

To listen on every IP address
python manage.py runserver 0.0.0.0:8000


into the views.py file: -

from django.http import HttpResponse

def hello(request):
    return HttpResponse("Hello world")


urls.py --
from django.conf.urls.defaults import patterns, include, url
from mysite.views import hello # first, import the function from the views.py 

urlpatterns = patterns('',
    url(r'^$', my_homepage_view), # match the root "/" directory
    url(r'^hello/$', hello),
    url(r'^time/plus/\d+/$', hours_ahead), # to be any number
    url(r'^time/plus/\d{1,2}/$', hours_ahead), # to be maxium 2 digits
    url(r'^time/plus/(\d{1,2})/$', hours_ahead) # () is to capture the input from the request 
)

we''re using parentheses to capture data from the matched text. Captured values will always be Unicode objects, not plain Python bytestrings,
"r" character in front of the regular expression string. This tells Python that the string is a .raw string. . its contents should not interpret backslashes.

In urls.py, you specify the url for a function, in views.py you define the function to respond to a url.

The views.py
from django.http import HttpResponse
import datetime
def Hello(request):
    return HttpResponse('Hello world!! :)')

def current_datetime(request):
    now = datetime.datetime.now()
    html = "<html><body>It is now <b>%s</b>.</body></html>" % now
    #return HttpResponse(datetime.datetime.now())
    return HttpResponse(html)

def hour_ahead(request, offset) :  # the offset is catched with (\d{1,2}) in urls.py
    try:
        offset = int(offset)
    except ValueError():
        raise Http404()
    dt = datetime.datetime.now() + datetime.timedelta(hours = offset)
    html = "<html><body>In %s hour(s), it will be %s.</body></html>" % (offset, dt)
    return HttpResponse(html)


Your Python Path -

let.s say your Python path is set to 
['', '/usr/lib/python2.7/site-packages', '/home/username/djcode']
An empty string, means .the current directory..

To check current python path:
>>> import sys
>>> print sys.path

Django removes the slash from the front of every incoming URL before it checks the URLpatterns.
By default, any request to a URL that doesn''t match a URLpattern and doesn''t end with a slash will be redirected to the same URL with a trailing slash.  (This is regulated by the APPEND_SLASH Django setting, which is covered in Appendix D.)
.................................
When Django project is in debug mode, the 404 page not found page give more details about url checking.

To get debug info page:
assert False
.................................
django processes request by first checking settings.py.
ROOT_URLCONF = 'mysite.urls' #This corresponds to the file mysite/urls.py.

In summary:

1. A request comes in to /hello/.
2. Django determines the root URLconf by looking at the ROOT_URLCONF setting.
3. Django looks at all of the URLpatterns in the URLconf for the first one that matches /hello/.
4. If it finds a match, it calls the associated view function with HttpRequest.
5. The view function returns an HttpResponse.
6. Django converts the HttpResponse to the proper HTTP response, which results in a Web page.

In settings.py default is: America/Chicago
Change to:
TIME_ZONE = 'America/New_York'

.................................
django template
1. Any text surrounded by a pair of braces (e.g., {{ person_name }}) is a variable. 
2. Any text that.s surrounded by curly braces and percent signs (e.g., {% if ordered_warranty %}) is a template tag.  The definition of a tag is quite broad: a tag just tells the template system to .do something..
3. Finally, the second paragraph of this template contains an example of a filter, which is the most convenient way to alter the formatting of a variable.  In this example, {{ ship_date|date:"F j, Y" }}, we.re passing the ship_date variable to the date filter, giving the date filter the argument "F j, Y".

To run a python shell with environemnt loaded:
python manage.py shell

manage.py shell command has one key difference: before starting the interpreter, it tells Django which settings file to use. 

>>> from django import template
>>> t = template.Template('My name is {{ name }}.') # template fills with Context
>>> c = template.Context({'name': 'Adrian'})
>>> print t.render(c)
My name is Adrian.
>>> t.render(c)
u'My name is Adrian.'

Once you have a "Template" object, you can pass it data by giving it a context.  A context is simply a set of template variable names and their associated values. A template uses this to populate its variables and evaluate its tags.

A "context" is represented in Django by the Context class, which lives in the django.template module. Its constructor takes one optional argument: a dictionary mapping variable names to variable values. Call the Template object.s render() method with the context to .fill. the template:

the return value of t.render(c) is a "Unicode object" . not a normal Python string.
......................
>>> from django.template import Template, Context
>>> t = Template('{{ var }} -- {{ var.upper }} -- {{ var.isdigit }}') # to check if it is digit
>>> t.render(Context({'var': 'hello'}))
u'hello -- HELLO -- False'
>>> t.render(Context({'var': '123'}))
u'123 -- 123 -- True'

Note that you do not include parentheses in the method calls. Also, it.s not possible to pass arguments to the methods; you can only call methods that have no required arguments. (We explain this philosophy later in this chapter.) 
Finally, dots are also used to access list indices, for example:

>>> from django.template import Template, Context
>>> t = Template('Item 2 is {{ items.2 }}.')
>>> c = Context({'items': ['apples', 'bananas', 'carrots']})
>>> t.render(c)
u'Item 2 is carrots.'

Dot lookups can be summarized like this: when the template system encounters a dot in a variable name, it tries the following lookups, in this order:

Dictionary lookup (e.g., foo["bar"])
Attribute lookup (e.g., foo.bar)
Method call (e.g., foo.bar())
List-index lookup (e.g., foo[2])
.................................
To prevent this, set the function attribute alters_data on the method:

def delete(self):
    # Delete the account
delete.alters_data = True
The template system won.t execute any method marked in this way.


Multiple uses of the same logical operator are fine, but you can.t combine different operators. For example, this is valid:

{% if athlete_list or coach_list or parent_list or teacher_list %}
Make sure to close each {% if %} with an {% endif %}.

For template:
for X in Y
{% for athlete in athlete_list reversed %} # the reversed list
...
{% endfor %}

{% if athlete_list %} #-- checking to make sure the list is not empty
    {% for athlete in athlete_list %}
        <p>{{ athlete.name }}</p>
    {% endfor %}
{% else %}
    <p>There are no athletes. Only computer programmers.</p>
{% endif %}

{% for athlete in athlete_list %} #-- it is the same as:
    <p>{{ athlete.name }}</p>
{% empty %} #-- when the list is empty
    <p>There are no athletes. Only computer programmers.</p>
{% endfor %}

"forloop.counter" is always set to an integer representing the number of times the loop has been entered. This is one-indexed, so the first time through the loop, forloop.counter will be set to 1. Here.s an example:

{% for item in todo_list %}
    <p>{{ forloop.counter }}: {{ item }}</p>
{% endfor %}

"forloop.revcounter" is always set to an integer representing the number of remaining items in the loop. 
"forloop.first" is a Boolean value set to True if this is the first time through the loop.
"forloop.last" is a Boolean value set to True if this is the last time through the loop. 
Favorite places:
{% for p in places %}{{ p }}{% if not forloop.last %}, {% endif %}{% endfor %}

"forloop.parentloop" is a reference to the forloop object for the parent loop, in case of nested loops. Here.s an example:

{% for country in countries %}
    <table>
    {% for city in country.city_list %}
        <tr>
        <td>Country #{{ forloop.parentloop.counter }}</td>
        <td>City #{{ forloop.counter }}</td>
        <td>{{ city }}</td>
        </tr>
    {% endfor %}
    </table>
{% endfor %}
.................................
The {% ifequal %} tag compares two values and displays everything between {% ifequal %} and {% endifequal %} if the values are equal.
{% ifequal section 'sitenews' %}
    <h1>Site News</h1>
{% else %}
    <h1>No News Here</h1>
{% endifequal %}

Django comment:
{# This is a comment #}

Use multi-line comments, use the {% comment %} template tag, like this:
{% comment %}
This is a
multi-line comment.
{% endcomment %}
......................
Filters

Takes the first element in a list and converts it to uppercase:
{{ my_list|first|upper }}

{{ bio|truncatewords:"30" }}
This displays the first 30 words of the bio variable.

"addslashes": Adds a backslash before any backslash, single quote, or double quote. This is useful if the produced text is included in a JavaScript string.

"date": Formats a date or datetime object according to a format string given in the parameter, for example:
{{ pub_date|date:"F j, Y" }}

"length": Returns the length of the value.
.................................
Template Loading
define TEMPLATE_DIRS in settings.py.
TEMPLATE_DIRS = (
    '/home/django/mysite/templates',  # need the , at the end
)
# or
import os.path
TEMPLATE_DIRS = (
    os.path.join(os.path.dirname(__file__), 'templates').replace('\\','/'),
)
This example uses the .magic. Python variable __file__, which is automatically set to the file name of the Python module in which the code lives. It gets the name of the directory that contains settings.py (os.path.dirname)

views.py:
from django.template.loader import get_template
from django.template import Context
from django.http import HttpResponse
import datetime

def current_datetime(request):
    now = datetime.datetime.now()
    t = get_template('current_datetime.html')
    html = t.render(Context({'current_date': now}))
    return HttpResponse(html)

[root@vmstlsup05 templates]# cat date_template.html
<html><body>It is now <b>{{current_date}}</b>.</body></html>
.................................
render() : request, template_file, and input for Context
# the same code as the above
from django.shortcuts import render
import datetime

def current_datetime(request):
    now = datetime.datetime.now()
    return render(request, 'current_datetime.html', {'current_date': now})
.................................
Subdirectories in get_template()
t = get_template('dateapp/current_datetime.html')
#or
return render(request, 'dateapp/current_datetime.html', {'current_date': now})
.................................
The "include" Template Tag
{% include %}. This tag allows you to include the contents of another template.
The argument to the tag should be the name of the template to include, and the template name can be either a variable or a hard-coded (quoted) string
{% include 'nav.html' %}
{% include template_name %}
.................................
Template Inheritance
In essence, template inheritance lets you build a base .skeleton. template that contains all the common parts of your site and defines .blocks. that child templates can override.

block tag can have any name for example, {% block sidebar %}, but has to be unique.

If you use {% extends %} in a template, it must be the first template tag in that template. Otherwise, template inheritance won.t work.
The argument to {% extends %} can be variable.

If you need to get the content of the block from the parent template, use "{{ block.super }}", which is a .magic. variable providing the rendered text of the parent template. This is useful if you want to add to the contents of a parent block instead of completely overriding it.

Now, the template and views become:
[root@vmstlsup05 templates]# cat base.html
<!DOCTYPE HTML PULBIC "-//W3C//DTD HTML 4.01//EN"
<html lang="en">
<head>
    <title>{% block title %}{% endblock %} </title>
</head>
<body>
    <h1>My helpfule timestamp site</h1>
    {% block content%}{% endblock %}
    {% block footer %}
    <hr>
    <p>Thanks for visiting my site.</p>
    {% endblock %}
</body>
</html>

[root@vmstlsup05 templates]# cat hello.html
{% extends "base.html"%}
{% block content %}Your message: {{hello_message}}{%endblock%}

[root@vmstlsup05 templates]# cat date_template.html
{% extends "base.html" %}
{% block title %}The current time{% endblock%}
{% block content %}
<p>It is now <b>{{current_date}}</b>.</p>
{% endblock %}

[root@vmstlsup05 templates]# cat hour_ahead.html
{% extends "base.html" %}
{% block title %}The expected time{% endblock%}
{% block content %}
<p>In {{offset}} hours, will be <b>{{dt}}</b>.</p>
{% endblock %}

#views.py
from django.shortcuts import render
import datetime
def Hello(request):
    return render(request,'hello.html',{'hello_message': 'Hello Hello :)'})

def current_datetime(request):
    now = datetime.datetime.now()
    return render(request,'date_template.html',{'current_date': now})

def hour_ahead(request, offset) :
    try:
        offset = int(offset)
    except ValueError():
        raise Http404()
    dt = datetime.datetime.now() + datetime.timedelta(hours = offset)
    #assert False
    return render(request,'hour_ahead.html',{'offset':offset,'dt':dt})

.................................
Model - # can't use model, auth, admin and session  with mongodb... :(
If you.re using Django.s database layer (models), you must create a Django app. Models must live within apps.

To create a new APP.

python manage.py startapp books
It will create a directory:
books/
    __init__.py
    models.py
    tests.py
    views.py
.................................
Form
"HttpRequest" objects contain several pieces of information about the currently requested URL:

Attribute/method	Description							Example
"request.path"		The full path, not including the domain but including the leading slash.	"/hello/"
"request.get_host()"	The host (i.e., the .domain,. in common parlance).  		"127.0.0.1:8000" or "www.example.com"
request.get_full_path()	The path, plus a query string (if available).  			"/hello/?print=true"
"request.is_secure()"	True if the request was made via HTTPS. Otherwise, False.	True or False

def current_url_view_good(request):
    return HttpResponse("Welcome to the page at %s" % request.path)

"request.META" is a Python dictionary containing all available HTTP headers for the given request . including the user.s IP address and user agent (generally the name and version of the Web browser). Note that the full list of available headers depends on which headers the user sent and which headers your Web server sets. Some commonly available keys in this dictionary are:

"HTTP_REFERER" . The referring URL, if any. (Note the misspelling of REFERER.)

"HTTP_USER_AGENT" . The user.s browser.s user-agent string, if any. This looks something like: "Mozilla/5.0 (X11; U; Linux i686; fr-FR; rv:1.8.1.17) Gecko/20080829 Firefox/2.0.0.17".

"REMOTE_ADDR" . The IP address of the client, e.g., "12.345.67.89". (If the request has passed through any proxies, then this might be a comma-separated list of IP addresses, e.g., "12.345.67.89,23.456.78.90".) 

example:

def display_metadata(request):
    #for i in request.META.items()
    values = request.META.items()
    values.sort()
    html = []
    for k, v in values:
        html.append('<tr><td>%s</td><td>%s</td></tr>' % (k, v))
    return HttpResponse('<table>%s</table>' % '\n'.join(html))

def template_metadata(request):
    return render(request,'metadata.html', {'items':request.META.items()})

[root@vmstlsup05 templates]# cat metadata.html
<!DOCTYPE HTML PULBIC "-//W3C//DTD HTML 4.01//EN"
<html lang="en">
<head>
    <title>{% block title %}{% endblock %} </title>
</head>
<body>
    <h1>Here is your information</h1>
    {% if items %} #-- checking to make sure the list is not empty
        {% for it in items %}
            <p>{{ it }}</p>
        {% endfor %}
    {% else %}
        <p>There are no info.</p>
    {% endif %}
    {% block footer %}
    <hr>
    <p>Thanks for visiting my site.</p>
    {% endblock %}
</body>
</html>

.................................
django file upload

Basic file uploads
Consider a simple form containing a FileField:

# In forms.py...
from django import forms

class UploadFileForm(forms.Form):
    title = forms.CharField(max_length=50)
    file  = forms.FileField()

A view handling this form will receive the file data in request.FILES, which is a dictionary containing a key for each FileField (or ImageField, or other FileField subclass) in the form. So the data from the above form would be accessible as request.FILES['file'].

Note that request.FILES will only contain data if the request method was POST and the <form> that posted the request has the attribute enctype="multipart/form-data". Otherwise, request.FILES will be empty.


curl --form upload=@localfilename --form press=OK [URL]

web client:
[root@vmstlsup05 python_code]# cat webclient.py
#!/usr/bin/python
import httplib
import sys
print sys.argv[1]
conn = httplib.HTTPConnection("192.168.115.51:8000")
conn.request("GET", '/' + sys.argv[1] + '/')
r1 = conn.getresponse()
print r1.status, r1.reason
.................................
Browser-Length Sessions vs. Persistent Sessions

You might have noticed that the cookie Google sent us at the beginning of this chapter contained expires=Sun, 17-Jan-2038 19:14:07 GMT;. Cookies can optionally contain an expiration date that advises the browser on when to remove the cookie. If a cookie doesn.t contain an expiration value, the browser will expire it when the user closes his or her browser window. You can control the session framework.s behavior in this regard with the "SESSION_EXPIRE_AT_BROWSER_CLOSE" setting.

By default, "SESSION_EXPIRE_AT_BROWSER_CLOSE" is set to False, which means session cookies will be stored in users. browsers for "SESSION_COOKIE_AGE" seconds (which defaults to two weeks, or 1,209,600 seconds). Use this if you
don.t want people to have to log in every time they open a browser.

If "SESSION_EXPIRE_AT_BROWSER_CLOSE" is set to True, Django will use browser-length cookies.
.................................
To install Flask
1. load virutalenv
. ./bin/activate
pip install Flask

to avoid this error:
    markupsafe/_speedups.c:12:20: error: Python.h: No such file or directory

Need to install this:
yum install python-devel

apt-get install python-dev #for ubuntu

To install from source code:
unpack
./configure
./make
./make install

from flask import Flask
app = Flask(__name__)
@app.route('/')
def hello_world():
    return 'Hello World!'
@app.route(r'/time/') # the trailing / allow redirect /time to /time/
def time():           # if defined as /time, visiting /time/ will be "page not found"  
    n = str(datetime.datetime.now())
    return  'now is %s' % n
@app.route('/user/<username>/') 
def show_user_profile(username):
    # show the user profile for that user
    return 'User %s' % username
@app.route('/post/<int:post_id>') : use variable as url, with converter int
def show_post(post_id):
    # show the post with the given id, the id is an integer
    return 'Post %d' % post_id
if __name__ == '__main__':
    app.debug = True  # auto detect changes in the script, no need to restart
    app.run('0.0.0.0',6000)
#or app.run(debug=True)

The following converters exist:

int	accepts integers
float	like int but for floating point values
path	like the default but also accepts slashes

To build a URL to a specific function you can use the "url_for()" function. It accepts the name of the function as first argument and a number of keyword arguments, each corresponding to the variable part of the URL rule.

url_for('static', filename='style.css')
.................................
File upload
from flask import request
from werkzeug import secure_filename
@app.route('/upload', methods=['GET', 'POST'])
def upload_file():
    if request.method == 'POST':
        f = request.files['the_file']
        f.save('/var/www/uploads/' + secure_filename(f.filename)) # using the name from the client side.
@app.route('/SDC/msg/', methods=['GET', 'POST'])
def receive_msg():
    if request.method == 'POST':
        print request.data
    return 'msg received!'
import requests
import sys
from flask import Response
@app.route('/spring/', methods=['GET', 'POST'])
def springboard():
    if request.method == 'POST':
        f = request.files['the_file']
        #f.save('/dev/shm/' + secure_filename(f.filename))
        url = 'http://192.168.115.51:6000/upload/' #to the target SD server
        fname=secure_filename(f.filename)
        files = {'the_file1': (fname, f)}
        r = requests.post(url, files=files)
        resp = Response("seems good!") # to build a respons obj
        resp.headers['SDGW'] = 'the hello server' # to add a field in header
        return resp

#The client side, even sending binary file.
curl -i -F the_file=@mbox http://192.168.115.51:5000/upload/ 1>&2 >/windata/tmp/ttt.html
curl -i -F the_file=@2013-07-18_2053_supprt2_bkp.tar.gz http://192.168.115.51:5000/upload/ 1>&2 >/windata/tmp/ttt.html
#To send JSON message
 curl -X POST -H "Content-Type: application/json" -d '[{"username":"xyz","password":"xyz"}]' http://192.168.114.174/SDC/msg/


#To do the same with a python webclient
import requests
import sys
import json
def main() :
    print sys.argv[1]
    url = 'http://192.168.115.51:5000/upload/'
    files = {'the_file': open(sys.argv[1], 'rb')}
    r = requests.post(url, files=files)
    #to get the request and respons header info
    # or
    #header = {'User-Agent': 's agent','C code':'this is OK'}
    #r = requests.post(url, files=files, headers=header)
    print "response header: ", r.headers
    print "request header ", r.request.headers['User-Agent']
if __name__ == '__main__':
        main()

import smtplib
def sendemail(TEXT):
    FROM ='root'
    TO = ["zwa@orsyp.com", "bta@orsyp.com"] # this is List of Email Ids
    SUBJECT='Warning message from SD system'
    #TEXT = "This message was sent with Python's smtplib. ..."
    print 'TEXT',TEXT
    TEXT= " " + TEXT + " "
    server=smtplib.SMTP('127.0.0.1',25)
    server.sendmail(FROM,TO, TEXT)
    server.quit()

.................................
To redirect a user to another endpoint, use the "redirect()" function; to abort a request early with an error code, use the "abort()" function:

from flask import abort, redirect, url_for

@app.route('/')
def index():
    return redirect(url_for('login'))

@app.route('/login')
def login():
    abort(401)
    this_is_never_executed()
.................................

# all the imports
import sqlite3
from flask import Flask, request, session, g, redirect, url_for, \
     abort, render_template, flash

# configuration
DATABASE = '/tmp/flaskr.db'
DEBUG = True                   # Turn off for prodction system
SECRET_KEY = 'development key' #The secret_key is needed to keep the client-side sessions secure.
			       #import os; os.urandom(24)
USERNAME = 'admin'
PASSWORD = 'default'

"Session": Flask uses a cookie signed with the SECRET_KEY. To access the current session you can use the session object:
"permanent":
If set to "True" the session lives for "permanent_session_lifetime" seconds. The default is 31 days. If set to "False" (which is the default) the session will be deleted when the user closes the browser.  

"permanent_session_lifetime"
A "timedelta" which is used to set the expiration date of a permanent session.  The default is 31 days which makes a permanent session survive for roughly one month.
This attribute can also be configured from the config with the "PERMANENT_SESSION_LIFETIME" configuration key. Defaults to timedelta(days=31)

"timedelta": A python object represents a duration, the difference between two dates or times.

from datetime import timedelta
from flask import session, app
session.permanent = True
app.permanent_session_lifetime = timedelta(minutes=5)

# create our little application :)
app = Flask(__name__)
app.config.from_object(__name__) # to load those upper case varibles above

"from_object()" will look at the given object (if it.s a string it will import it) and then look for all uppercase variables defined there. In our case, the configuration we just wrote a few lines of code above. You can also move that into a separate file.
Usually, it is a good idea to load a configuration from a configurable file.  This is what "from_envvar()" can do, replacing the from_object() line above:
#app.config.from_envvar('FLASKR_SETTINGS', silent=True)
an environment variable called "FLASKR_SETTINGS" to specify a config file to be loaded which will then override the default values. The "switch" just tells Flask to not complain if no such environment key is set.
 
def connect_db():
    return sqlite3.connect(app.config['DATABASE'])

if __name__ == '__main__':
    app.run()
.................................
To create the sqlite3 database
sqlite3 /tmp/flaskr.db < schema.sql

[zwa@vmstlsup05 flaskr]$ cat schema.sql
drop table if exists entries;
create table entries (
  id integer primary key autoincrement,
  title text not null,
  text text not null
);

#Or use closing() in contextlib  function to initialize datebase and maintain the connection
from contextlib import closing
def init_db():
    with closing(connect_db()) as db:
        with app.open_resource('schema.sql', mode='r') as f: 
            db.cursor().executescript(f.read())
        db.commit()

The "closing()" helper function allows us to keep a connection open for the duration of the with block. 
When we connect to a database we get a connection object (here called db) that can give us a "cursor". On that cursor there is a method to execute a complete script. Finally we only have to commit the changes. SQLite 3 and other transactional databases will not commit unless you explicitly tell it to.
Then initialize the DB
>>> from flaskr import init_db
>>> init_db()
.................................
Flask allows us to do that with the "before_request()", "after_request()" and "teardown_request()" decorators:
Functions marked with before_request() are called before a request and passed no arguments. Functions marked with after_request() are called after a request and passed the response that will be sent to the client. They have to return that response object or a different one. 
Different from "before_request()" and "after_request()", "teardown_request()" got called when an excption is raised.
@app.before_request
def before_request():
    g.db = connect_db()

@app.teardown_request
def teardown_request(exception):
    db = getattr(g, 'db', None)
    if db is not None:
        db.close()

Flask provides the special "g" object that stores current database connection. 

@app.route('/')
def show_entries():
    cur = g.db.execute('select title, text from entries order by id desc')
    entries = [dict(title=row[0], text=row[1]) for row in cur.fetchall()]
    return render_template('show_entries.html', entries=entries)

@app.route('/add', methods=['POST'])
def add_entry():
    if not session.get('logged_in'): #checking the session key, set when logging in
        abort(401)
    g.db.execute('insert into entries (title, text) values (?, ?)', #question mark, important
                 [request.form['title'], request.form['text']])
    g.db.commit()
    flash('New entry was successfully posted')
    return redirect(url_for('show_entries'))
@app.route('/login', methods=['GET', 'POST'])
def login():
    error = None
    if request.method == 'POST':
        if request.form['username'] != app.config['USERNAME']:
            error = 'Invalid username'
        elif request.form['password'] != app.config['PASSWORD']:
            error = 'Invalid password'
        else:
            session['logged_in'] = True
            flash('You were logged in')
            return redirect(url_for('show_entries'))
    return render_template('login.html', error=error)
@app.route('/logout')
def logout():
    session.pop('logged_in', None)
    flash('You were logged out')
    return redirect(url_for('show_entries'))
# the app.run part has to be at the end of the view file.
......................
Flask unittest

################################# -
To fatch setuptools
wget https://bitbucket.org/pypa/setuptools/raw/0.8/ez_setup.py -O - | python

Using get-pip

$ curl -O https://raw.github.com/pypa/pip/master/contrib/get-pip.py
$ [sudo] python get-pip.py
################################# -
import json

data = [ { 'a':'A', 'b':(2, 4), 'c':3.0 } ]
print 'DATA:', repr(data)

data_string = json.dumps(data)
print 'JSON:', data_string

$ python json_simple_types.py

DATA: [{'a': 'A', 'c': 3.0, 'b': (2, 4)}]
JSON: [{"a": "A", "c": 3.0, "b": [2, 4]}]

>>> import json
>>>
>>> data = [ { 'a':'A', 'b':(2, 4), 'c':3.0 } ]
>>> data_string = json.dumps(data)
>>> print 'ENCODED:', data_string
ENCODED: [{"a": "A", "c": 3.0, "b": [2, 4]}]

>>> decoded = json.loads(data_string)
>>> print 'DECODED:', decoded
DECODED: [{u'a': u'A', u'c': 3.0, u'b': [2, 4]}]

>>> print 'ORIGINAL:', type(data[0]['b'])
ORIGINAL: <type 'tuple'>
>>> print 'DECODED :', type(decoded[0]['b'])
DECODED : <type 'list'>
.................................
import json

data = [ { 'a':'A', 'b':(2, 4), 'c':3.0 } ]
print 'DATA:', repr(data)

unsorted = json.dumps(data)
print 'JSON:', json.dumps(data)
print 'SORT:', json.dumps(data, sort_keys=True)

first = json.dumps(data, sort_keys=True)
second = json.dumps(data, sort_keys=True)

print 'UNSORTED MATCH:', unsorted == first
print 'SORTED MATCH  :', first == second

$ python json_sort_keys.py

DATA: [{'a': 'A', 'c': 3.0, 'b': (2, 4)}]
JSON: [{"a": "A", "c": 3.0, "b": [2, 4]}]
SORT: [{"a": "A", "b": [2, 4], "c": 3.0}]
UNSORTED MATCH: False
SORTED MATCH  : True
.................................
import json

data = [ { 'a':'A', 'b':(2, 4), 'c':3.0 } ]
print 'DATA:', repr(data)

print 'NORMAL:', json.dumps(data, sort_keys=True)
print 'INDENT:', json.dumps(data, sort_keys=True, indent=2)

$ python json_indent.py

DATA: [{'a': 'A', 'c': 3.0, 'b': (2, 4)}]
NORMAL: [{"a": "A", "b": [2, 4], "c": 3.0}]
INDENT: [
  {
    "a": "A",
    "b": [
      2,
      4
    ],
    "c": 3.0
  }
]
.................................
import json

data = [ { 'a':'A', 'b':(2, 4), 'c':3.0 } ]
print 'DATA:', repr(data)
print 'repr(data)             :', len(repr(data))
print 'dumps(data)            :', len(json.dumps(data))
print 'dumps(data, indent=2)  :', len(json.dumps(data, indent=2))
print 'dumps(data, separators):', len(json.dumps(data, separators=(',',':')))

$ python json_compact_encoding.py

DATA: [{'a': 'A', 'c': 3.0, 'b': (2, 4)}]
repr(data)             : 35
dumps(data)            : 35
dumps(data, indent=2)  : 76
dumps(data, separators): 29
.................................
The JSON format expects the keys to a dictionary to be strings. If you have other types as keys in your dictionary, trying to encode the object will produce a ValueError. One way to work around that limitation is to skip over non-string keys using the skipkeys argument:
import json

data = [ { 'a':'A', 'b':(2, 4), 'c':3.0, ('d',):'D tuple' } ]

print 'First attempt'
try:
    print json.dumps(data)
except (TypeError, ValueError) as err:
    print 'ERROR:', err

print
print 'Second attempt'
print json.dumps(data, skipkeys=True)

$ python json_skipkeys.py

First attempt
ERROR: keys must be a string

Second attempt
[{"a": "A", "c": 3.0, "b": [2, 4]}]
.................................
class MyObj(object):
    def __init__(self, s):
        self.s = s
    def __repr__(self):
	return '<MyObj(%s)>' % self.s

import json
import json_myobj

obj = json_myobj.MyObj('instance value goes here')

print 'First attempt'
try:
    print json.dumps(obj)
except TypeError, err:
    print 'ERROR:', err

def convert_to_builtin_type(obj):
    print 'default(', repr(obj), ')'
    # Convert objects to a dictionary of their representation
    d = { '__class__':obj.__class__.__name__, 
          '__module__':obj.__module__,
          }
    d.update(obj.__dict__)
    return d

print
print 'With default'
print json.dumps(obj, default=convert_to_builtin_type)

$ python json_dump_default.py

First attempt
ERROR: <MyObj(instance value goes here)> is not JSON serializable

With default
default( <MyObj(instance value goes here)> )
{"s": "instance value goes here", "__module__": "json_myobj", "__class__": "MyObj"}

ults and create a MyObj instance, we need to tie in to the decoder so we can import the class from the module and create the instance. For that, we use the object_hook argument to loads().

The object_hook is called for each dictionary decoded from the incoming data stream, giving us a chance to convert the dictionary to another type of object. The hook function should return the object it wants the calling application to receive instead of the dictionary.

import json

def dict_to_object(d):
    if '__class__' in d:
        class_name = d.pop('__class__')
        module_name = d.pop('__module__')
        module = __import__(module_name)
        print 'MODULE:', module
        class_ = getattr(module, class_name)
        print 'CLASS:', class_
        args = dict( (key.encode('ascii'), value) for key, value in d.items())
        print 'INSTANCE ARGS:', args
        inst = class_(**args)
    else:
        inst = d
    return inst

encoded_object = '[{"s": "instance value goes here", "__module__": "json_myobj", "__class__": "MyObj"}]'

myobj_instance = json.loads(encoded_object, object_hook=dict_to_object)
print myobj_instance

Since json converts string values to unicode objects, we need to re-encode them as ASCII strings before using them as keyword arguments to the class constructor.

$ python json_load_object_hook.py

MODULE: <module 'json_myobj' from '/Users/dhellmann/Documents/PyMOTW/src/PyMOTW/json/json_myobj.pyc'>
CLASS: <class 'json_myobj.MyObj'>
INSTANCE ARGS: {'s': u'instance value goes here'}
[<MyObj(instance value goes here)>]
.................................
import json
import tempfile

data = [ { 'a':'A', 'b':(2, 4), 'c':3.0 } ]

f = tempfile.NamedTemporaryFile(mode='w+')
json.dump(data, f)
f.flush()

print open(f.name, 'r').read()

$ python json_dump_file.py

[{"a": "A", "c": 3.0, "b": [2, 4]}]
.................................
import json
import tempfile

f = tempfile.NamedTemporaryFile(mode='w+')
f.write('[{"a": "A", "c": 3.0, "b": [2, 4]}]')
f.flush()
f.seek(0)

print json.load(f)  #json.loads is to load string, json.load is to load file

$ python json_load_file.py

[{u'a': u'A', u'c': 3.0, u'b': [2, 4]}]
.................................
################################# -
#!/usr/bin/python

# import modules used here -- sys is a very standard one
import sys

# Gather our code in a main() function
def main():
    print 'Hello there', sys.argv[1]
    # Command line args are in sys.argv[1], sys.argv[2] ...
    # sys.argv[0] is the script name itself and can be ignored

# Standard boilerplate to call the main() function to begin
# the program.
if __name__ == '__main__': # means the script is running directly, not imported  -
    main()

$ python hello.py Guido
Hello there Guido
$ ./hello.py Alice    # without needing 'python' first (Unix)
Hello there Alice

The outermost statements in a Python file, or "module", do its one-time setup -- those statements run from top to bottom the first time the module is imported somewhere, setting up its variables and functions. A Python module can be run directly -- as above "python hello.py Bob" -- or it can be imported and used by some other module. When a Python file is run directly, the special variable "__name__" is set to "__main__". Therefore, it's common to have the boilerplate if __name__ ==... shown above to call a main() function when the module is run directly, but not when the module is imported by some other module.
................................. '-
Functoin
import sys
def repeat(s, exclaim):
    """Returns the string s repeated 3 times.
    If exclaim is true, add exclamation marks.
    """

    result = s + s + s * 3 # can also use "s * 3" which is faster (Why?)
    if exclaim == 'True':
        result = result + '!!!'
    return result     # result , local variable

def main():
        print sys.argv[1], sys.argv[2]
        print repeat(sys.argv[1], sys.argv[2])
#def main():
#    print repeat('Yay', False)      ## YayYayYay
#    print repeat('Woo Hoo', True)   ## Woo HooWoo HooWoo Hoo!!!

if __name__ == '__main__':
    main()

.................................
string
A double quoted string literal can contain single quotes without any fuss (e.g. "I didn't do it") and likewise single quoted string can contain double quotes. A string literal can span multiple lines, but there must be a backslash \ at the end of each line to escape the newline. String literals inside triple quotes, """" or ''', can multiple lines of text. 
"""
s = 'hi'
print s[1]          ## i
print len(s)        ## 2
print s + ' there'  ## hi there

  pi = 3.14
  ##text = 'The value of pi is ' + pi      ## NO, does not work
  text = 'The value of pi is '  + str(pi)  ## yes

For numbers, the standard operators, +, /, * work in the usual way. There is no ++ operator, but +=, -=, etc. work. If you want integer division, it is most correct to use 2 slashes -- e.g. 6 // 5 is 1 

  raw = r'this\t\n and that'
  print raw     ## this\t\n and that
    
  multi = """It was the best of times.
  It was the worst of times."""

s.lower(), s.upper() -- returns the lowercase or uppercase version of the string
s.strip() -- returns a string with whitespace removed from the start and end
s.isalpha()/s.isdigit()/s.isspace()... -- tests if all the string chars are in the various character classes
s.startswith('other'), s.endswith('other') -- tests if the string starts or ends with the given other string
s.find('other') -- searches for the given other string (not a regular expression) within s, and returns the first index where it begins or -1 if not found
s.replace('old', 'new') -- returns a string where all occurrences of 'old' have been replaced by 'new'
s.split('delim') -- returns a list of substrings separated by the given delimiter. The delimiter is not a regular expression, it's just text. 'aaa,bbb,ccc'.split(',') -> ['aaa', 'bbb', 'ccc'].  '  As a convenient special case s.split() (with no arguments) splits on all whitespace chars.
s.join(list) -- opposite of split(), joins the elements in the given list together using the string as the delimiter. e.g. '---'.join(['aaa', 'bbb', 'ccc']) -> aaa---bbb---ccc

.................................
String slice

  H  e  l  l  o
0   1  2  3  4
-5 -4 -3 -2 -1
.................................
String %
%d int, %s string, %f/%g floating point
# % operator
text = "%d little pigs come out or I'll %s and %s and %s" % (3, 'huff', 'puff', 'blow down')
# add parens to make the long-line work:
text = ("%d little pigs come out or I'll %s and %s and %s" %
    (3, 'huff', 'puff', 'blow down'))
.................................
Unicode string
> ustring = u'A unicode \u018e string \xf1'
> ustring
u'A unicode \u018e string \xf1'

To convert a unicode string to bytes with an encoding such as 'utf-8', call the ustring.encode('utf-8') method on the unicode string. Going the other direction, the unicode(s, encoding) function converts encoded plain bytes to a unicode string: 

## (unistring from above contains a unicode string)
> s = unistring.encode('utf-8')
> s
'A unicode \xc6\x8e string \xc3\xb1'  ## bytes of utf-8 encoding
> t = unicode(s, 'utf-8')             ## Convert bytes back to a unicode string
> t == unistring                      ## It's the same as the original, yay!
True
.................................
If statement
The "zero" values all count as false: None, 0, empty string, empty list, empty dictionary. There is also a Boolean type with two values: True and False (converted to an int, these are 1 and 0). Python has the usual comparison operations: ==, !=, <, <=, >, >=. Unlike Java and C, == is overloaded to work correctly with strings. The boolean operators are the spelled out words *and*, *or*, *not* (Python does not use the C-style && || !).

  if speed >= 80:
    print 'License and registration please'
    if mood == 'terrible' or speed >= 100:
      print 'You have the right to remain silent.'
    elif mood == 'bad' or speed >= 90:
      print "I'm going to have to write you a ticket."
      write_ticket()
    else:
      print "Let's try to keep it under 80 ok?"

  if speed >= 80: print 'You are so busted'
  else: print 'Have a nice day'
.................................
list

  colors = ['red', 'blue', 'green']
  print colors[0]    ## red
  print colors[2]    ## green
  print len(colors)  ## 3

 b = colors   ## Does not copy the list, it is a pointer to the same part of memeory
To copy 1D list:
b=a[::]
For 2D list:
b = [x[:] for x in a]

The "empty list" is just an empty pair of brackets [ ]. The '+' works to append two lists, so [1, 2] + [3, 4] yields [1, 2, 3, 4] (this is just like + with strings).
To measure number of items in list or the lenght of the list:
len(list)
.................................
for loop
  squares = [1, 4, 9, 16]
  sum = 0
  for num in squares:
    sum += num
  print sum  ## 30

for ch in s: print ch 

in loop
  list = ['larry', 'curly', 'moe']
  if 'curly' in list:
    print 'yay'

.................................
Range
  ## print the numbers from 0 through 99
  for i in range(100):
    print i
There is a variant xrange() which avoids the cost of building the whole list for performance sensitive cases 

while loop
  ## Access every 3rd element in a list
  i = 0
  while i < len(a):
    print a[i]
    i = i + 3

Python also has the standard while-loop, and the *break* and *continue* statements work as in C++ and Java, altering the course of the innermost loop.

To duplicate a list, instead to sharing the same list:
B=A[:]
list.append(elem) -- adds a single element to the end of the list. Common error: does not return the new list, just modifies the original.
list.insert(index, elem) -- inserts the element at the given index, shifting elements to the right.

list.extend(list2) adds the elements in list2 to the end of the list. Using + or += on a list is similar to using extend().

list.index(elem) -- searches for the given element from the start of the list and returns its index. Throws a ValueError if the element does not appear (use "in" to check without a ValueError).

list.remove(elem) -- searches for the first instance of the given element and removes it (throws ValueError if not present)

list.sort() -- sorts the list in place (does not return it). (The sorted() function shown below is preferred.)

list.reverse() -- reverses the list in place (does not return it)

list.pop(index) -- removes and returns the element at the given index. Returns the rightmost element if index is omitted (roughly the opposite of append()).

  list = ['larry', 'curly', 'moe']
  list.append('shemp')         ## append elem at end
  list.insert(0, 'xxx')        ## insert elem at index 0
  list.extend(['yyy', 'zzz'])  ## add list of elems at end
  print list  ## ['xxx', 'larry', 'curly', 'moe', 'shemp', 'yyy', 'zzz']
  print list.index('curly')    ## 2

  list.remove('curly')         ## search and remove that element
  list.pop(1)                  ## removes and returns 'larry'
  print list  ## ['xxx', 'moe', 'shemp', 'yyy', 'zzz']

note that the above methods do not *return* the modified list, they just modify the original list.

  list = [1, 2, 3]
  print list.append(4)   ## NO, does not work, append() returns None
  ## Correct pattern:
  list.append(4)
  print list  ## [1, 2, 3, 4]

  list = []          ## Start as the empty list
  list.append('a')   ## Use append() to add elements
  list.append('b')

  list = ['a', 'b', 'c', 'd']
  print list[1:-1]   ## ['b', 'c']
  list[0:2] = 'z'    ## replace ['a', 'b'] with ['z']
  print list         ## ['z', 'c', 'd']

.................................
sort
  a = [5, 1, 4, 3]
  print sorted(a)  ## [1, 3, 4, 5]
  print a  ## [5, 1, 4, 3]

  strs = ['aa', 'BB', 'zz', 'CC']
  print sorted(strs)  ## ['BB', 'CC', 'aa', 'zz'] (case sensitive)
  print sorted(strs, reverse=True)   ## ['zz', 'aa', 'CC', 'BB']

  strs = ['ccc', 'aaaa', 'd', 'bb']
  print sorted(strs, key=len)  ## ['d', 'bb', 'ccc', 'aaaa'], sort by string length
  ## "key" argument specifying str.lower function to use for sorting
  print sorted(strs, key=str.lower)  ## ['aa', 'BB', 'CC', 'zz']

  ## Say we have a list of strings we want to sort by the last letter of the string.
  strs = ['xc', 'zb', 'yd' ,'wa']

  ## Write a little function that takes a string, and returns its last letter.
  ## This will be the key function (takes in 1 value, returns 1 value).
  def MyFn(s):
    return s[-1]

  ## Now pass key=MyFn to sorted() to sort by the last letter:
  print sorted(strs, key=MyFn)  ## ['wa', 'zb', 'xc', 'yd']

a=[(3,4),(5,9),(6,20),(8,15),(10,30)]
sorted_by_second = sorted(data, key=lambda tup: tup[1])
or:
data.sort(key=lambda tup: tup[1])  // sorts in place
.................................
Tuples
  tuple = (1, 2, 'hi')
  print len(tuple)  ## 3
  print tuple[2]    ## hi
  tuple[2] = 'bye'  ## NO, tuples cannot be changed
  tuple = (1, 2, 'bye')  ## this works

To create a size-1 tuple, the lone element must be followed by a comma.

  tuple = ('hi',)   ## size-1 tuple
  (x, y, z) = (42, 13, "hike")
  print z  ## hike
  (err_string, err_code) = Foo()  ## Foo() returns a length-2 tuple
............................................
List Comprehensions
The syntax is [ expr for var in list ] 

  nums = [1, 2, 3, 4]
  squares = [ n * n for n in nums ]   ## [1, 4, 9, 16]

  strs = ['hello', 'and', 'goodbye']
  shouting = [ s.upper() + '!!!' for s in strs ] ## ['HELLO!!!', 'AND!!!', 'GOODBYE!!!']

  ## Select values <= 2
  nums = [2, 8, 1, 6]
  small = [ n for n in nums if n <= 2 ]  ## [2, 1]

  ## Select fruits containing 'a', change to upper case
  fruits = ['apple', 'cherry', 'bannana', 'lemon']
  afruits = [ s.upper() for s in fruits if 'a' in s ]
  ## ['APPLE', 'BANNANA']
.................................
Dict
  ## Can build up a dict by starting with the the empty dict {}
  ## and storing key/value pairs into the dict like this:
  ## dict[key] = value-for-that-key
  dict = {}
  dict['a'] = 'alpha'
  dict['g'] = 'gamma'
  dict['o'] = 'omega'

  print dict  ## {'a': 'alpha', 'o': 'omega', 'g': 'gamma'}

  print dict['a']     ## Simple lookup, returns 'alpha'
  dict['a'] = 6       ## Put new key/value into dict
  'a' in dict         ## True
  ## print dict['z']                  ## Throws KeyError
  if 'z' in dict: print dict['z']     ## Avoid KeyError
  print dict.get('z')  ## None (instead of KeyError)

  ## By default, iterating over a dict iterates over its keys.
  ## Note that the keys are in a random order.
  for key in dict: print key
  ## prints a g o
  
  ## Exactly the same as above
  for key in dict.keys(): print key

  ## Get the .keys() list:
  print dict.keys()  ## ['a', 'o', 'g']

  ## Likewise, there's a .values() list of values
  print dict.values()  ## ['alpha', 'omega', 'gamma']

  ## Common case -- loop over the keys in sorted order,
  ## accessing each key/value
  for key in sorted(dict.keys()):
    print key, dict[key]
  
  ## .items() is the dict expressed as (key, value) tuples
  print dict.items()  ##  [('a', 'alpha'), ('o', 'omega'), ('g', 'gamma')]

  ## This loop syntax accesses the whole dict by looping
  ## over the .items() tuple list, accessing one (key, value)
  ## pair on each iteration.
  for k, v in dict.items(): print k, '>', v
  ## a > alpha    o > omega     g > gamma
>>> d
{(200, 600): 3, (1, 1): 10, (2, 4): 100}
>>> sorted(d.items(), key=lambda t: t[1])
[((200, 600), 3), ((1, 1), 10), ((2, 4), 100)]
>>> sorted(d.items(), key=lambda t: t[0])  
[((1, 1), 10), ((2, 4), 100), ((200, 600), 3)]

>>> b
{'ss': 10, 'a': 1000, 'sd': 1}
>>> sorted(b.items(), key=lambda x:x[0]) # sort by dict key
[('a', 1000), ('sd', 1), ('ss', 10)]
>>> sorted(b.items(), key=lambda x:x[1]) #sort by dict value
[('sd', 1), ('ss', 10), ('a', 1000)]
>>> sorted(b.items(),reverse=True, key=lambda x:x[1]) # sort by reversed dict value
[('a', 1000), ('ss', 10), ('sd', 1)]

>>> a=dict()
>>> a.items
<built-in method items of dict object at 0x10fa430>
>>> a.items()
[]
>>> a['w']=10
>>> a
{'w': 10}
>>> a.items()
[('w', 10)]
>>> a
{'w': 10}
>>> a.keys()
['w']
>>> a.values()
[10]

  hash = {}
  hash['word'] = 'garfield'
  hash['count'] = 42
  s = 'I want %(count)d copies of %(word)s' % hash  # %d for int, %s for string
  # 'I want 42 copies of garfield'

  var = 6
  del var  # var no more!
  
  list = ['a', 'b', 'c', 'd']
  del list[0]     ## Delete first element
  del list[-2:]   ## Delete last two elements
  print list      ## ['b']

  dict = {'a':1, 'b':2, 'c':3}
  del dict['b']   ## Delete 'b' entry
  print dict      ## {'a':1, 'c':3}

#To copy dictionary and set, instead of creating a new pointer
a=dict()
b=set()
c=a.copy()
d=b.copy()

.................................
File

Instead of 'r', use 'w' for writing, and 'a' for append. The special mode 'rU' is the "Universal" option for text files where it's smart about converting different line-endings so they always come through as a simple '\n'.   '
  # Echo the contents of a file
  f = open('foo.txt', 'rU')
  for line in f:   ## iterates over the lines of the file
    print line,    ## trailing , so print does not add an end-of-line char
                   ## since 'line' already includes the end-of line.
  f.close()

Reading one line at a time has the nice quality that not all the file needs to fit in memory at one time -- handy if you want to look at every line in a 10 gigabyte file without using 10 gigabytes of memory. The 'f.readlines()' method reads the whole file into memory and returns its contents as a list of its lines. The 'f.read()' method reads the whole file into a single string, which
can be a handy way to deal with the text all at once, such as with regular expressions we'll see later.   '

For writing, 'f.write(string)' method is the easiest way to write data to an open output file. Or you can use "print" with an open file, but the syntax is nasty: "print >> f, string". 

.................................
Files Unicode

import codecs

f = codecs.open('foo.txt', 'rU', 'utf-8')
for line in f:
  # here line is a *unicode* string

............................................
Regular expression

To compile regular expression:
>>> import re, string; pattern = re.compile('[\W_]+')
>>> pattern.sub('', string.printable)
'0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'
>>> string.printable
'0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~\t\n\r\x0b\x0c'

>>> st
'an example word:cat!!'
>>> re.sub(r'\W+', '', st)
'anexamplewordcat'


match = re.search(pat, str)

str = 'an example word:cat!!'
match = re.search(r'word:\w\w\w', str)
# If-statement after search() tests if it succeeded
  if match:                      
    print 'found', match.group() ## 'found word:cat'
  else:
    print 'did not find'
The code match = re.search(pat, str) stores the search result in a variable named "match". Then the if-statement tests the match -- if true the search succeeded and match.group() is the matching text (e.g. 'word:cat'). Otherwise if the match is false (None to be more specific), then the search did not succeed, and there is no matching text.

The 'r' at the start of the pattern string designates a python "raw" string which passes through backslashes without change 

a, X, 9, < -- ordinary characters just match themselves exactly. The meta-characters which do not match themselves because they have special meanings are: . ^ $ * + ? { [ ] \ | ( ) (details below)

. (a period) -- matches any single character except newline '\n' \w -- (lowercase w) matches a "word" character: a letter or digit or underbar [a-zA-Z0-9_]. Note that although "word" is the mnemonic for this, it only matches a single word char, not a whole word. \W (upper case W) matches any non-word character.

\b -- boundary between word and non-word

\s -- (lowercase s) matches a single whitespace character -- space, newline, return, tab, form [ \n\r\t\f]. \S (upper case S) matches any non-whitespace character.

\t, \n, \r -- tab, newline, return

\d -- decimal digit [0-9] (some older regex utilities do not support but \d, but they all support \w and \s)

^ = start, $ = end -- match the start or end of the string

\ -- inhibit the "specialness" of a character. So, for example, use \. to match a period or \\ to match a slash. If you are unsure if a character has special meaning, such as '@', you can put a slash in front of it, \@, to make sure it is treated just as a character.


+ -- 1 or more occurrences of the pattern to its left, e.g. 'i+' = one or more i's     '
* -- 0 or more occurrences of the pattern to its left
? -- match 0 or 1 occurrences of the pattern to its left

  str = 'purple alice-b@google.com monkey dishwasher'
  match = re.search(r'\w+@\w+', str)
  if match:
    print match.group()  ## 'b@google'

Square brackets can be used to indicate a set of chars, so [abc] matches 'a' or 'b' or 'c'. The codes \w, \s etc. work inside square brackets too with the one exception that dot (.) just means a literal dot. For the emails problem, the square brackets are an easy way to add '.' and '-' to the set of chars which can appear around the @ with the pattern r'[\w.-]+@[\w.-]+' to get the whole email address:

  match = re.search(r'[\w.-]+@[\w.-]+', str) # means it could be word, . or -, and multiple of them 
  if match:
    print match.group()  ## 'alice-b@google.com'

(More square-bracket features) You can also use a dash to indicate a range, so [a-z] matches all lowercase letters. To use a dash without indicating a range, put the dash last, e.g. [abc-]. An up-hat (^) at the start of a square-bracket set inverts it, so [^ab] means any char except 'a' or 'b'.


Group Extraction

The "group" feature of a regular expression allows you to pick out parts of the matching text. Suppose for the emails problem that we want to extract the username and host separately. To do this, add parenthesis ( ) around the username and host in the pattern, like this: r'([\w.-]+)@([\w.-]+)'. In this case, the parenthesis do not change what the pattern will match, instead they establish logical "groups" inside of the match text. On a successful search, match.group(1) is the match text corresponding to the 1st left parenthesis, and match.group(2) is the text corresponding to the 2nd left parenthesis. The plain match.group() is still the whole match text as usual.

  str = 'purple alice-b@google.com monkey dishwasher'
  match = re.search('([\w.-]+)@([\w.-]+)', str)
  if match:
    print match.group()   ## 'alice-b@google.com' (the whole match)
    print match.group(1)  ## 'alice-b' (the username, group 1)
    print match.group(2)  ## 'google.com' (the host, group 2)

A common workflow with regular expressions is that you write a pattern for the thing you are looking for, adding parenthesis groups to extract the parts you want.

findall
findall() is probably the single most powerful function in the re module.  Above we used re.search() to find the first match for a pattern. findall() finds *all* the matches and returns them as a list of strings, with each string representing one match.

  ## Suppose we have a text with many email addresses
  str = 'purple alice@google.com, blah monkey bob@abc.com blah dishwasher'

  ## Here re.findall() returns a list of all the found email strings
  emails = re.findall(r'[\w\.-]+@[\w\.-]+', str) ## ['alice@google.com', 'bob@abc.com']
  for email in emails:
    # do something with each found email string
    print email

  # Open file
  f = open('test.txt', 'r')
  # Feed the file text into findall(); it returns a list of all the found
  # strings
  strings = re.findall(r'some pattern', f.read())

  str = 'purple alice@google.com, blah monkey bob@abc.com blah dishwasher'
  tuples = re.findall(r'([\w\.-]+)@([\w\.-]+)', str)
  print tuples  ## [('alice', 'google.com'), ('bob', 'abc.com')]
  for tuple in tuples:
    print tuple[0]  ## username
    print tuple[1]  ## host

The option flag is added as an extra argument to the search() or findall() etc., e.g. re.search(pat, str, re.IGNORECASE).

IGNORECASE -- ignore upper/lowercase differences for matching, so 'a' matches both 'a' and 'A'.

DOTALL -- allow dot (.) to match newline -- normally it matches anything but newline. This can trip you up -- you think .* matches everything, but by default it does not go past the end of a line. Note that \s (whitespace) includes newlines, so if you want to match a run of whitespace that may include a newline, you can just use \s*

MULTILINE -- Within a string made of many lines, allow ^ and $ to match the start and end of each line. Normally ^/$ would just match the start and end of the whole string.

Greedy vs. Non-Greedy 

There is an extension to regular expression where you add a ? at the end, such as .*? or .+?, changing them to be non-greedy. Now they stop as soon as they can. So the pattern '(<.*?>)' will get just '<b>' as the first match, and '</b>' as the second match, 

Replacement

The re.sub(pat, replacement, str) function searches for all the instances of pattern in the given string, and replaces them. The replacement string can include '\1', '\2' which refer to the text from group(1), group(2), and so on from the original matching text.

  str = 'purple alice@google.com, blah monkey bob@abc.com blah dishwasher'
  ## re.sub(pat, replacement, str) -- returns new string with all replacements,
  ## \1 is group(1), \2 group(2) in the replacement
  print re.sub(r'([\w\.-]+)@([\w\.-]+)', r'\1@yo-yo-dyne.com', str)
  ## purple alice@yo-yo-dyne.com, blah monkey bob@yo-yo-dyne.com blah dishwasher

............................................
File System -- os, os.path, shutil

The *os* and *os.path* modules include many functions to interact with the file system. The *shutil* module can copy files.

os module docs

filenames = os.listdir(dir) -- list of filenames in that directory path (not including . and ..). The filenames are just the names in the directory, not their absolute paths.

os.path.join(dir, filename) -- given a filename from the above list, use this to put the dir and filename together to make a path

os.path.abspath(path) -- given a path, return an absolute form, e.g.  /home/nick/foo/bar.html

os.path.dirname(path), os.path.basename(path) -- given dir/foo/bar.html, return the dirname "dir/foo" and basename "bar.html"

os.path.exists(path) -- true if it exists

os.mkdir(dir_path) -- makes one dir, os.makedirs(dir_path) makes all the needed dirs in this path

shutil.copy(source-path, dest-path) -- copy a file (dest path directories should exist)

## Example pulls filenames from a dir, prints their relative and absolute paths
import os
def printdir(dir):
  filenames = os.listdir(dir)
  for filename in filenames:
    print filename  ## foo.txt
    print os.path.join(dir, filename) ## dir/foo.txt (relative to current dir)
    print os.path.abspath(os.path.join(dir, filename)) ## /home/nick/dir/foo.txt

for root, dirs, _ in os.walk(base_dir): 
#to iter through directory
list the root dir, the sub dirs [list] and files [list] , need another loop to
iter through all sub dir or files. 

use os.path.join(), to avoie \ in string 

import os
base_dir='d:\mp3\music\ipod_new'
for root, dirs, _ in os.walk(base_dir): #to iter through directory
	for d in dirs:
		source=os.path.join(base_dir , d ) 
		print source
		for sroot, _, files in os.walk(source):
			for sf in files:
				print sf 
				t_file=os.path.join(base_dir,sf)
				s_file=os.path.join(source,sf)
				print s_file, t_file
				os.rename(s_file, t_file) 

.................................
Running External Processes -- commands
commands and os.command are deprecated. use subprocess!

The *commands* module is a simple way to run an external command and capture its output.

commands module docs
(status, output) = commands.getstatusoutput(cmd) -- runs the command, waits for it to exit, and returns its status int and output text as a tuple. The command is run with its standard output and standard error combined into the one output text. The status will be non-zero if the command failed. Since the standard-err of the command is captured, if it fails, we need to print some indication of what happened.

output = commands.getoutput(cmd) -- as above, but without the status int.

There is a commands.getstatus() but it does something else, so don''t use it -- dumbest bit of method naming ever!

If you want more control over the running of the sub-process, see the "popen2" module (http://docs.python.org/lib/module-popen2.html) 

There is also a simple os.system(cmd) which runs the command and dumps its output onto your output and returns its error code. This works if you want to run the command but do not need to capture its output into your python data structures.

## Given a dir path, run an external 'ls -l' on it --
## shows how to call an external program
def listdir(dir):
  cmd = 'ls -l ' + dir
  print "Command to run:", cmd   ## good to debug cmd before actually running it
  (status, output) = commands.getstatusoutput(cmd)
  if status:    ## Error case, print the command's output to stderr and exit
    sys.stderr.write(output)
    sys.exit(1)
  print output  ## Otherwise do something with the command's output
.................................
import subprocess
subprocess.call("command1")
subprocess.call(["command1", "arg1", "arg2"])
subprocess.call(["ls", "-l", "/etc/resolv.conf"])

try:
    retcode = call("mycmd" + " myarg", shell=True)
    if retcode < 0:
        print >>sys.stderr, "Child was terminated by signal", -retcode
    else:
        print >>sys.stderr, "Child returned", retcode
except OSError, e:
    print >>sys.stderr, "Execution failed:", e

p = subprocess.Popen("date", stdout=subprocess.PIPE, shell=True) # shell=True, open a new shell
(output, err) = p.communicate()
print "Today is", output

p = subprocess.Popen(["ls", "-l", "/etc/resolv.conf"], stdout=subprocess.PIPE)
output, err = p.communicate()
print "*** Running ls -l command ***\n", output
#The output will be print out after the whole command complete.
# or
cmdping = "ping -c4 www.cyberciti.biz"
p = subprocess.Popen(cmdping, shell=True, stderr=subprocess.PIPE)
while True:
    out = p.stderr.read(1)
    if out == '' and p.poll() != None:
        break
    if out != '':
        sys.stdout.write(out)
        sys.stdout.flush()
.................................
Exceptions
  try:
    ## Either of these two lines could throw an IOError, say
    ## if the file does not exist or the read() encounters a low level error.
    f = open(filename, 'rU')
    text = f.read()
    f.close()
  except IOError:
    ## Control jumps directly to here if any of the above lines throws IOError.
    sys.stderr.write('problem reading:' + filename)
  ## In any case, the code then continues with the line after the try/except
You can get a pointer to the exception object itself with syntax "except IOError, e: .. (e points to the exception object)".

.................................
HTTP -- urllib and urlparse

The module *urllib* provides url fetching -- making a url look like a file you can read form. The *urlparse* module can take apart and put together urls.

urllib module docs

ufile = urllib.urlopen(url) -- returns a file like object for that url

text = ufile.read() -- can read from it, like a file (readlines() etc. also work)

info = ufile.info() -- the meta info for that request. info.gettype() is the mime time, e.g. 'text/html'

baseurl = ufile.geturl() -- gets the "base" url for the request, which may be different from the original because of redirects 

urllib.urlretrieve(url, filename) -- downloads the url data to the given file path

urlparse.urljoin(baseurl, url) -- given a url that may or may not be full, and the baseurl of the page it comes from, return a full url. Use geturl() above
to provide the base url.
## Given a url, try to retrieve it. If it's text/html,
## print its base url and its text.
def wget(url):
  ufile = urllib.urlopen(url)  ## get file-like object for url
  info = ufile.info()   ## meta-info about the url content
  if info.gettype() == 'text/html':
    print 'base url:' + ufile.geturl()
    text = ufile.read()  ## read all its text
    print text

## Version that uses try/except to print an error message if the
## urlopen() fails.
def wget2(url):
  try:
    ufile = urllib.urlopen(url)
    if ufile.info().gettype() == 'text/html':
      print ufile.read()
  except IOError:
    print 'problem reading url:', url
#################################
>>> '{0:_^11}'.format('hello')
'___hello___'

#Create a string 11 chars long fill with __

>>> '{0:.^20}'.format('hello')
'.......hello........'

#################################
explicit line joining: \

<< : shift left 2 digits-
>>> 2<<2
8
>>> 3<<2
12
>>>
>> : shift right 2 digits -
>>> 11>>2
2

& : bit-wise AND , both digit are 1
>>> 5 & 3
1
| : bit-wise OR, one of the digit is 1
>>> 5 | 3
7

^ : (bit-wise XOR) Bitwise XOR of the numbers
>>> 5^3
6

~ (bit-wise invert) The bit-wise inversion of x is -(x+1)
~5 gives -6

>>> a
6
>>> a *= 3
>>> a
18

**  : Exponentiation

if ... elif ... else -
input  : to reate from stdin-
number = 23
guess = int(input('Enter an integer : '))
if guess == number:
	print('Congratulations, you guessed it.') # New block starts here
	print('(but you do not win any prizes!)') # New block ends here
elif guess < number:
	print('No, it is a little higher than that') # Another block
	# You can do whatever you want in a block ...
else:
	print('No, it is a little lower than that')
	# you must have guessed > number to reach here
	print('Done')
	# This last statement is always executed, after the if statement is
	# executed

To input string, need '':
>>> x = input('put it here : ')
put it here : without quote
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "<string>", line 1
    without quote
                ^
SyntaxError: unexpected EOF while parsing

>>> x = input('put it here : ')
put it here : 'with quote'
>>> x
'with quote'

......................
while ... else  loop
number = 23
running = True
while running:
	guess = int(input('Enter an integer : '))
	if guess == number:
		print('Congratulations, you guessed it.')
		running = False # this causes the while loop to stop
	elif guess < number:
		print('No, it is a little higher than that.')
	else:
		print('No, it is a little lower than that.')
else:
	print('The while loop is over.')
	# Do anything else you want to do here
print('Done')

True : 1
False : 0

......................
For loop

for i in range(1, 5):
	print(i)
else:
	print('The for loop is over')

For range:
If we supply a third number to range, then that becomes the step count. For
example, range(1,5,2) gives [1,3]. It does not include the second number in
range.

Range generage 1 number a time. To see the full list:
list(range(1,5))

......................
break
if you break out of a for or while loop, any corresponding loop else block is
not executed.
while True:
	s = input('Enter something : ')
	if s == 'quit':
		break
	print('Length of the string is', len(s))
print('Done')

.................................
The continue statement is used to tell Python to skip the rest of the
statements
in the current loop block and to continue to the next iteration of the loop.
while True:
	s = input('Enter something : ')
	if s == 'quit':
		break
	if len(s) < 3:
		print('Too small')
		continue
	print('Input is of sufficient length')
	# Do other kinds of processing here...
.................................
variables defined in a function are local.
To make it global
global x, y, z

x = 50
def func():
	global x
	print('x is', x)
	x = 2
	print('Changed global x to', x)
func()
print('Value of x is', x)

.................................
Default Argument Values
Only those parameters which are at the end of the parameter list can be given
default argument values

def say(message, times = 1):
	print(message * times)

>>>say('Hello')

>>>say('World', 5)

......................
Keyword argument

def func(a, b=5, c=10):
	print('a is', a, 'and b is', b, 'and c is', c)

>>>func(3, 7)

>>>func(25, c=24)

>>>func(c=50, a=100)
.................................
VarArgs parameters
def total(initial=5, *numbers, **keywords):
	count = initial
	for number in numbers:
		count += number
	for key in keywords:
		count += keywords[key]
	return count
print(total(10, 1, 2, 3, vegetables=50, fruits=100))

When we declare a starred parameter such as *param, then all the positional
arguments from that point till the end are collected as a "tuple" called
.param..

Similarly, when we declare a double-starred parameter such as **param, then
all the keyword arguments from that point till the end are collected as a
"dictionary" called .param..  
.................................

def total(initial=5, *numbers, extra_number):
	count = initial
	for number in numbers:
		count += number
		count += extra_number
		print(count)
total(10, 1, 2, 3, extra_number=50)

The "extra_number" can be called with keyword only, since all input before that will be considered for "*numbers"


This one has to variable, extra_number is keyword only.
total(initial=5, *, extra_number)

The return statement is used to return from a function

def someFunction():
	pass
The pass statement is used in Python to indicate an empty block of statements.
......................
DocStrings

def printMax(x, y):
	'''Prints the maximum of two numbers.
	The two values must be integers.'''
	x = int(x) # convert to integers, if possible
	y = int(y)
	if x > y:
		print(x, 'is maximum')
	else:
		print(y, 'is maximum')

>>> printMax(3, 5)
>>> print(printMax.__doc__)  # to print DocString with function object
>>> attribute __doc__ 
5

......................
import sys
print('The command line arguments are:')
for i in sys.argv:
	print(i)
print'\n\nThe PYTHONPATH is', sys.path, '\n'


Run "import os; print(os.getcwd())" to find out the current directory of your
program.

To add path for python module, add .pth file to an existing directory in python path.
[root@vmstlsup05 site-packages]# cat test_project.pth
/home/djcode/mysite/mysite
or
>>> import sys
>>> sys.path.append("/Library/Subversion/django_src/trunk")
>>> import django
or
export PYTHONPATH="$PYTHONPATH;/Library/Subversion/django_src/trunk"
.................................
Byte-compiled .pyc files

These .pyc files are usually created in the same directory as the
corresponding .py files. If Python does not have permission to write to files
in
that directory, then the .pyc files will not be created.

To import a specific function, and do not need to type "math.sqrt", just sqrt.
from math import sqrt
print("Square root of 16 is", sqrt(16))

...................... '
Module's name

if __name__ == '__main__':
	print('This program is being run by itself')
else:
	print('I am being imported from another module')

......................
dir()
use the built-in dir function to list the identifers that an object defnes.

>>> import json
>>> dir(json)
['JSONDecoder', 'JSONEncoder', '__all__', '__author__', '__builtins__',
'__doc__
', '__file__', '__name__', '__package__', '__path__', '__version__',
'_default_d
ecoder', '_default_encoder', 'decoder', 'dump', 'dumps', 'encoder', 'load',
'loa
ds', 'scanner']


>>> import sys
>>> a = 5 # create a new variable 'a'
>>> dir()
['__builtins__', '__doc__', '__name__', '__package__', 'a', 'sys']
>>> del a # delete/remove a name
>>> dir()
['__builtins__', '__doc__', '__name__', '__package__', 'sys']
>>>
.................................
Packages
Packages are just folders of modules with a special __init__.py file that
indicates to Python that this folder is special because it contains Python
modules.
- <some folder present in the sys.path>/
	- world/
		- __init__.py
		- asia/
			- __init__.py
			- india/
				- __init__.py
				- foo.py
		- africa/
			- __init__.py
			- madagascar/
				- __init__.py
				- bar.py

.................................
# This is my shopping list
shoplist = ['apple', 'mango', 'carrot', 'banana']
print('I have', len(shoplist), 'items to purchase.')
print('These items are:', end=' ') # end is to finish print with " " not, a \n, so all in one line
#that's V3 syntax, For V2
print 'These items are:',
for item in shoplist:
	print(item, end=' ')
print('\nI also have to buy rice.')
shoplist.append('rice')
print('My shopping list is now', shoplist)
print('I will sort my list now')
shoplist.sort()
print('Sorted shopping list is', shoplist)
print('The first item I will buy is', shoplist[0])
olditem = shoplist[0]
del shoplist[0]
print('I bought the', olditem)
print('My shopping list is now', shoplist)
.................................

Tuple
One major feature of tuples is that they are immutable like strings.

An empty tuple is constructed by an empty pair of parentheses such as myempty
= ().
One item tuple
singleton = (2 , )

a tuple within a tuple does not lose its identity.
zoo = ('python', 'elephant', 'penguin') # remember the parentheses are optional
print('Number of animals in the zoo is', len(zoo))
new_zoo = 'monkey', 'camel', zoo
print('Number of cages in the new zoo is', len(new_zoo))
print('All animals in new zoo are', new_zoo)
print('Animals brought from old zoo are', new_zoo[2]) # the 3rd tuple
print('Last animal brought from old zoo is', new_zoo[2][2])  # the 3rd item in
the 3rd tuple
print('Number of animals in the new zoo is', len(new_zoo)-1+len(new_zoo[2]))

.................................
Dictionary
Note that you can use only immutable objects (like strings) for the keys of a
dictionary but you can use either immutable or mutable objects for the values
of the dictionary

ab = { 'Swaroop' : 'swaroop@swaroopch.com',
		'Larry' : 'larry@wall.org',
		'Matsumoto' : 'matz@ruby-lang.org',
		'Spammer' : 'spammer@hotmail.com'
}
print("Swaroop's address is", ab['Swaroop'])  
# Deleting a key-value pair
del ab['Spammer']
print('\nThere are {0} contacts in the address-book\n'.format(len(ab)))
for name, address in ab.items(): # loop through dictionary
	print('Contact {0} at {1}'.format(name, address))
# Adding a key-value pair
ab['Guido'] = 'guido@python.org'
if 'Guido' in ab:
	print("\nGuido's address is", ab['Guido'])

.................................
SET
 A set is an unordered collection with no duplicate elements. Basic uses include membership testing and eliminating duplicate entries.

To duplcate a set, instead of sharing the same set
B=A.copy()

>>> bri = set(['brazil', 'russia', 'india'])
>>> 'india' in bri
True
>>> 'usa' in bri
False
>>> bric = bri.copy()
>>> bric.add('china')
>>> bric.issuperset(bri)
True
>>> bri.remove('russia')
>>> bri & bric # OR bri.intersection(bric)
{'brazil', 'india'}
.................................
References

print('Simple Assignment')
shoplist = ['apple', 'mango', 'carrot', 'banana']
mylist = shoplist # mylist is just another name pointing to the same object!

If you want to make a copy of a list or such kinds of sequences or complex objects (not simple objects such as integers), then you have to use the slicing operation to make a copy.
.................................
String methods
name = 'Swaroop' # This is a string object
if name.startswith('Swa'):
    print('Yes, the string starts with "Swa"')
if 'a' in name:
    print('Yes, it contains the string "a"')
if name.find('war') != -1:
    print('Yes, it contains the string "war"')
delimiter = '_*_'
mylist = ['Brazil', 'Russia', 'India', 'China']
print(delimiter.join(mylist))
#Brazil_*_Russia_*_India_*_China

.................................
import os
import time
# 1. The files and directories to be backed up are specified in a list.
source = ['"C:\\My Documents"', 'C:\\Code']
# Notice we had to use double quotes inside the string for names with spaces
# in it.
# 2. The backup must be stored in a main backup directory
target_dir = 'E:\\Backup' # Remember to change this to what you will be using
# 3. The files are backed up into a zip file.
# 4. The name of the zip archive is the current date and time
target = target_dir + os.sep + time.strftime('%Y%m%d%H%M%S') + '.zip'
# 5. We use the zip command to put the files in a zip archive
zip_command = "zip -qr {0} {1}".format(target, ' '.join(source))
# Run the backup
if os.system(zip_command) == 0:   # to run shell command
    print('Successful backup to', target)
else:
    print('Backup FAILED')
import os.sep

Notice the use of the "os.sep" variable - this gives the directory separator
according
to your operating system i.e. it will be ./. in Linux and Unix, it will be
.\\. in
Windows and .:. in Mac OS. 

today = target_dir + os.sep + time.strftime('%Y%m%d')
# The current time is the name of the zip archive
now = time.strftime('%H%M%S')

if not os.path.exists(today):
	os.mkdir(today) # make directory
	print('Successfully created directory', today)
.................................
Class

The "__init__" method is run as soon as an object of a class is instantiated.
The method is useful to do any initialization you want to do with your object.

self : is always the first argv of a class function.

class Person:
	def __init__(self, name):
		self.name = name
	def sayHi(self):
		print('Hello, my name is', self.name)

p = Person('Swaroop')
p.sayHi()
# This short example can also be written as Person('Swaroop').sayHi()

"Class variables" are shared - they can be accessed by all instances of that class.  There is only one copy of the class variable and when any one object makes a change to a class variable, that change will be seen by all the other instances.

"Object variables" are owned by each individual object/instance of the class.
............................................
class Robot:
    '''Represents a robot, with a name.'''
    # A class variable, counting the number of robots, no self, accessed with
    # class name
    population = 0 
    def __init__(self, name):  # name is object variable , defined with self,
		#accessed with object name
		'''Initializes the data.'''
		self.name = name
		print('(Initializing {0})'.format(self.name))
		# When this person is created, the robot
		# adds to the population
		Robot.population += 1
    def __del__(self):   #de-structure
		'''I am dying.'''
		print('{0} is being destroyed!'.format(self.name))
		Robot.population -= 1
		if Robot.population == 0:
			print('{0} was the last one.'.format(self.name))
		else:
			print('There are still {0:d} robots working.'.format(Robot.population))
    def sayHi(self):
		'''Greeting by the robot.
		Yeah, they can do that.'''
		print('Greetings, my masters call me {0}.'.format(self.name))
    def howMany():  # a class method
		'''Prints the current population.'''
		print('We have {0:d} robots.'.format(Robot.population))
		howMany = staticmethod(howMany)

    #or  vvv
    @staticmethod
    def howMany():
		'''Prints the current population.'''
		print('We have {0:d} robots.'.format(Robot.population))
		#   ^^^

droid1 = Robot('R2-D2')
droid1.sayHi()
Robot.howMany()
droid2 = Robot('C-3PO')
droid2.sayHi()
Robot.howMany()
print("\nRobots can do some work here.\n")
print("Robots have finished their work. So let's destroy them.")
del droid1  # call the self.__del__ function
del droid2  
Robot.howMany()

A class method can be: either a "classmethod" or a "staticmethod" depending on whether we need to know which class we are part of. Since we don.t need such information, we will go for staticmethod .

All class members are public. One exception: If you use data members with names using the double underscore prefix such as "__privatevar", Python uses name-mangling to e.ectively make it a "private variable".
.................................
Inheritage
class SchoolMember:
    '''Represents any school member.'''
    def __init__(self, name, age):
		self.name = name
		self.age = age
		print('(Initialized SchoolMember: {0})'.format(self.name))
    def tell(self):
		'''Tell my details.'''
		print('Name:"{0}" Age:"{1}"'.format(self.name, self.age), end=" ")
class Teacher(SchoolMember):
    '''Represents a teacher.'''
    def __init__(self, name, age, salary):
		SchoolMember.__init__(self, name, age) # call the parent constructor
		self.salary = salary
		print('(Initialized Teacher: {0})'.format(self.name))
    def tell(self):
		SchoolMember.tell(self)
		print('Salary: "{0:d}"'.format(self.salary))
class Student(SchoolMember):
    '''Represents a student.'''
    def __init__(self, name, age, marks):
		SchoolMember.__init__(self, name, age) # call the parent constructor
		self.marks = marks
		print('(Initialized Student: {0})'.format(self.name))
    def tell(self):
		SchoolMember.tell(self)
		print('Marks: "{0:d}"'.format(self.marks))
t = Teacher('Mrs. Shrividya', 40, 30000)
s = Student('Swaroop', 25, 75)
print() # prints a blank line
members = [t, s]
for member in members:
member.tell() # works for both Teachers and Students, not using the parent tell() method

$ python3 inherit.py
(Initialized SchoolMember: Mrs. Shrividya)
(Initialized Teacher: Mrs. Shrividya)
(Initialized SchoolMember: Swaroop)
(Initialized Student: Swaroop)
Name:"Mrs. Shrividya" Age:"40" Salary: "30000"
Name:"Swaroop" Age:"25" Marks: "75"

Python always starts looking for methods in the actual type. If can not find, it will try parent classes.
.................................
file operation
r: read
w: write
a: append
t: text mode
b: binary mode
def reverse(text):
    return text[::-1]
def is_palindrome(text):
    return text == reverse(text)
something = input('Enter text: ')
if (is_palindrome(something)):
    print("Yes, it is a palindrome")
else:
    print("No, it is not a palindrome")
.................................
s = re.sub(r'[^\w]','',something) # to remove no word chars
if (is_palindrome(s.lower())):    # to make it all lower case 
    print("Yes, it is a palindrome")
else:
    print("No, it is not a palindrome")

poem = '''\
Programming is fun
When the work is done
if you wanna make your work also fun:
use Python!
'''
f = open('poem.txt', 'w') # open for 'w'riting
f.write(poem) # write text to file
f.close() # close the file
f = open('poem.txt') # if no mode is specified, 'r'ead mode is assumed by default
while True:
    line = f.readline()
    if len(line) == 0: # Zero length indicates EOF
		break
    print(line, end='')
f.close() # close the file
.................................
pickle

import pickle
# the name of the file where we will store the object
shoplistfile = 'shoplist.data'
# the list of things to buy
shoplist = ['apple', 'mango', 'carrot']
# Write to the file
f = open(shoplistfile, 'wb')
pickle.dump(shoplist, f) # dump the object to a file
f.close()
del shoplist # destroy the shoplist variable
# Read back from the storage
f = open(shoplistfile, 'rb')
storedlist = pickle.load(f) # load the object from the file
print(storedlist)
Output:
$ python3 pickling.py
['apple', 'mango', 'carrot']
.................................
Exception
try:
    text = input('Enter something --> ')
except EOFError:
    print('Why did you do an EOF on me?')
except KeyboardInterrupt:
    print('You cancelled the operation.')
else:
    print('You entered {0}'.format(text))

class ShortInputException(Exception):
    '''A user-defined exception class.'''
    def __init__(self, length, atleast):
		Exception.__init__(self)
		self.length = length
		self.atleast = atleast
try:
    text = input('Enter something --> ')
    if len(text) < 3:
	raise ShortInputException(len(text), 3) # raise here, catch later
    # Other work can continue as usual here
except EOFError:
    print('Why did you do an EOF on me?')
except ShortInputException as ex:
    print('ShortInputException: The input was {0} long, expected at least {1}'\
	.format(ex.length, ex.atleast))
else:
    print('No exception was raised.')
.................................
finally

import time
try:
    f = open('poem.txt')
    while True: # our usual file-reading idiom
		line = f.readline()
		if len(line) == 0:
			break
		print(line, end='')
	time.sleep(2) # To make sure it runs for a while
except KeyboardInterrupt:
    print('!! You cancelled the reading from the file.')
finally: # this part will run before the program close.
    f.close()
    print('(Cleaning up: Closed the file)')

Output:
$ python3 finally.py
Programming is fun
When the work is done
if you wanna make your work also fun:
!! You cancelled the reading from the file.
(Cleaning up: Closed the file)
.................................
The with statement
with open("poem.txt") as f:
    for line in f:
	print(line, end='')
File will automatically be closed by the "with open".
.................................
sys, warning module
>>> import sys
>>> sys.version_info

import sys, warnings
if sys.version_info.major < 3:
    warnings.warn("Need Python 3.0 for this program to run", RuntimeWarning)
else:
    print('Proceed as normal')

.................................
logging , os ,platform

>>> platform.platform().startswith('Linux') #the output string of platform.platform() has method startswith()
True

import os, platform, logging
if platform.platform().startswith('Windows'):
    #os.path.join() will ensure the path matches the format expected by the operating system.
    logging_file = os.path.join(os.getenv('HOMEDRIVE'), os.getenv('HOMEPATH'), 'test.log')
else:
    logging_file = os.path.join(os.getenv('HOME'), 'test.log')
print("Logging to", logging_file)

logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s : %(levelname)s : %(message)s',
    filename = logging_file,
    filemode = 'w',
)

logging.debug("Start of the program")
logging.info("Doing something")
logging.warning("Dying now")

.................................
>>> a, *b = [1, 2, 3, 4]
>>> a
1
>>> b
[2, 3, 4]

To switch values:
>>> a = 5; b = 8
>>> a, b = b, a
>>> a, b
(8, 5)

Return 2 values, a tuple, from a function
>>> def get_error_details():
... return (2, 'second error details')
...
>>> errnum, errstr = get_error_details()
>>> errnum
2
>>> errstr
'second error details'

special methods
__init__(self, ...) This method is called just before the newly created object is returned for usage.
__del__(self) Called just before the object is destroyed
__str__(self) Called when we use the print function or when str()is used.
__lt__(self, other) Called when the less than operator (<) is used. Similarly, there are special methods for all the operators (+, >, etc.)
__getitem__(self, key) Called when x[key] indexing operation is used.
__len__(self) Called when the built-in len() function is used for the sequence object.
.................................
Lambda Forms
# use lambda to create a new function to sort the output. 
# for lambda, input i output is i['y'], so it is ascending according to y's
# value 
points = [ { 'x' : 2, 'y' : 3 }, { 'x' : 4, 'y' : 1 } ]
points.sort(key=lambda i : i['y'])
print(points)
Output:
[{'x': 4, 'y': 1}, {'x': 2, 'y': 3}]


>>> def f (x): return x**2
... 
>>> print f(8)
64
>>> 
# x is input, out put is x**2
>>> g = lambda x: x**2
>>> 
>>> print g(8)
64
.................................
List Comprehension

listone = [2, 3, 4]
listtwo = [2*i for i in listone if i > 2]
print(listtwo)

Output:
$ python3 list_comprehension.py
[6, 8]
.................................
map, reduce
map: Return an iterator that applies function to every item of iterable, yielding the results. 
map is to apply a function to every item in the sequence, avoid a loop!
>>> list(map(upper, ['sentence', 'fragment']))
['SENTENCE', 'FRAGMENT']
Is the same as below:
>>> [upper(s) for s in ['sentence', 'fragment']]
['SENTENCE', 'FRAGMENT']

reduce: Apply function of two arguments cumulatively to the items of iterable, from left to right, so as to reduce the iterable to a single value. 
For example, reduce(lambda x, y: x+y, [1, 2, 3, 4, 5]) calculates ((((1+2)+3)+4)+5). 
It is similar to map; map to update each item, reduce is to use all item together. 

from operator import add
expr = "28+32+++32++39"
print reduce(add, map(int, filter(bool, expr.split("+")))) #Calling type int

To get the total length of a list of strings.

>>> ss = ["UA", "PyCon", "2012"]
>>> reduce(operator.add, map(len, ss))
11

#Python type is callable, explicit cast
>>> map(str, range(5))
['0', '1', '2', '3', '4']

#CLASSES ARE CALLABLE
>>> class Speaker(object):
...     def __init__(self, name):
...         self.name = name
>>> map(Speaker, ["Alexey", "Andrey", "Vsevolod"])
[<__main__.Speaker>, <__main__.Speaker>, <__main__.Speaker>]

#INSTANCE METHODS ARE CALLABLE
# counting the occurrence of 1,2,3,4,5 in the next list
>>> map([1,2,3,4,5].count, [1,2,3,10,11])
[1, 1, 1, 0, 0]
.................................
To avoid loop
#With loop
>>> name = None
>>> while name is None:
...    name = raw_input()
...    if len(name) < 2:
...        name = None

def get_name():
    name = raw_input()
    return name if len(name) >= 2 else get_name()

.................................
operator
>>> operator.add(1,2)
3
>>> operator.mul(3,10)
30
>>> operator.pow(2,3)
8
>>> operator.itemgetter(1)([1,2,3])
2

>>> from operator import itemgetter
>>> itemgetter(3)([1,2,3,4,5])
4

>>> from operator import attrgetter as attr
>>> class Speaker(object):
...     def __init__(self, name):
...         self.name = "[name] " + name
... 
>>> alexey = Speaker("Alexey")
>>> attr("name")(alexey)
'[name] Alexey'

>>> list(itertools.chain([1,2,3], [10,20,30])) # connect together
[1, 2, 3, 10, 20, 30]
>>> list(itertools.chain(*(map(xrange, range(5)))))
[0, 0, 1, 0, 1, 2, 0, 1, 2, 3]
>>> list(itertools.starmap(lambda k,v: "%s => %s" % (k,v), #a:1, b:2 becomes  a=>1, b=>2
...                        {"a": 1, "b": 2}.items()))
['a => 1', 'b => 2']
>>> list(itertools.imap(pow, (2,3,10), (5,2,3))) # 2^5, 3^2, 10^3
[32, 9, 1000]
#It is the same as below. imap works for infinite iterator arguments
>>> map(pow,(2,3,10), (5,2,3))

>>> dict(itertools.izip("ABCD", [1,2,3,4])) # merge a string and a list, 
#the string char become key, the list become values.
{'A': 1, 'C': 3, 'B': 2, 'D': 4}

To check if a key exist in a dictionary:
if 'l' in tree[2]: print "true"
.................................
current_speaker = {name: "Alexey Kachayev", talk: "FP with Python"}

def ask_question(question):
    print "{name}, I have question {question} about your {talk}"

def quit(reason):
    current_speaker = {name: "Andrey Svetlov"} # <-- this will fail
    current_speaker["talk"] = "Oh, boring..." # <-- mutable state
.................................
"Functional programming" wants to avoid state changes as much as possible and works with data flowing between functions. In Python you might combine the two approaches by writing functions that take and return instances representing objects in your application (e-mail messages, transactions, etc.).

A Python "iterator" must support a method called __next__() that takes no arguments and always returns the next element of the stream. If there are no more elements in the stream, __next__() must raise the StopIteration exception. Iterators don.t have to be finite, though; it.s perfectly reasonable to write an iterator that produces an infinite stream of data.
.................................
Generator expressions and list comprehensions

line_list = ['  line 1\n', 'line 2  \n', ...]

# Generator expression -- returns iterator
stripped_iter = (line.strip() for line in line_list)

# List comprehension -- returns list
stripped_list = [line.strip() for line in line_list]

stripped_list = [line.strip() for line in line_list if line != ""]

With a "list comprehension", you get back a Python list; stripped_list is a list containing the resulting lines, not an iterator. "Generator expressions" return an iterator that computes the values as necessary, not needing to materialize all the values at once. This means that list comprehensions aren.t useful if you.re working with iterators that return an infinite stream or a very large amount of data. Generator expressions are preferable in these situations.

Generator expressions are surrounded by parentheses (.().) and list comprehensions are surrounded by square brackets (.[].). Generator expressions have the form:

( expression for expr in sequence1
             if condition1
             for expr2 in sequence2
             if condition2
             for expr3 in sequence3 ...
             if condition3
             for exprN in sequenceN
             if conditionN )


"The elements of the generated output will be the successive values of expression." The if clauses are all optional; if present, expression is only evaluated and added to the result when condition is true.

Generator expressions always have to be written inside parentheses, but the parentheses signalling a function call also count. If you want to create an iterator that will be immediately passed to a function you can write: 

obj_total = sum(obj.count for obj in list_all_objects())

>>> seq1 = 'abc'
>>> seq2 = (1,2,3)
>>> [(x, y) for x in seq1 for y in seq2]  
[('a', 1), ('a', 2), ('a', 3),
 ('b', 1), ('b', 2), ('b', 3),
 ('c', 1), ('c', 2), ('c', 3)]
.................................
Generator
 the local variables weren.t thrown away on exiting a function? What if you could later resume the function where it left off? This is what generators provide; they can be thought of as resumable functions.

>>> def generate_ints(N):
...    for i in range(N):
...        yield i

Any function containing a yield keyword is a generator function; this is detected by Python.s bytecode compiler which compiles the function specially as a result.
yield return a generator.
>>> gen = generate_ints(3)
>>> gen  
<generator object generate_ints at ...>
>>> next(gen)
0
>>> next(gen)
1
>>> next(gen)
2
>>> next(gen)
Traceback (most recent call last):
  File "stdin", line 1, in ?
  File "stdin", line 2, in generate_ints
StopIteration

Inside a generator function, return value is semantically equivalent to raise StopIteration(value).
# A recursive generator that generates Tree leaves in in-order.
def inorder(t):
    if t:
        for x in inorder(t.left):
            yield x

        yield t.label

        for x in inorder(t.right):
            yield x

.................................
filter
filter(predicate, iter) returns an iterator over all the sequence elements that meet a certain condition, 
>>> list(filter(is_even, range(10)))
[0, 2, 4, 6, 8]
.................................
enumerate: to build index
enumerate(iter) counts off the elements in the iterable, returning 2-tuples containing the count and each element.

>>>
>>> for item in enumerate(['subject', 'verb', 'object']):
...     print(item)
(0, 'subject')
(1, 'verb')
(2, 'object')

enumerate() is often used when looping through a list and recording the indexes at which certain conditions are met:

f = open('data.txt', 'r')
for i, line in enumerate(f):
    if line.strip() == '':
        print('Blank line at line #%i' % i)
.................................
sorted
sorted(iterable, key=None, reverse=False) collects all the elements of the iterable into a list, sorts the list, and returns the sorted result. The key and reverse arguments are passed through to the constructed list.s sort() method.

>>>
>>> import random
>>> # Generate 8 random numbers between [0, 10000)
>>> rand_list = random.sample(range(10000), 8)
>>> rand_list  
[769, 7953, 9828, 6431, 8442, 9878, 6213, 2207]
>>> sorted(rand_list)  
[769, 2207, 6213, 6431, 7953, 8442, 9828, 9878]
>>> sorted(rand_list, reverse=True)  
[9878, 9828, 8442, 7953, 6431, 6213, 2207, 769]
.................................
any(iter) and all(iter)

The any(iter) and all(iter) built-ins look at the truth values of an iterable.s contents. any() returns True if any element in the iterable is a true value, and all() returns True if all of the elements are true values: 
>>>
>>> any([0,1,0])
True
>>> any([0,0,0])
False
>>> any([1,1,1])
True
>>> all([0,1,0])
False
>>> all([0,0,0])
False
>>> all([1,1,1])
True
.................................
zip(iterA, iterB, ...) takes one element from each iterable and returns them in a tuple:

zip(['a', 'b', 'c'], (1, 2, 3)) =>
  ('a', 1), ('b', 2), ('c', 3)
.................................
itertools

To get permutations and combinations

itertools.combinations(iterable, r)
itertools.permutations(iterable, r=None)

combination of 4
>>> s = [ 2 , 1, 0 , 0 , -1 , -2 ]
>>> print(list(itertools.combinations(s,4)))
[(2, 1, 0, 0), (2, 1, 0, -1), (2, 1, 0, -2), (2, 1, 0, -1), (2, 1, 0, -2), (2,
1, -1, -2), (2, 0, 0, -1), (2, 0, 0, -2), (2, 0, -1, -2), (2, 0, -1, -2), (1,
0, 0, -1), (1, 0, 0, -2), (1, 0, -1, -2), (1, 0, -1, -2), (0, 0, -1, -2)]

>>> print(list(itertools.permutations(s,4)))
......................
itertools.count(n) returns an infinite stream of integers, increasing by 1
each time. You can optionally supply the starting number, which defaults to 0:

itertools.count() =>
  0, 1, 2, 3, 4, 5, 6, 7, 8, 9, ...
itertools.count(10) =>
  10, 11, 12, 13, 14, 15, 16, 17, 18, 19, ...
...........
itertools.cycle(iter) saves a copy of the contents of a provided iterable and returns a new iterator that returns its elements from first to last. The new iterator will repeat these elements infinitely.  

itertools.cycle([1,2,3,4,5]) =>
  1, 2, 3, 4, 5, 1, 2, 3, 4, 5, ...
itertools.repeat(elem, [n]) returns the provided element n times, or returns
the element endlessly if n is not provided.

itertools.repeat('abc') =>
  abc, abc, abc, abc, abc, abc, abc, abc, abc, abc, ...
itertools.repeat('abc', 5) =>
  abc, abc, abc, abc, abc
...........
itertools.chain(iterA, iterB, ...) takes an arbitrary number of iterables as input, and returns all the elements of the first iterator, then all the elements of the second, and so on, until all of the iterables have been exhausted.

itertools.chain(['a', 'b', 'c'], (1, 2, 3)) =>
  a, b, c, 1, 2, 3
...........
itertools.islice(iter, [start], stop, [step]) returns a stream that.s a slice of the iterator. With a single stop argument, it will return the first stop elements. If you supply a starting index, you.ll get stop-start elements, and if you supply a value for step, elements will be skipped accordingly. Unlike Python.s string and list slicing, you can.t use negative values for start, stop, or step.

itertools.islice(range(10), 8) =>
  0, 1, 2, 3, 4, 5, 6, 7
itertools.islice(range(10), 2, 8) =>
  2, 3, 4, 5, 6, 7
itertools.islice(range(10), 2, 8, 2) =>
  2, 4, 6
...........
itertools.tee(iter, [n]) replicates an iterator; it returns n independent iterators that will all return the contents of the source iterator. If you don.t supply a value for n, the default is 2. Replicating iterators requires saving some of the contents of the source iterator, so this can consume significant memory if the iterator is large and one of the new iterators is consumed more than the others.

itertools.tee( itertools.count() ) =>
   iterA, iterB

where iterA ->
   0, 1, 2, 3, 4, 5, 6, 7, 8, 9, ...

and   iterB ->
   0, 1, 2, 3, 4, 5, 6, 7, 8, 9, ...
.................................
The operator module contains a set of functions corresponding to Python.s operators. Some examples are 
operator.add(a, b) (adds two values), 
operator.ne(a, b) (same as a != b), and 
operator.attrgetter('id') (returns a callable that fetches the .id attribute).

......................
"itertools.starmap(func, iter)" assumes that the iterable will return a stream of tuples, and calls func using these tuples as the arguments:
It can work directly on elements within a tuple. So, each tuple got one result.

itertools.starmap(os.path.join,
                  [('/bin', 'python'), ('/usr', 'bin', 'java'),
                   ('/usr', 'bin', 'perl'), ('/usr', 'bin', 'ruby')])
=>
  /bin/python, /usr/bin/java, /usr/bin/perl, /usr/bin/ruby
.................................
itertools.filterfalse(predicate, iter) is the opposite of filter(), returning
all elements for which the predicate returns false:

itertools.filterfalse(is_even, itertools.count()) =>
  1, 3, 5, 7, 9, 11, 13, 15, ...
.................................
itertools.takewhile(predicate, iter) returns elements for as long as the
predicate returns true. Once the predicate returns false, the iterator will
signal the end of its results.

def less_than_10(x):
    return x < 10

itertools.takewhile(less_than_10, itertools.count()) =>
  0, 1, 2, 3, 4, 5, 6, 7, 8, 9

itertools.takewhile(is_even, itertools.count()) =>
  0
.................................
itertools.dropwhile(predicate, iter) discards elements while the predicate
returns true, and then returns the rest of the iterable.s results.

itertools.dropwhile(less_than_10, itertools.count()) =>
  10, 11, 12, 13, 14, 15, 16, 17, 18, 19, ...

itertools.dropwhile(is_even, itertools.count()) =>
  1, 2, 3, 4, 5, 6, 7, 8, 9, 10, ...
.................................
itertools.compress(data, selectors) takes two iterators and returns only those
elements of data for which the corresponding element of selectors is true,
stopping whenever either one is exhausted:

itertools.compress([1,2,3,4,5], [True, True, False, False, True]) =>
   1, 2, 5
.................................
In python 2.7 
The "itertools.combinations_with_replacement(iterable, r)" function relaxes a
different constraint: elements can be repeated within a single tuple.
Conceptually an element is selected for the first position of each tuple and
then is replaced before the second element is selected.

itertools.combinations_with_replacement([1, 2, 3, 4, 5], 2) =>
  (1, 1), (1, 2), (1, 3), (1, 4), (1, 5),
  (2, 2), (2, 3), (2, 4), (2, 5),
  (3, 3), (3, 4), (3, 5),
  (4, 4), (4, 5),
  (5, 5)
.................................
groupby
 itertools.groupby(iter, key_func=None)
key_func(elem) is a function that can compute a key value for each element returned by the iterable. If you don.t supply a key function, the key is simply each element itself.

groupby() collects all the consecutive elements from the underlying iterable
that have the same key value, and returns a stream of 2-tuples containing a
key value and an iterator for the elements with that key.

city_list = [('Decatur', 'AL'), ('Huntsville', 'AL'), ('Selma', 'AL'),
             ('Anchorage', 'AK'), ('Nome', 'AK'),
             ('Flagstaff', 'AZ'), ('Phoenix', 'AZ'), ('Tucson', 'AZ'),
             ...
            ]

def get_state(city_state):
    return city_state[1]

itertools.groupby(city_list, get_state) =>
  ('AL', iterator-1),
  ('AK', iterator-2),
  ('AZ', iterator-3), ...

where
iterator-1 =>
  ('Decatur', 'AL'), ('Huntsville', 'AL'), ('Selma', 'AL')
iterator-2 =>
  ('Anchorage', 'AK'), ('Nome', 'AK')
iterator-3 =>
  ('Flagstaff', 'AZ'), ('Phoenix', 'AZ'), ('Tucson', 'AZ')

groupby() assumes that the underlying iterable.s contents will already be sorted based on the key. Note that the returned iterators also use the underlying iterable, so you have to consume the results of iterator-1 before requesting iterator-2 and its corresponding key.
.................................
To get all of the combination of 4, which add to 0.
import itertools
s = [ 2, 1, 0, 0, -1 , -2 ]
def detector(a, b, c, d) :
    if a+b+c+d == 0 :
	print 'got one: ', (a, b, c, d)

list(itertools.starmap(detector, list(itertools.combinations(s,4))))
.................................
ls -l json dump procedure
#!/usr/bin/python
import commands
import json
import time
from itertools import *
d = 'ls -l /home/python_code'
(status, output) = commands.getstatusoutput(d)
s = output.split('\n')[1::]
jd = []
with open("/home/python_code/ls.dump", 'w') as f:
    for i in xrange(len(s)) :
        list1 = s[i].split()
        list1[4] = int(list1[4])
        list1.insert(0,int(time.strftime('%Y%m%d%H%M')))
        jd.append(dict(izip(['TS', 'perm','hlink','owner','group','size','mmon','mdate','mtime','name'], list1)))
    json.dump(jd, f)
    f.flush()

# need to use json.dump at the last, so it takes care of the "," in between
# each dictionary
.................................
functools
.................................
pymongo

import pymongo
from pymongo import MongoClient
client = MongoClient()

>>> client = MongoClient('localhost', 27017)
>>> db = client.test_database
or
>>> db = client['test-database']

>>> collection = db.test_collection
or
>>> collection = db['test-collection']

>>> import datetime
>>> post = {"author": "Mike",
...         "text": "My first blog post!",
...         "tags": ["mongodb", "python", "pymongo"],
...         "date": datetime.datetime.utcnow()}
>>> posts = db.posts
>>> post_id = posts.insert(post)
>>> post_id
ObjectId('...')

When a document is inserted a special key, "_id", is automatically added if the document doesn.t already contain an "_id" key. 
sting all of the collections in our database:

>>> db.collection_names()
[u'system.indexes', u'posts']
Note The system.indexes collection is a special internal collection that was created automatically.

>>> posts.find_one()
{u'date': datetime.datetime(...), u'text': u'My first blog post!', u'_id': ObjectId('...'), u'author': u'Mike', u'tags': [u'mongodb', u'python', u'pymongo']}

>>> posts.find_one({"author": "Mike"})
{u'date': datetime.datetime(...), u'text': u'My first blog post!', u'_id': ObjectId('...'), u'author': u'Mike', u'tags': [u'mongodb', u'python', u'pymongo']}

>>> posts.find_one({"author": "Eliot"})
............................................
#!/usr/bin/python
import pymongo
from pymongo import MongoClient
import json
with open('/home/python_code/ls.dump', 'r') as f :
    jd = json.load(f)
client = MongoClient('localhost', 27017)
db = client.test_database
post_id = db.posts.insert(jd)


#!/usr/bin/python
import pymongo
from pymongo import MongoClient
client = MongoClient('localhost', 27017)
db = client.test_database
print(db.posts.find_one({"name":"4_numbers.py"}))
print(db.collection_names())
.................................
>>> post_id
ObjectId(...)
>>> posts.find_one({"_id": post_id})
{u'date': datetime.datetime(...), u'text': u'My first blog post!', u'_id': ObjectId('...'), u'author': u'Mike', u'tags': [u'mongodb', u'python', u'pymongo']}

Note that an ObjectId is not the same as its string representation:

>>> post_id_as_str = str(post_id)
>>> posts.find_one({"_id": post_id_as_str}) # No result
>>>

To convert the ObjectId from a string before passing it to find_one:

from bson.objectid import ObjectId

# The web framework gets post_id from the URL and passes it as a string
def get(post_id):
    # Convert from string to ObjectId:
    document = client.db.collection.find_one({'_id': ObjectId(post_id)})  # note:  ObjectId(post_id)})  


MongoDB stores data in BSON format. BSON strings are "UTF-8" encoded so PyMongo must ensure that any strings it stores contain only valid UTF-8 data.
.................................
To list all records
>>> for post in posts.find():
...   post
...

>>> for post in posts.find({"author": "Mike"}):
...   post
...

TO put in a list
list(db.posts.find())

>>> posts.count()
3
>>> posts.find({"author": "Mike"}).count()
2
>>> db.posts.find({"owner":"root"}).count()
15
>>> db.posts.find({"name":"ttt"}).count()
1

list(db.posts.find({"owner":"root"}).sort("name"))

d =  list(db.posts.find({"name":"ttt"}))
>>> d[0]["name"]
u'ttt'

To find all posts before the date
>>> d = datetime.datetime(2009, 11, 12, 12)
>>> for post in posts.find({"date": {"$lt": d}}).sort("author"):
...   print post
...

>>> posts.find({"date": {"$lt": d}}).sort("author").explain()["cursor"]
u'BasicCursor'
>>> posts.find({"date": {"$lt": d}}).sort("author").explain()["nscanned"]
3
>>> db.posts.find({"size":{"$gt": 200}}).sort("name").explain()["cursor"]
u'BasicCursor'
>>> db.posts.find({"size":{"$gt": 200}}).sort("name").explain()["nscanned"]
15
list(db.posts.find({"size":{"$lt":100}}))

Add a compound index and look at the same information:
>>> from pymongo import ASCENDING, DESCENDING
>>> posts.create_index([("date", DESCENDING), ("author", ASCENDING)])
u'date_-1_author_1'
>>> posts.find({"date": {"$lt": d}}).sort("author").explain()["cursor"]
u'BtreeCursor date_-1_author_1'
>>> posts.find({"date": {"$lt": d}}).sort("author").explain()["nscanned"]
2
Now the query is using a BtreeCursor (the index) and only scanning over the 2 matching documents.
.................................
pydoc string
python -m pydoc pymongo
.................................
virutalenv
pip install virtualenv

Run this with non-root user:
virtualenv --no-site-packages myenv
cd myenv
bin/easy_install pymongo
bin/python -c import pymongo

To use virutalevn:
cd myenv
. ./bin/activate

# with write concern option
connection = Connection('localhost', 27017, w=3, j=True)

""" An example of how to connect to MongoDB """
import sys
from pymongo import Connection
from pymongo.errors import ConnectionFailure
def main():
    """ Connect to MongoDB """
    try:
	c = Connection(host="localhost", port=27017)
	print "Connected successfully"
    except ConnectionFailure, e:
	sys.stderr.write("Could not connect to MongoDB: %s" % e)
	sys.exit(1)
    # Get a Database handle to a database named "mydb"
    dbh = c["mydb"]
    # Demonstrate the db.connection property to retrieve a reference to the
    # Connection object should it go out of scope. In most cases, keeping a
    # reference to the Database object for the lifetime of your program should
    # be sufficient.
    assert dbh.connection == c
    print "Successfully set up a database handle"

if __name__ == "__main__":
    main()
.................................
Assert statements are a convenient way to insert debugging assertions into a program:

assert_stmt ::=  "assert" expression ["," expression]
The simple form, assert expression, is equivalent to

if __debug__:
   if not expression: raise AssertionError
The extended form, assert expression1, expression2, is equivalent to

if __debug__:
   if not expression1: raise AssertionError(expression2)
............................................
Collections are created lazily in MongoDB, whenever you access them.
To generate objectID
pymongo.objectid.ObjectId

Unless you are certain you don.t need synchronous writes, we recommend that
you pass the .safe=True. keyword argument to inserts, updates, removes and
findAndModify operations:
# safe=True ensures that your write
# will succeed or an exception will be thrown
safe=True is deprecated, use write concern instead

dbh.users.insert(user_doc, safe=True)

connection = Connection('localhost', 27017, w=3, j=True)
Passing w=0 disables write acknowledgement and all other write concern options.
w=1 is the new safe=True and it is now the default. Other options like j=True or w=3 work the same as before. You can still set options per-operation: 

client.db.collection.insert({'foo': 'bar'}, w=1)
ReplicaSetConnection is also obsolete, of course, and succeeded by MongoReplicaSetClient.

# w=2 means the write will not succeed until it has
# been written to at least 2 servers in a replica set.
#Note that passing any value of .w. to a write method in PyMongo implies setting .safe=True. also.
dbh.users.insert(user_doc, w=2)


MongoDB treats each property as having an implicit boolean AND. It natively supports boolean OR queries, but you must use a special operator ($or) to achieve it. 

Query firstname == jane and surname == doe
q = {
    "firstname" : "jane",
    "surname" : "doe"
}
q = {
    "score" : { "$gt" : 0 }
}

# Assuming we already have a database handle in scope named dbh
# find a single document with the username "janedoe".
user_doc = dbh.users.find_one({"username" : "janedoe"})
if not user_doc:
    print "no document found for username janedoe"
#Notice that find_one()will return Noneif no document is found.

# Assuming we already have a database handle in scope named dbh
# find all documents with the firstname "jane".
# Then iterate through them and print out the email address.
users = dbh.users.find({"firstname":"jane"})
for user in users:
    print user.get("email") # or user["email"]

list(db.posts.find({"owner":"root"})) # to list everything

# Only retrieve the "email" field from each matching document.
# db.collection.find( <query>, <projection> )
The <projection> argument describes the result set in the form of a document.  Projections specify or limit the fields to return.
users = dbh.users.find({"firstname":"jane"}, {"email":1})
for user in users:
    print user.get("email")

docs = db.posts.find({"owner":"root"},{"size":1}) # only get 1 field
for d in docs: 
    print d.get("size")

list(db.posts.find({"owner":"root"},{"size":1}))
list(db.posts.find({"owner":"root"},{"size":1,"hlink":1}))

# Find out how many documents are in users collection, efficiently
userscount = dbh.users.find().count()
print "There are %d documents in users collection" % userscount

# Return all user with firstname "jane" sorted
# in descending order by birthdate (ie youngest first) Should be only one () for sort?
users = dbh.users.find({"firstname":"jane"}).sort(("dateofbirth", pymongo.DESCENDING))
for user in users:
    print user.get("email")

list(db.posts.find({"owner":"root"},{"size":1,"name":1}).sort("size", pymongo.ASCENDING))

# Return all user with firstname "jane" sorted
# in descending order by birthdate (ie youngest first)
users = dbh.users.find({"firstname":"jane"}, sort=[("dateofbirth", pymongo.DESCENDING)])
for user in users:
    print user.get("email")

# Return at most 10 users sorted by score in descending order
# This may be used as a "top 10 users highscore table"
users = dbh.users.find().sort(("score", pymongo.DESCENDING)).limit(10)
for user in users:
    print user.get("username"), user.get("score", 0)

list(db.posts.find({"owner":"root"},{"size":1,"name":1}).sort("size", pymongo.ASCENDING).limit(3))

To find all the value of "size" is string type:
list(db.posts.find({"size": {'$type': 2}},{"size":1}))

Double	1
String	2
Object	3
Array	4
Binary data	5
Undefined (deprecated)	6
Object id	7
Boolean	8
Date	9
Null	10

To find everything and list the 2 fields:
list(db.posts.find({},{"name":1,"size":1}))

# Return at most 20 users sorted by name,
# skipping the first 20 results in the set
users = dbh.users.find().sort("surname", pymongo.ASCENDING).limit(20).skip(20)


MongoDB.s Snapshot Mode guarantees that documents which are modified during the lifetime of a query are returned only once in a result set. In other words, duplicates are eliminated, and you should not have to worry about them.
However, Snapshot Mode does have some limitations. Snapshot Mode cannot be used with sorting, nor can it be used with an index on any property other than _id.

# Traverse the entire users collection, employing Snapshot Mode
# to eliminate potential duplicate results.
for user in dbh.users.find(snapshot=True):
    print user.get("username"), user.get("score", 0)
.................................
To update the whole record
# first query to get a copy of the current document
import copy
old_user_doc = dbh.users.find_one({"username":"janedoe"})
new_user_doc = copy.deepcopy(old_user_doc)
# modify the copy to change the email address
new_user_doc["email"] = "janedoe74@example2.com"
# run the update query
# replace the matched document with the contents of new_user_doc
dbh.users.update({"username":"janedoe"}, new_user_doc, safe=True)
.................................
Write Concern options

update modifiers
# run the update query, using the $set update modifier.
# we do not need to know the current contents of the document
# with this approach, and so avoid an initial query and
# potential race condition.
dbh.users.update({"username":"janedoe"}, {"$set":{"email":"janedoe74@example2.com"}}, safe=True)

# update the email address and the score at the same time
# using $set in a single write.
dbh.users.update({"username":"janedoe"}, {"$set":{"email":"janedoe74@example2.com", "score":1}}, safe=True)
It only update the first record it finds.

# once we supply the "multi=True" parameter, all matched documents  will be updated
dbh.users.update({"score":0},{"$set":{"flagged":True}}, multi=True, safe=True)

db.posts.update({"owner":"root"},{"$set": {"owner":'zwa'}}, w=1, multi=True)


# Delete all documents in user collection with score 1
dbh.users.remove({"score":1}, safe=True)

# To find and remove all docs, which size is in string; and then list the 2 fields of all docs 
list(db.posts.find({"size": {'$type': 2}},{"size":1}))
db.posts.remove({"size": {'$type': 2}},w=1)
list(db.posts.find({},{"name":1,"size":1}))

# Delete all documents in user collection
dbh.users.remove(None, safe=True)
Clearing a collection with remove()differs from dropping the collection via drop_collection()in that the indexes will remain intact.

Table 2-2. MongoDB query operators
Operator Meaning 		Example 						SQL Equivalent
$gt 	Greater Than 		"score":{"$gt":0} 					>
$lt 	Less Than 		"score":{"$lt":0} 					<
$gte 	Greater Than or Equal 	"score":{"$gte":0} 					>=
$lte 	Less Than or Equal 	"score":{"$lte":0} 					.
$all 	Array Must Contain All 	"skills":{"$all":["mongodb","python"]} 			N/A
$exists Property Must Exist 	"email":{"$exists":True} 				N/A
$mod 	Modulo X Equals Y 	"seconds":{"$mod":[60,0]} 				MOD()
$ne 	Not Equals 		"seconds":{"$ne":60} 					!=
$in 	In 			"skills":{"$in":["c","c++"]} 				IN
$nin 	Not In 			"skills":{"$nin":["php","ruby","perl"]} 		NOT IN
$nor 	Nor 			"$nor":[{"language":"english"},{"country":"usa"}] 	N/A
$or 	Or 			"$or":[{"language":"english"},{"country":"usa"}] 	OR
$size 	Array Must Be Of Size 	"skills":{"$size":3} 					N/A

Table 2-3. MongoDB update modifiers
Modifier 	Meaning 				Example
$inc 		Atomic Increment 			"$inc":{"score":1}
$set 		Set Property Value 			"$set":{"username":"niall"}
$unset 		Unset (delete) Property	 		"$unset":{"username":1}
$push 		Atomic Array Append (atom) 		"$push":{"emails":"foo@example.com"}
$pushAll 	Atomic Array Append (list) 		"$pushall":{"emails":["foo@example.com","foo2@example.com"]}
$addToSet 	Atomic Append-If-Not-Present 		"$addToSet":{"emails":"foo@example.com"} 
$pop 		Atomic Array Tail Remove 		"$pop":{"emails":1}
$pull 		Atomic Conditional Array Item Removal 	"$pull":{"emails":"foo@example.com"}
$pullAll 	Atomic Array Multi Item Removal 	"$pullAll":{"emails":["foo@example.com", "foo2@example.com"]}
$rename 	Atomic Property Rename 			"$rename":{"emails":"old_emails"}

.................................
Embeded document
user_doc = {
    "username":"foouser",
    "twitter":{
	"username":"footwitter",
	"password":"secret",
	"email":"twitter@example.com"
    },
    "facebook":{
	"username":"foofacebook",
	"password":"secret",
	"email":"facebook@example.com"
    },
    "irc":{
	"username":"fooirc",
	"password":"secret",
    }
}

embedded sub-documents can be queried against just like their top-level counterparts.
user_doc = dbh.users.find_one({"facebook.username":"foofacebook"})


The dot notation can also be used in update statements with update modifiers such as $set to set the value of an individual sub-property:

# update modifiers such as $set also support the dot notation
dbh.users.update({"facebook.username":"foofacebook"},
    {"$set":{"facebook.username":"bar"}}, safe=True)

# A user document demonstrating one-to-many relationships using embedding
# Here we map multiple email addresses (along with whether or not the email
# is the user's primary email address) to a single user.
user_doc = {
    "username":"foouser",
    "emails":[
	{
	"email":"foouser1@example.com",
	"primary":True
	},
	{
	"email":"foouser2@example2.com",
	"primary":False
	},
	{
	"email":"foouser3@example3.com",
	"primary":False
	}
    ]
}
# Insert the user document
dbh.users.insert(user_doc, safe=True)
# Retrieve the just-inserted document via one of its many email addresses
user_doc_result = dbh.users.find_one({"emails.email":"foouser1@example.com"})
# Assert that the original user document and the query result are the same
assert user_doc == user_doc_result
# Remove the "foouser2@example2.com" email address sub-document from the  embedded list
del user_doc_result["emails"][1]
# Now write the new emails property to the database
# May cause data to be lost due to the race between read and write  ---
dbh.users.update({"username":"foouser"},{"$set":{"emails":user_doc_result}}, safe=True)

# Atomically remove an email address from a user document race-free using the
# $pull update modifier
# Use $pull to atomically remove the "foouser2@example2.com" email
# sub-document
dbh.users.update({"username":"foouser"}, {"$pull":{"emails":{"email":"foouser2@example2.com"}}}, safe=True)

"$pull" will remove the entire document from the array in an atomic fashion, meaning there is no opportunity for a race condition.

# Use $pull to atomically remove all email sub-documents with primary not equal to True
dbh.users.update({"username":"foouser"}, {"$pull":{"emails":{"primary":{"$ne":True}}}, safe=True)


The "$push" update modifier is used to atomically append an element to an array. To the end of an arry only.

# Use $push to atomically append a new email sub-document to the user document
new_email = {"email":"fooemail4@exmaple4.com", "primary":False}
dbh.users.update({"username":"foouser"}, {"$push":{"emails":new_email}}, safe=True)


# Insert the user document
dbh.users.insert(user_doc, safe=True)
# Now make the "foouser2@example2.com" email address primrary
dbh.users.update({"emails.email":"foouser2@example2.com"}, {"$set":{"emails.$.primary":True}}, safe=True)
# Now make the "foouser1@example.com" email address not primary
dbh.users.update({"emails.email":"foouser1@example.com"}, {"$set":{"emails.$.primary":False}}, safe=True)

Note that the $ operator cannot be used with upserts (see section on upserts later in this chapter) additionally it only works with the first matched element.

.................................
Indexing
MongoDB offers two kinds of indexes out-of-the-box: Btree indexes and geospatial indexes. The btree indexes in MongoDB are much the same as the equivalents in MySQL or PostgreSQL.
MongoDB indexes can span multiple fields (a.k.a. compound indexes), just like RDBMS.
In MongoDB, btree indexes can have a "direction". This direction is only useful in the case of compound indexes, where the index direction should match the sort direction or range query direction for optimal performance. For example, if you are querying a range (say, A through C) on first name and last name and then sorting in ascending order on last name, your compound index direction should also be ascending.
Using a btree index will incur a performance hit on writes, as the database must now update the index in addition to the data. Indeces take memory space. Your database will run fastest when it resides entirely in memory, and indexes can considerably add to its size. 
Btree indexes also transparently support indexing multi-value properties, that is, properties where the value is an array.
# If we place an index on property "emails.email",
# e.g. dbh.users.create_index("emails.email")
# this find_one query can use a btree index
user = dbh.users.find_one({"emails.email":"foouser2@example2.com"})

Btree indexes in MongoDB are also important when performing server-side sorting of results. Without an index on the property you are sorting by, MongoDB will run out of memory when trying to sort anything greater than a relatively small results set (approx. 4Mb at time of writing). If you expect that you will be sorting result sets larger than 4Mb, you should specify an index on the sort key. It is easy to underestimate this and find exceptions are being raised on queries against larger, real-world data which were not anticipated during development.

For a singlekey index, only the key needs to be provided. A compound index is slightly more complicated.a list of 2-tuples (key, direction) must be supplied.
# Create index on username property
dbh.users.create_index("username")
# Create a compound index on first_name and last_name properties
# with ascending index direction
dbh.users.create_index([("first_name", pymongo.ASCENDING), ("last_name", pymongo.ASCENDING)])

 To give a custom name during creation, supply the name=<str>parameter to the create_index()method:
# Create a compound index called "name_idx" on first_name and last_name properties
# with ascending index direction
dbh.users.create_index([
    ("first_name", pymongo.ASCENDING),
    ("last_name", pymongo.ASCENDING)
    ],
    name="name_idx")

It should be noted that index creation locks the database by default.
Building  an  index  in  the  background may take slightly longer, and will still cause additional load on the system, but the database should otherwise remain available.
# Create index in the background
# Database remains usable
dbh.users.create_index("username", background=True)

MongoDB btree indexes can be used to enforce a uniqueness constraint on a particular property.
# Create index with unique constraint on username property
dbh.users.create_index("username", unique=True)

This means that when a unique constraint is added to a btree index in MongoDB, the database will prevent you from having multiple documents in the collection which are missing the indexed property. Because all missing properties are null.
For example, if you have created a unique index for the usernameproperty in a users collection, only one document in that collection may be permitted to lack a username property. 

# Create index with unique constraint on username property
# instructing MongoDB to drop all duplicates after the first document it finds.
dbh.users.create_index("username", unique=True, drop_dups=True)
# Could equally be written:
dbh.users.create_index("username", unique=True, dropDups=True)

# Create index on username property called "username_idx"
dbh.users.create_index("username", name="username_idx")
# Delete index called "username_idx"
dbh.users.drop_index("username_idx")

# Create a compound index on first_name and last_name properties
# with ascending index direction
dbh.users.create_index([("first_name", pymongo.ASCENDING), ("last_name", pymongo.ASCENDING)])
# Delete this index
dbh.users.drop_index([("first_name", pymongo.ASCENDING), ("last_name", pymongo.ASCENDING)])

All  indexes  in  a  collection  can  be  dropped  in  a  single  statement using  the  method:
Collection.drop_indexes() 

use the Collection.index_information()method. This returns a dictionary in which each key is the name of an index.
>>> db.posts.index_information()
{u'_id_': {u'key': [(u'_id', 1)], u'v': 1}}
The value associated with each key is an additional  dictionary.  These  second-level  dictionaries  always  contain  a special  key called key, which is an entry containing the original index specifier.including index direction.  This  original  index  specifier  was  the  data  passed  to  the create_index() method when the index was first created. The second-level dictionaries may also contain additional options such as unique constraints and so on.

.................................
Geospatial indexing
MongoDB uses geohashing, a public domain algorithm developed by Gustavo Niemeyer, which translates geographic proximity into lexical proximity. Hence, a database supporting range queries (such as MongoDB) can be efficiently used to query for points near and within bounds.
MongoDB  provides  the  "$near" and  "$within" operators  which  constitute  the primary means for performing geospatial queries in the system. Using "$near", you can efficiently sort documents in a collection by their proximity to a given point. The "$within" operator allows you to specify a bounds for the query. Supported boundary definitions include "$box" for a rectangular shape,  "$circle" for a circle. In MongoDB 1.9 and up, the "$polygon" operator allows for convex and concave polygon boundaries.

Before you can use the geospatial queries, you must have a geospatial index.  Each document can have only one location property queried  efficiently  by  MongoDB.  

Geospatial indexes by default limit acceptable values for the location property on documents to those within GPS. That is, co-ordinates must be in the range -180 .. +180. If you have co-ordinates outside of this range, MongoDB will raise an exception when you attempt to create the geospatial index on the colleciton. If you wish to index values outside of the range of regular GPS, you can specify this at index creation time.

# location property is an array with x,y ordering
user_doc = {
    "username":"foouser",
    "user_location":[x,y]
}
# location property is an array with y,x ordering
user_doc = {
    "username":"foouser",
    "user_location":[y,x]
}
import bson
# location property is a sub-document with y,x ordering
loc = bson.SON()
loc["y"] = y
loc["x"] = x
user_doc = {
    "username":"foouser",
    "user_location":loc
}
# Create geospatial index on "user_location" property.
dbh.users.create_index([("user_location", pymongo.GEO2D)])
# Create geospatial index on "user_location" property.
dbh.users.create_index([("user_location", pymongo.GEO2D), ("username", pymongo.ASCENDING)])

"$near" will sort query results by proximity to specified point. By default, $nearwill try to find the closest 100 results.
using "$near", you will almost always want to specify a maximum distance on the query.
In most cases, a max distance of around 5 degrees should be sufficient. Since we are using decimal degrees (a.k.a GPS) co-ordinates, the units of max distance is degrees. 1 degree is roughly 69 miles. If you only care about a relatively small set of results (for example, the nearest 10 coffee shops), limiting the query to 10 results should also aid performance.
# Find the 10 users nearest to the point 40, 40 with max distance 5 degrees
nearest_users = dbh.users.find(
    {"user_location":
    {"$near" : [40, 40],
    "$maxDistance":5}}).limit(10)
# Print the users
for user in nearest_users:
# assume user_location property is array x,y
    print "User %s is at location %s,%s" %(user["username"], user["user_location"][0], user["user_location"[1])

Next let us try using the "$within" geospatial operator to find points within a certain boundary. This can be useful when searching for POI.s in a specific county/city or even well-defined neighbourhood within a city. 

To specify a rectangle to search within, you simply provide the lower-left and top-right co-ordinates as elements in an array. For example:
box = [[50.73083, -83.99756], [50.741404, -83.988135]]
users_in_boundary = dbh.users.find({"user_location":{"$within": {"$box":box}}})

Here is how we could make a geospatial lookup for 10 users within a radius of 5 degress centered at the point 40, 40:
users_in_circle = dbh.users.find({"user_location":{"$within":{"$center":[40, 40, 5]}}}).limit(10)

MongoDB.s spherical model has a few extra caveats. First and foremost, you must use (longitude, latitude) ordering of your co-ordinates.  the units for distances with  $nearSphereand  $centerSphereare always expressed in radians. This includes when using  $maxDistancewith  $nearSphereor $centerSphere.
To translate from kilometers to radians, simply divide the kilometer value by the radius of the earth which is approximately 6371 km (or 3959 miles).
# Find the 10 users nearest to the point 40, 40 with max distance 5 degrees
# Uses the spherical model provided by MongoDB 1.8.x and up
earth_radius_km = 6371.0
max_distance_km = 5.0
max_distance_radians = max_distance_km / earth_radius_km
nearest_users = dbh.users.find(
    {"user_location":
    {"$nearSphere" : [40, 40],
    "$maxDistance":max_distance_radians}}).limit(10)
# Print the users
for user in nearest_users:
# assume user_location property is array x,y
print "User %s is at location %s,%s" %(user["username"], user["user_location"][0], user["user_location"[1])

The Python dictclass. getmethod easily lets you specify a default value for a property should it be missing.
total_score = 0
for username in ("jill", "sam", "cathy"):
    user_doc = dbh.users.find_one({"username":username})
    total_score += user_doc.get("score", 0)

# Email each supplier of this product.
# Default value is the empty list so no special casing
# is needed if the suppliers property is not present.
for supplier in product_doc.get("suppliers", []):
    email_supplier(supplier)

# Perfectly legal insert - MongoDB will not complain
dbh.users.insert({"username":"janedoe"})
# Also perfectly legal - MongoDB will not complain
dbh.users.insert({"username":l337})

 Thus, in the context of a Web application, validating and/or coercing the types of any inputs to write queries before issuing them is strongly advised. You may consider using the FormEncode or Colander Python libraries to help with such validation.

.................................
Update-or-Insert: Upserts in MongoDB
This is the essence of an upsert: If a document already exists, update it.  Otherwise create a new document.

Collection.save(),  Collection.update() and  Collection.find_and_modify()

"save()" offers almost identical functionality to insert()with the following exceptions: 
"save()" can perform upserts and 
"save()" cannot insert multiple documents in a single call.

save() only works with _id property.
save()is quite easy to understand. If you pass it a document without an _id property it will perform an  insert(), creating a brand new document. If, on the other hand, you pass it a document which contains an  _idproperty it will update the existing document corresponding to that  _idvalue, overwriting the already-present document with the document passed to save().

# Good implementation using upsert=True
# If session_id exists, update, otherwise insert.
def edit_or_add_session(description, session_id):
    dbh.sessions.update({"session_id":session_id}, {"$set":{"session_description":description}}, safe=True, upsert=True)

To avoid race condition, where a document is modified during an update and read operation. Need a way to update the account balance and return the new value in a single, atomic operation. 
# User X adds $20 to his/her account, so we atomically increment
# account_balance and return the resulting document
ret = dbh.users.find_and_modify({"username":username}, {"$inc":{"account_balance":20}}, safe=True, new=True)
new_account_balance = ret["account_balance"]

# Store weekly scores in sub-document
user_doc = {
    "scores_weekly":{
	"2011-01":10,
	"2011-02":3,
	"2011-06":20
    }
}
# Fetch the score for the current week
import datetime
now = datetime.datetime.utcnow()
current_year = now.year
current_week = now.isocalendar()[1]
# Default missing keys to a score of zero
user_doc["scores_weekly"].get("%d-%d" %(current_year, current_week), 0)

# Update the score for the current week
import datetime
username = "foouser"
now = datetime.datetime.utcnow()
current_year = now.year
current_week = now.isocalendar()[1]
# Use atomic update modifier to increment by 24
dbh.users.update({"username":username}, {"$inc":{"scores_weekly.%s-%s" %(current_year, current_week):24}}, safe=True)


# Store daily, weekly, monthly and total scores in user document
user_doc = {
    "scores_weekly":{
	"2011-01":10,
	"2011-02":3,
	"2011-06":20
    },
    "scores_daily":{
	"2011-35":2,
	"2011-59":7,
	"2011-83":15
    },
    "scores_monthly":{
	"2011-09":30,
	"2011-10":43,
	"2011-11":24
    },
    "score_total":123
}

# Update the score for the current week
import datetime
username = "foouser"
now = datetime.datetime.utcnow()
current_year = now.year
current_month = new.month
current_week = now.isocalendar()[1]
current_day = now.timetuple().tm_yday
# Use atomic update modifier to increment by 24
dbh.users.update({"username":username}, 
    {"$inc":{
	"scores_weekly.%s-%s" %(current_year, current_week):24,
	"scores_daily.%s-%s" %(current_year, current_day):24,
	"scores_monthly.%s-%s" %(current_year, current_month):24,
	"score_total":24,
	}
    }, 
    safe=True)
.................................
list avaialbe collections in a db
db.collection_names()
db.command("buildinfo")
.................................
WSGI
Web server side --

The server must provide two things: an "environ" dictionary, and a "start_response" function. 
The "environ" dictionary needs to have the usual things present -- it is similar to the CGI environment. 
"start_response" is a callable that takes two arguments, 
	"status" -- containing a standard HTTP status string like 200 OK -- and 
	"response_headers" -- a list of standard HTTP response headers.

The Web server dispatches a request to the framework/app by calling the application:

iterable = app(environ, start_response)
for data in iterable:
   # send data to client

It's the framework/app's responsibility to build the headers, call start_response, and build the data returned in iterable.
It's the Web server's responsibility to serve both the headers and the data up via HTTP.

Web framework/app side --

The Web framework/app is represented to the server as a Python callable. It can be a class, an object, or a function. The arguments to "__init__", "__call__", or the function must be as above: an "environ" object and a "start_response" callable.

The Web framework/app must call "start_response" before returning or yielding any data.

The Web framework/app should return any data in an iterable form -- e.g.  return [ page ].

Middleware --

Middleware components must obey both the Web server side and the Web app/framework side of things, plus a few more minor niggling restrictions.

.................................
An example WSGI application

Here is a very simple WSGI application that returns a static "Hello world!" page.

def simple_app(environ, start_response):
    status = '200 OK'
    response_headers = [('Content-type','text/plain')]
    start_response(status, response_headers)
    return ['Hello world!\n']

Here is a very simple middleware application that uppercases everything sent:

class Upperware:
   def __init__(self, app):
      self.wrapped_app = app

   def __call__(self, environ, start_response):
      for data in self.wrapped_app(environ, start_response):
         return data.upper()

To instantiate/run these from the server''s perspective, you just do the obvious:   

serve(simple_app)
or

wrapped_app = Upperware(simple_app)
serve(wrapped_app)

Yes, it can be that simple: here is my code for running Trac via scgiserver.

#!/usr/bin/env python
from trac.web.main import dispatch_request as app
from scgiserver import serve_application

PREFIX=''
PORT=4101

serve_application(app, PREFIX, PORT)
#################################
Inorder and Preorder/Postorder Traversal
Preorder: root, left, right
Postorder: left, right, root
Inorder: left, root, right
class BinaryTree:
    def __init__(self,data,left=None, right=None):
        self.data = data
        self.left = left
        self.right = right
    def set_left(self, data):
        self.left = data
        self.left.parent = self
    def set_right(self, data):
        self.right = data
        self.right.parent = self
    def __unicode__(self) :
        return '%s' % self.data
def preorder_dfs(tree) : # eager, depth-first, get the complete tree at once
    nodes = []
    if(tree != None):
        print tree.data
        nodes.append(tree.data)
        nodes.extend(preorder_dfs(tree.left))
        nodes.extend(preorder_dfs(tree.right))
    return nodes
nodes = []
def preorder(tree): # root, left, right
    if (tree != None):
        print tree.data
        nodes.append(tree.data)
    if (tree.left != None):
        preorder(tree.left)
    if (tree.right != None):
        preorder(tree.right)
    return nodes
def inorder(tree): # left, root, right
    if (tree.left != None):
        inorder(tree.left)
    if (tree != None):
        print tree.data
        nodes.append(tree.data)
    if (tree.right != None):
        inorder(tree.right)
    return nodes
def postorder(tree): # left, right, root
    if (tree.left != None):
        postorder(tree.left)
    if (tree.right != None):
        postorder(tree.right)
    if (tree != None):
        print tree.data
        nodes.append(tree.data)
    return nodes
if __name__ == "__main__":
    #t = []
    root = BinaryTree(7)
    t10 = BinaryTree(10)
    t2 = BinaryTree(2)
    t4 = BinaryTree(4)
    t3 = BinaryTree(3)
    t1 = BinaryTree(1)
    t8 = BinaryTree(8)
    t11 = BinaryTree(11)
    root.set_left(t10)
    root.set_right(t2)
    t10.set_left(t4)
    t10.set_right(t3)
    t3.set_right(t1)
    t2.set_left(t8)
    t8.set_left(t11)
    print postorder(root)
#################################
To construct a Btree from preorder and inorder
def maketree(preorder, inorder):
    global tree
    if len(preorder) == 0:
        return None

    root = preorder[0]
    ind = inorder.index(root)
    linorder = filter(lambda x: inorder.index(x) < ind, inorder)
    lpreorder = filter(lambda x: x in linorder, preorder)
    rinorder = filter(lambda x: inorder.index(x) > ind, inorder)
    rpreorder = filter(lambda x: x in rinorder, preorder)
    #tree[root] = [maketree(lpreorder, linorder), maketree(rpreorder, rinorder)]
    if len(inorder) == 1: # To mark, leaf with L, node with N
        tree[root] = [maketree(lpreorder, linorder), maketree(rpreorder, rinorder),'L']
    else :
        tree[root] = [maketree(lpreorder, linorder), maketree(rpreorder, rinorder),'N']

    return root

tree = {}
pO = [7,10,4,3,1,2,8,11]
iO = [4,10,3,1,7,11,8,2]
maketree(pO, iO)
print tree
#################################
#finding the LCA, lowest common ancestor, with preorder + inorder, then
#identify, the LCA must be the on in the pre[0] middle of the 2 nodes.
def lowest_ancestor(pre,ino, a, b) :
#pre=[3, 5, 6, 2, 7, 4, 1, 0, 8]
#ino=[6, 5, 7, 2, 4, 3, 0, 1, 8]
    if len(pre) == 0:
        return None
    lino=filter(lambda x: ino.index(x) < ino.index(pre[0]), ino)
    lpre=filter(lambda x: x in lino, pre)
    rino=filter(lambda x: ino.index(x) > ino.index(pre[0]), ino)
    rpre=filter(lambda x: x in rino, pre)
    if a in ino and b in ino:
        idx_a = ino.index(a)
        idx_b = ino.index(b)
        print idx_a, idx_b
        if ino.index(pre[0]) in range(min(idx_a,idx_b),max(idx_a,idx_b)+1):
            print "found lowest common ancestor:", pre[0]
        else:
            lowest_ancestor(lpre,lino, a,b)
            lowest_ancestor(rpre,rino, a,b)
#################################
depth first search and width first search
class node:
    def __init__(self,name):
        self.name = name
        self.neighbor = []
    def add_neighbor(self, neighbor):
        self.neighbor.append(neighbor)

def clone_dfs(node):
    global bag , new_graph
    if node not in bag:
        bag.append(node.name)
        new_graph.append(node)
        if node.neighbor != None and  len(node.neighbor) > 0:
            for item in node.neighbor:
                clone_dfs(item)

def clone_bfs(node):
    global bag , new_graph
    i = 0
    bag.append(node.name)
    new_graph.append(node)
    while i <= (len(new_graph) -1):
        node = new_graph[i]
        if len(node.neighbor) > 0:
            for item in node.neighbor:
                if node not in bag:
                    bag.append(item.name)
                    new_graph.append(item)
        i += 1

if __name__ == "__main__":
    graph = []
    bag = []
    new_graph = []
    for j in range(1,10) :
        graph.append(node(j))
    graph[0].add_neighbor(graph[1])
    graph[0].add_neighbor(graph[2])
    graph[0].add_neighbor(graph[3])
    graph[1].add_neighbor(graph[4])
    graph[1].add_neighbor(graph[5])
    graph[2].add_neighbor(graph[6])
    graph[2].add_neighbor(graph[7])
    graph[3].add_neighbor(graph[8])
    #clone_dfs(graph[0])
    clone_bfs(graph[0])

    print new_graph
    for item in new_graph:
        print item.name,
        for ite in item.neighbor:
            print " <-", ite.name

############################################
To use binary digit 
>>> 0b100
4
>>> 0b100 ^ 0b001
5

To use Hex digit
>>> 0x10
16
To convert to binary or hex:
>>> bin(4)
'0b100'
>>> bin(400)
'0b110010000'
>>> hex(400)
'0x190'
>>> int(bin(400)[2:])
110010000

To convert binary back to int:
>>> int('0b'+bin(400)[2::],2)
400
#################################
with three calls to the Python multiprocessing.Pool class, I can create a
worker pool of 8 processes, and perform a distributed map and a distributed
reduce operation (lines 64, 70, 76). Pool.map takes care of the partitioning
and distribution of the data over the worker pool by itself.
http://mikecvet.wordpress.com/2010/07/02/parallel-mapreduce-in-python/
from multiprocessing import Pool

 # Generate count tuples for title-cased tokens
  single_count_tuples = pool.map(Map, partitioned_text)

 # Collapse the lists of tuples into total term frequencies
  term_frequencies = pool.map(Reduce, token_to_tuples.items())
#################################
import random
def randomwalk_list():
    last, rand = 1, random.random() # init candidate elements
    nums = []                       # empty list
    while rand > 0.1:               # threshhold terminator
        if abs(last-rand) >= 0.4:   # accept the number
            last = rand
            nums.append(rand)       # add latest candidate to nums
        else:
            print '*',              # display the rejection
        rand = random.random()      # new candidate
    nums.append(rand)               # add the final small element
    return nums

for num in randomwalk_list():
    print num,

#One trick available in Python 2.1 and earlier is to use a "static" function-local variable to remember things about the last invocation of a function. Obviously, global variables could do the same job, but they cause the familiar problems with pollution of the global namespace,
#-Python does not have an "official" static scoping declaration. However, if named parameters are given mutable default values, the parameters can act as persistent memories of previous invocations. Lists, specifically, are handy mutable objects that can conveniently even hold multiple values.
#Using a "static" approach, we can write a function like:

import random
def randomwalk_static(last=[1]):    # init the "static" var(s)
    rand = random.random()          # init a candidate value
    if last[0] < 0.1:               # threshhold terminator
        return None                 # end-of-stream flag
    while abs(last[0]-rand) < 0.4:  # look for usable candidate
        print '*',                  # display the rejection
        rand = random.random()      # new candidate
    last[0] = rand                  # update the "static" var
    return rand

num = randomwalk_static()
while num is not None:
    print num,
    num = randomwalk_static()

#New iterator class
import random
class randomwalk_iter:
    def __init__(self):
        self.last = 1               # init the prior value
        self.rand = random.random() # init a candidate value
    def __iter__(self):
        return self                 # simplest iterator creation
    def next(self):
        if self.rand < 0.1:         # threshhold terminator
            raise StopIteration     # end of iteration
        else:                       # look for usable candidate
            while abs(self.last-self.rand) < 0.4:
                print '*',          # display the rejection
                self.rand = random.random() # new candidate
            self.last = self.rand   # update prior value
            return self.rand

for num in randomwalk_iter():
    print num,
#if elem in iterator ; also supported
......................
# using the generator instead
from __future__ import generators   # only needed for Python 2.2
import random
def randomwalk_generator():
    last, rand = 1, random.random() # initialize candidate elements
    while rand > 0.1:               # threshhold terminator
        print '*',                  # display the rejection
        if abs(last-rand) >= 0.4:   # accept the number
            last = rand             # update prior value
            yield rand              # return AT THIS POINT
        rand = random.random()      # new candidate
    yield rand                      # return the final small element

#To call it manually
gen = randomwalk_generator()
try:
    while 1: print gen.next(),
except StopIteration:
    pass
# or
for num in randomwalk_generator():
    print_short(num)

# A recursive generator that generates Tree leaves in in-order.
def inorder(t):
	if t:
		for x in inorder(t.left):
			yield x
		yield t.label
		for x in inorder(t.right):
			yield x
# fibonacci-up-to-n function
# function version
def fibon(n):
    a = b = 1
    result = []
    for i in xrange(n):
        result.append(a)
        a, b = b, a + b
    return result
# generator version
def fibon(n):
    a = b = 1
    for i in xrange(n):
        yield a
        a, b = b, a + b
for x in fibon(1000000):
    print x,

def fib(n,memo=dict()): # with memoization
  print (n)
  if  n==0 or n==1: 
    memo[n]= 1
    return 1
  if n in memo.keys(): return memo[n]
  else:
    memo[n]=fib(n-1,memo)+fib(n-2,memo)
    return  memo[n]

# in python 3.3, the functools come with lru_cache, no need to implement memo!!
from functools import lru_cache 
@lru_cache(maxsize=None)
def fib1(n):
  #print (n)
  if  n==0 or n==1: 
    return 1
  return  fib1(n-1)+fib1(n-2)
print(fib1(35))
#for i in range(35):
#  print(fib(i))


#################################
for _ in xrange(10): # _ is for something, I don't care. So no varible name for it.
    print "Hello, world."
#################################
a heap is a binary tree, in which the key of a node is always larger or smaller then
its leaves; and it always fill left branch first.

"Height of a node"
We define the height of a node in a tree to be a number of edges on the
longest simple downward path from a node to a leaf.

Four basic procedures on heap are:
Heapify, which runs in O(lg n) time.
Build-Heap, which runs in linear time.
Heap Sort, which runs in O(n lg n) time.
Extract-Max, which runs in O(lg n) time.
#heapsort:
A = [ 6, 5, 3, 1, 8, 7, 2, 4 , 222,44545,78687,3423,6756754,2342,44]
H=[]
n = len(A) -1 # last index
def heapify_max(a,i,le): # finding max, recursive backward
    if i >=0:
        if (2*i+1 <= le) and a[i] < a[(2*i+1)] :
            a[i],a[(2*i+1)] = a[(2*i+1)], a[i]
            print "==",i, a[i], a[(2*i+1)]
        if (2*i+2 <= le) and a[i] < a[(2*i+2)] :
            a[i],a[(2*i+2)] = a[(2*i+2)],a[i]
            print "==",i, a[i], a[(2*i+2)]
        i-=1
        print a
        heapify_max(a,i,le)
def heapify_max_gen(a,i,le): # finding max, recursive backward
    while i >=0:
        if (2*i+1 <= le) and a[i] < a[(2*i+1)] :
            a[i],a[(2*i+1)] = a[(2*i+1)], a[i]
            print "==",i, a[i], a[(2*i+1)]
        if (2*i+2 <= le) and a[i] < a[(2*i+2)] :
            a[i],a[(2*i+2)] = a[(2*i+2)],a[i]
            print "==",i, a[i], a[(2*i+2)]
        i-=1
        print i,a
        yield  a
        heapify_max_gen(a,i,le)

def switch_out(a):
    a[0],a[-1] = a[-1], a[0]
    H.append(a.pop(-1))
for i in range(len(A) -1,-1,-1):
    heapify_max(A,i,i)
    #for x in heapify_max_gen(A,i,i):
    #   print x
    switch_out(A)
    print '======:',i,A,H
#################################
#path sum preorder
def path_sum_preorder(t):
    global inp
    if t != None:
        sum.append(t.data)
        print sum, t.data
        for x in path_sum_preorder(t.left) :
            yield x
        for x in  path_sum_preorder(t.right):
            yield x
        if t.left == None and t.right == None:
            print "got a leaf", t.data
            sumx = reduce(lambda x,y:x+y, sum)
            print sum, "path sum:", sumx
            if sumx == inp : print "match!!"
            yield sum
        sum.pop(-1)
for x in path_sum_preorder(root):
	pass
############################################
#finding unique path in m*n square box.
def path_finder(x,y):
    global path, m,n
    if x == m and y == n:
        return 1
    if x >m or y > n:
        return 0
    return path_finder(x+1, y) +  path_finder(x,y+1)
print path_finder(1,1)
############################################
# building balanced kd tree
# the nearest k points in kd tree
a=[(11,2), (-1, -2), (100, 20), (12, -40), (-11,20),(23,15),(7,33)]
class kdtree:
    def __init__(self,point):
        self.point=point
        self.left=None
        self.right=None
def build_kdtree(b,  i ): # i is the depth in the tree
    global dimension
    s = []
    if len(b) >1 :
        m=i%dimension
        s =sorted(b, key = lambda tup:tup[m])
        point =s[len(s)/2] # median point, build balanced kd tree
    elif len(b) == 1: point = b[0]
    else : return
    t=kdtree(point)
    t.left=build_kdtree(s[:len(s)/2:], i+1 ) # left/down, smaller
    t.right=build_kdtree(s[len(s)/2+1::], i+1) # right/up, bigger
    return t
def nn_in_kdtree(t,depth):
    global best_d, dimension, q, route,k
    if t !=None:
        print "x,y:", t.point[0],t.point[1]
        d = pow(abs(t.point[0] - q[0]),2) +pow(abs(t.point[1]-q[1]),2)
        if len(best_d) <k: #  no enough points
            best_d.append((d,t))
        elif d <= best_d[-1][0]: # when new distace is smaller than largest existing one
            best_d=sorted(best_d, key = lambda tup:tup[0])
            try: best_d.index((d,t)) # exist?
            except ValueError:
                best_d[-1] =(d,t)
                print 'best_d +',best_d[-1][0],best_d[-1][1].point
        m = depth % dimension
        if q[m] >= t.point[m]:
            nn_in_kdtree(t.left,depth+1)
        else:
            nn_in_kdtree(t.right, depth+1)
        for i in range(k): # to check if the radius can span over the plane  
            if best_d[i][0] > pow(abs(t.point[m]-q[m]),2):
                if q[m] < t.point[m]:
                    nn_in_kdtree(t.left,depth+1)
                else:
                    nn_in_kdtree(t.right, depth+1)
dimension = len(a[0])
q=(60,40)
k=3
best_d=[]
dimension = len(a[0])
print a
root=build_kdtree(a,0)
print "root",root.point[0], root.point[1]
nn_in_kdtree(root,1)
for i in range(k):
    print "q and",k," closet point and distance square:", q,
best_d[i][1].point, best_d[i][0]
################################# Counting sort
a=[22,3,556,223,56556,23,2,767,0, 55, 19,200,300,452,95]
m=max(a) +1
count=[0] * m
for i in a:
    count[i]=1

for j in range(m):
    if count[j] ==1: print j
#################################
There are two key attributes that a problem must have in order for dynamic programming to be applicable: "optimal substructure" and "overlapping subproblems". If a problem can be solved by combining optimal solutions to non-overlapping subproblems, the strategy is called "divide and conquer" instead. This is why mergesort and quicksort are not classified as dynamic programming problems.

#################################
The asymptotic notation consists of a bunch of operators, written as Greek letters. The most important ones (and the only ones we’ll be using) are O (originally an omicron but now usually called “Big Oh”), Ω(omega), and Θ(theta). The definition for the O operator can be used as a foundation for the other two. The expression O(g), for some function g(n), represents a set of functions, and a function f(n) is in this set if it satisfies the following condition: there exists a natural number n0 and a positive constant c such that 
f(n) ≤ cg(n)

Basically, O(g) is the set of functions that do not grow faster than g. 

2.4n^2 + 7 ∈ O(n^2) 

n ∈ O(n^2) 

 O can be used to express loose limits as well: any function that is better (that is, doesn’t grow faster) than g can be found in O(g).  


the running time of appending n numbers to a Python list is O(n), while inserting n numbers at its beginning is O(n^2).


The other two, Ω and Θ, are just variations of O. Ω is its complete opposite: a function f is in Ω(g) if it satisfies the following condition: there exists a natural number n0 and a positive constant c such that 
f(n) ≥ cg(n) 

for all n ≥ n0 . So, where O forms a so-called asymptotic upper bound, Ω forms an asymptotic lower bound.


The sets formed by Θ are simply intersections of the other two, that is, Θ(g) = O(g) ∩ Ω(g). In other words, a function f is in Θ(g) if it satisfies the following condition: there exists a natural number n0and two positive constants c1 and c2 such that 

c1g(n) ≤ f(n) ≤ c2g(n) 

for all n ≥ n0 . This means that f and g have the same asymptotic growth. For example, 3n^2 + 2 is Θ(n^2), but we could just as well write that n^2 is Θ(3n^2 + 2).  By supplying an upper bound and a lower bound at the same time, the Θ operator is the most informative of the three, and I will use it when possible.  


Actually, the relationship is even stricter: fis o(g), where the “Little Oh” is a stricter version if “Big Oh.” Intuitively, instead of “doesn’t grow faster than,” it means “grows slower than.” Formally, it states that f(n)/g(n) converges to zero as n grows to infinity.


Complexity 	Name  		Examples, Comments 
Θ(1)  		Constant 	Hash table lookup and modification (see black box sidebar on dict). 
Θ(lg n)  	Logarithmic Binary search (see Chapter 6). Logarithm base unimportant. 
Θ(n)  		Linear 		Iterating over a list. 
Θ(nlg n)  	Loglinear  	Optimal sorting of arbitrary values (see Chapter 6). Same as Θ(lg n!). 
Θ(n^2)  	Quadratic 	Comparing nobjects to each other (see Chapter 3). 
Θ(n^3)  	Cubic 		Floyd and Warshall’s algorithms (see Chapters 8 and 9). 
O(n^k) 		Polynomial  knested for loops over n (if k is pos. integer). For any constant k> 0.  
Ω(k^n) 		Exponential Producing every subset of n items (k= 2; see Chapter 3). Any k> 1. 
Θ(n!)  		Factorial 	Producing every ordering of n values.  

•  In a sum, only the dominating summand matters. 
For example, Θ(n^2 + n^3 + 42) = Θ(n^3). 
•  In a product, constant factors don’t matter. 
For example, Θ(4.2nlg n) = Θ(nlg n).

 For Oand Ω, there is a third principle we usually follow: 
•  Keep your upper or lower limits tight. 

•  Θ(f) + Θ(g) = Θ(f+ g) 
•  Θ(f) · Θ(g) = Θ(f· g)

nums size n:
nums.append(1) # Θ(1)
nums.insert(0,2) #Θ(n)

seq1 and seq2, where seq1 contains n elements and seq2 contains m elements. The following code will then have a running time of Θ(nm): 
s = 0 
for x in seq1: 
	for y in seq2: 
		s += x*y 
......................
The timeitfunction will run your code multiple times for increased precision,
>>> import timeit 
>>> timeit.timeit("x = 2 + 2") 
0.034976959228515625 
>>> timeit.timeit("x = sum(range(10))") 
0.92387008666992188 

$ python -m timeit -s"import mymodule as m" "m.myfunction()"  

or just do this:
import time
start_time=time.time()
run something
end_time=time.time()
print('duration:' , end_time-start_time)
...........
import cProfile 
cProfile.run('main()')

...........trace module
......................graph
• A graph G = (V, E) consists of a set of nodes, V, and edges between them, E.  If the edges have a direction, we say the graph is directed. 

• Nodes with an edge between them are adjacent. The edge is then incident to both.  The nodes that are adjacent to v are the neighborsof v. 

• A subgraph of G = (V, E) consists of a subset of V and a subset of E. A path in G is a subgraph where the edges connect the nodes in a sequence, without revisiting any node. A cycle is like a path, except that the last edge links the last node to the first. 

• If we associate a weight with each edge in G, we say that G is a weighted graph. The length of a path or cycle is the sum of its edge weights, or, for unweighted graphs, simply the number of edges. 

• A forest is a cycle-free graph, and a connected graph is a tree. In other words, a forest consists of one or more trees.

Hashing involves computing some (often seemingly random) integer value from 
an arbitrary object. This value can then be used, for example, as an index
into an array (subject to some 
adjustments to make it fit the index range). 
>>> hash(42) 
42 
>>> hash("Hello, world!") 
-1886531940 
accessing elements of a dictor set can be assumed to take constant (expected) time

...................... Adjacency Lists and the Like 
Listing 2-1.A Straightforward Adjacency Set Representation 
a, b, c, d, e, f, g, h = range(8) 
N = [ 
	{b, c, d, e, f}, 	# a 
	{c, e}, 			# b 
	{d}, 				# c 
	{e}, 				# d 
	{f}, 				# e 
	{c, g, h}, 			# f 
	{f, h}, 			# g 
	{f, g} 				# h 
] 

>>> b in N[a] # Neighborhood membership 
True 
>>> len(N[f]) # Degree 
3 

Listing 2-2.Adjacency Lists 
a, b, c, d, e, f, g, h = range(8) 
N = [ 
	[b, c, d, e, f],# a 
	[c, e], 		# b 
	[d], 			# c 
	[e], 			# d 
	[f], 			# e 
	[c, g, h], 		# f 
	[f, h], 		# g 
	[f, g] 			# h 
] 
To run python in interactive mode:
python -i listing_2_1.py 

A recurring theme when working with graphs is that the best representation depends on what you need to do with your graph. For example, using adjacency lists (or arrays) keeps the overhead low and 
lets you efficiently iterate over N(v) for any node v. However, checking whether u and v are neighbors is Θ(N(v)), which can be problematic if the graph is dense(that is, if it has many edges). In these cases, adjacency sets may be the way to go. 

use set for membership check; use list for iteration

delete/append from the end othe list take contant time.

Listing 2-3.Adjacency dicts with Edge Weights 
a, b, c, d, e, f, g, h = range(8) 
N = [ 
{b:2, c:1, d:3, e:9, f:4},  # a 
{c:4, e:3}, 				# b 
{d:8}, 						# c 
{e:7}, 						# d 
{f:5}, 						# e 
{c:2, g:2, h:2}, 			# f 
{f:1, h:6},			 		# g 
{f:9, g:8} 					# h 
] 
The adjacency dict version can be used just like the others, with the additional edge weight functionality: 
>>> b in N[a] # Neighborhood membership 
True 
>>> len(N[f]) # Degree 
3 
>>> N[a][b] # Edge weight for (a, b) 
2 
Listing 2-4.A Dict with Adjacency Sets 
N = { 
	'a': set('bcdef'), 
	'b': set('ce'), 
	'c': set('d'), 
	'd': set('e'), 
	'e': set('f'), 
	'f': set('cgh'), 
	'g': set('fh'), 
	'h': set('fg') 
} 
......................Adjacency Matrices 
Listing 2-5.An Adjacency Matrix, Implemented with Nested Lists 
a, b, c, d, e, f, g, h = range(8) 
# a b c d e f g h 
N = [[0,1,1,1,1,1,0,0], # a 
	[0,0,1,0,1,0,0,0], # b 
	[0,0,0,1,0,0,0,0], # c 
	[0,0,0,0,1,0,0,0], # d 
	[0,0,0,0,0,1,0,0], # e 
	[0,0,1,0,0,0,1,1], # f 
	[0,0,0,0,0,1,0,1], # g 
	[0,0,0,0,0,1,1,0]] # h 

>>> N[a][b] # Neighborhood membership 
1 
>>> sum(N[f]) # Degree 
3 

First, as long as we aren’t allowing self-loops (that is, we’re not working with pseudographs), the diagonal is all false. Also, we often implement undirected graphs by adding edgesin both directions to our representation. This means that the adjacency matrix for anundirected graph will be symmetric.

Listing 2-6.A Weight Matrix with Infinite Weight for Missing Edges 
a, b, c, d, e, f, g, h = range(8) 
_ = float('inf') 
# a b c d e f g h 
W = [[0,2,1,3,9,4,_,_], # a 
	[_,0,4,_,3,_,_,_], # b 
	[_,_,0,8,_,_,_,_], # c 
	[_,_,_,0,7,_,_,_], # d 
	[_,_,_,_,0,5,_,_], # e 
	[_,_,2,_,_,0,2,2], # f 
	[_,_,_,_,_,1,0,6], # g 
	[_,_,_,_,_,9,8,0]] # h 

>>> W[a][b] < inf # Neighborhood membership 
True 
>>> W[c][e] < inf # Neighborhood membership 
False 
>>> sum(1 for w in W[a] if w < inf) - 1 # Degree 
5 
Note that 1 is subtracted from the degree sum,because we don’t want to count the diagonal. The degree calculation here is Θ(n), whereas both membership and degree could easily be found in constant time with the proper structure. 

Where an empty list-based weight or adjacency matrix for nnodes is created, for example, like this 
>>> N = [[0]*10 for i in range(10)] 

in NumPy, you can use the zerosfunction: 
>>> import numpy as np 
>>> N = np.zeros([10,10]) 

......................Implementing Trees
>>> T = [["a", "b"], ["c"], ["d", ["e", "f"]]] 
>>> T[0][1] 
'b' 
>>> T[2][1][0] 
'e' 

Listing 2-7. A Binary Tree Class 
class Tree: 
	def __init__(self, left, right): 
		self.left = left 
		self.right = right 
You can use the Tree class like this: 
>>> t = Tree(Tree("a", "b"), Tree("c", "d")) 
>>> t.right.left 
'c' 

# each node point to its its kids and sibling
Listing 2-8. A Multiway Tree Class 
class Tree: 
	def __init__(self, kids, next=None): 
		self.kids = self.val = kids 
		self.next = next 

>>> t = Tree(Tree("a", Tree("b", Tree("c", Tree("d"))))) 
>>> t.kids.next.next.val 
'c' 

......................THE BUNCH PATTERN 
class Bunch(dict): 
	def __init__(self, *args, **kwds): 
		super(Bunch, self).__init__(*args, **kwds) 
		self.__dict__ = self 

>>> x = Bunch(name="Jayne Cobb", position="Public Relations") 
>>> x.name
'Jayne Cobb'

>>> T = Bunch 
>>> t = T(left=T(left="a", right="b"), right=T(left="c")) 
>>> t.left 
{'right': 'b', 'left': 'a'} 
>>> t.left.right 
'b' 
>>> t['left']['right'] 
'b' 
>>> "left" in t.right 
True 
>>> "right" in t.right 
False 

•  When performance is important, rely on actual profiling rather than intuition. You may have hidden bottlenecks, and they may be nowhere near where you suspect they are. 
•  When correctness is critical, the best thing you can do is calculate your answer more than once, using separate implementations (preferably written by separate programmers).

......................Hidden Squares 
>>> from random import randrange 
>>> L = [randrange(10000) for i in range(1000)] 
>>> 42 in L 
False 
>>> S = set(L) 
>>> 42 in S 
False 
#membership checks are linear for lists and constant for sets.

>>> s = "" 
>>> for chunk in input(): 
... 	s += chunk 

The problem is that (without the optimizations) you need to create a new string for every += operation, copying the contents of the previous one. Because string is not mutable.
.................................
A better solution:
>>> chunks = [] 
>>> for chunk in input(): 
... chunks.append(chunk) 
... 
>>> s = ''.join(chunks) 

or
>>> s = ''.join(input()) 

Appending allows you to overallocate with a percentage so that the available
space grows exponentially, and the append cost is constant when averaged
(amortized) over all the operations.


>>> res = [] 
>>> for lst in lists: 
... res.extend(lst) 
#is better than:
>>> result = sum(lists, []) #using +=

......................float
The first lesson here is to never compare floats for equality.

#round(number[, ndigits])

>>> def almost_equal(x, y, places=7): 
... return round(abs(x-y), places) == 0 
... 
>>> almost_equal(sum(0.1 for i in range(10)), 1.0) 
True 

#or
>>> from decimal import * 
>>> sum(Decimal("0.1") for i in range(10)) == Decimal("1.0") 
True 

#you might find tools such as Sage useful, it is slower though

sage: 3/5 * 11/7 + sqrt(5239) 
13*sqrt(31) + 33/35 

Each logarithm has a fixed base; the most common one in algorithmics is the
base-2 logarithm, written log2 or simply lg. (The base-10 logarithm is
conventionally written simply log, while the so-called natural logarithm, with
base e, is written ln). The logarithm gives us the exponent we need (for the
given base), so if n= 2^k , then lg n= k. In Python, you can use the log
function of the math module to get logarithms.  

The factorial, or n!, is calculated as n¡Á (n¨C1) ¡Á (n¨C2) ¡­ 1. It can be
used, among other things, to 
calculate the number of possible orderings of nelements.
......................
 a=[[0]*10 for i in range(10)]
#different from 
a=[[0]*10]*10 # this is the same list 10 times, update [0][0], also update [1][0], [2][0] ... 


Logarithms can have different bases, but algorists don''t usually care.  the difference between (loga n) and (logb n) is just a constant factor (loga b), which disappears when we use asymptotic notation.

Anything that involves finding or modifying a certain position will normally take constant time in Python lists, as their underlying implementation is arrays. You have to traverse a linked list to do this (on average, half the list), giving a linear running time. Swapping things around, once you know the positions, is constant in both.  Modifying the list structure (by inserting or deletingelement, except at the end)is generally linear for arrays (and Python lists) but can in many casesbe done in constant time for linked lists.  

for any binary tree, the internal node is m, leaf is n, m = n-1

DAG, directed acyclic graph

The number of (undirected) edges incident on a node v(that is, the size of
N(v)) is called its degree, often written d(v).

We can also partition the neighborhood of a node into an in-neighborhood,
sometimes called parents, and an outneighborhood, or children.  

We say that H spans Gif W= V. That is, a spanning subgraph is one that covers
all the nodes of the original graph, but with less edge.

........... Tree
1.  Tis a tree (that is, it is acyclic and connected). 
2.  Tis acyclic, and has n¨C1 edges. 
3.  Tis connected, and has n¨C1 edges. 
4.  Any two nodes are connected by exactly one path. 
5.  Tis acyclic, but adding any new edge to it will create a cycle. 
6.  Tis connected, but removing any edge yields two connected components.


Spanning trees are simply spanning subgraphs that happen to be trees.

The spanning tree resulting from a traversal, called the traversal tree, is
rooted at the starting node.

A digraph whose underlying graph is a rooted tree, and where all the directed
edges point away from the root (that is, all nodes can be reached by directed
paths from the root) is called an arborescence,
The term oriented treeis used both about rooted (undirected) trees and
arborescences, because the edges of a rooted tree have an implicit direction
away from the root. 

DAGs are quite natural as representations for dependencies, as cyclic
dependencies are generally impossible (or, at least, undesirable).
DAGs also form the basis for the technique of dynamic programming, discussed
in Chapter 8.


In a perfectly balanced binary, the tree has always 2 brenches: it has n
leafs, n-1 interneral nodes, and  n-1 brenches/edges

n
E i = n(n-1)/2
i=0

# n + (n-1) + (n-2) + ... + 1

h-1
E 2^i = n-1
i=0
# h is the hight of the binary tree, 2^h=n, or h = lg n, from n cut falf and
# down to 1, need h times.
# n/2 + n/4 + n/8 + ... + 1
# or 1 + 2 + 4 + ...+ n/2
#This is an example of bisection, or binary search, one of the most important
and well-known logarithmic algorithms.

A geometric (or exponential) series is a sum of k^i , where i= 0¡­n, for some
constant k. If kis greater than 1, the sum will always be ¦¨(k^(n+1)). The
doubling sum is just a special case. 

the sum of i^k , where i= 1¡­n, for some positive constant k, will always be
¦¨(n (k+1)). The hand shake sum is just a special case.  


any algorithm that needs to check every subset of the input objects
necessarily has an exponential running time complexity. 
A K bit binary represents a set of K member, there are 2^K possible subset.

'Permutations' are orderings. If n people queue up for movie tickets, how many
possible lines can we get?  You can compute n! by multiplying n (the number of
possible people in the first position) by n¨C1 (remaining options for the
second position) and n¨C2 (third¡­), and so forth, down to 1: 
n!=n¡¤(n-1) ¡¤(n-2) ¡¤... ¡¤2 ¡¤1

Combinations, C(n,k).  This is also called the binomial coefficient (or
sometimes the choose function) and is read ¡°n choose k.¡±
C(n,k)= n!/k!(n-k)!

The fact that C(n,k) counts the number of possible subsets of size kand 2^n
counts the number of possible subsets in total, gives us the following
beautiful equation:
n
E   C(n,k) = 2^k
k=0

...........Recursion and Recurrences 

Table 3-1. Some Basic Recurrences with Solutions, as Well as Some Sample
Applications 
# Recurrence  			Solution 	Example Applications 
1  T(n) = T(n-1) + 1 	Θ(n)  		Processing a sequence, for example, with reduce 
2  T(n) = T(n-1) + n  	Θ(n^2)  	Handshake problem 
3  T(n) = 2T(n-1) + 1  Θ(2^n)  	Towers of Hanoi 
4  T(n) = 2T(n-1) + n  Θ(2^n) 
5  T(n) = T(n/2) + 1  	Θ(lgn)  	Binary search (see the black box sidebar on bisectin Chapter 6) 
6  T(n) = T(n/2) + n  	Θ(n)  		Randomized Select, average case (see Chapter 6) 
7  T(n) = 2T(n/2) + 1  	Θ(n)  		Tree traversal (see Chapter 5) 
8  T(n) = 2T(n/2) + n  	Θ(nlg n)  	Sorting by divide and conquer(see Chapter 6)


Python¡¯s sorting algorithm, timsort, is a naturally adaptive version of merge
sort. It manages to achieve the linear best-case running time while keeping
the loglinear worst case.

Listing 3-1.Gnome Sort, An Example Sorting Algorithm 
def gnomesort(seq): 
	i = 0 
	while i < len(seq): 
		if i == 0 or seq[i-1] <= seq[i]: 
			i += 1 
		else: 
			seq[i], seq[i-1] = seq[i-1], seq[i] 
			i -= 1 

Listing 3-2.Merge Sort, Another Example Sorting Algorithm 
def mergesort(seq): 
	mid = len(seq)//2 
	lft, rgt = seq[:mid], seq[mid:] 
	if len(lft) > 1: lft = mergesort(lft) 
	if len(rgt) > 1: rgt = mergesort(rgt) 
	res = [] 
	while lft and rgt: 
		if lft[-1] >= rgt[-1]: 
			res.append(lft.pop()) 
		else: 
			res.append(rgt.pop()) 
	res.reverse() 
	return (lft or rgt) + res 


Reduction means transforming one problem to another. We normally reduce an
unknown problem to one we know how to solve. The reduction may involve
transforming both the input (so it works with the new problem) and the output
(so it¡¯s valid for the original problem).  
To change to something known.

?  Induction(or, mathematical induction) is used to show that a statement is
true for a large class of objects (often the natural numbers). We do this by
first showing it to be true for a base case (such as the number 1) and then
showing that it ¡°carries over¡± from one object to the next (if it¡¯s true
for n¨C1, then it¡¯s true for n). 
To show it work with both 1 and n

?  Recursionis what happens when a function callsitself. Here we need to make
sure the function works correctly for a (nonrecursive) base case and that it
combines results from the recursive calls into a valid solution.  
To accululate result.


1 +3 +5 +...+(2n-3) +(2n-1) =n^2

Listing 4-1.Recursive Insertion Sort 
#find the max one, and swith 
def ins_sort_rec(seq, i): 
	if i==0: return 						# Base case -- do nothing 
	ins_sort_rec(seq, i-1) 					# Sort 0..i-1 
	j = i 									# Start "walking" down 
	while j > 0 and seq[j-1] > seq[j]: 		# Look for OK spot 
		seq[j-1], seq[j] = seq[j], seq[j-1] # Keep moving seq[j] down 
		j -= 1 								# Decrement j 

Listing 4-2.Insertion Sort 
def ins_sort(seq): 
	for i in range(1,len(seq)): 				# 0..i-1 sorted so far 
		j = i 									# Start "walking" down 
		while j > 0 and seq[j-1] > seq[j]: 		# Look for OK spot 
			seq[j-1], seq[j] = seq[j], seq[j-1] # Keep moving seq[j] down 
			j -= 1 								# Decrement j 

Listing 4-3.Recursive Selection Sort , 
#find the max one and put the back, then, find the next max one
def sel_sort_rec(seq, i): 
	if i==0: return 						# Base case -- do nothing 
	max_j = i 								# Idx. of largest value so far 
	for j in range(i): 						# Look for a larger value 
		if seq[j] > seq[max_j]: max_j = j 	# Found one? Update max_j 
	seq[i], seq[max_j] = seq[max_j], seq[i] # Switch largest into place 
	sel_sort_rec(seq, i-1) 					# Sort 0..i-1 

Listing 4-4.Selection Sort 
def sel_sort(seq): 
	for i in range(len(seq)-1,0,-1): 			# n..i+1 sorted so far 
		max_j = i 								# Idx. of largest value so far 
		for j in range(i): 						# Look for a larger value 
			if seq[j] > seq[max_j]: max_j = j 	# Found one? Update max_j 
		seq[i], seq[max_j] = seq[max_j], seq[i] # Switch largest into place 

.................................  For change seat issue
The key is that only the one whose seat is wanted by someone else can change seat.

M=[2,2,0,5,3,5,7,4]
#M=[1,2,3,1,0,4]
# the idea is that only the ones, whose seats are wanted by someone else can
# change seat.
# it is to find all the seats, no one wanted, and remove the person on it.

def find_max_permutation(M,A=None):
    if A==None:
        A=set(range(len(M))) # concerned seat
    if len(A) == 1: return A # no one can switch seat
    B=set([M[i] for i in A])  # wanted seat
    C= A - B                 # not wanted seat
    #for i in C: M.pop(i)
    if C:
        A = A - C
        return find_max_permutation(M, A)
    return A
...................... counting sort
from collections import defaultdict 
def counting_sort(A, key=lambda x: x): 
	B, C = [], defaultdict(list) 	# Output and "counts" 
	for x in A: 
		C[key(x)].append(x) # "Count" key(x) 
	for k in range(min(C), max(C)+1): # For every key in the range 
		B.extend(C[k]) # Add values in sorted order 
	return B 

If several values have the same key, they’ll end up in the original order with respect to each other. Sorting algorithms with this property are called 'stable'. 

Counting sort can be extended to greater value ranges by sorting numbers on individual digits(or strings on individual characters or bit vectors on fixed-size chunks). If you first sort on the least significant digit, because of stability, sorting on the second least significant digit won’t destroy the internal ordering from the first run. (This is a bit like sorting column by column in a spreadsheet.) This means that for d digits, you can sort nnumbers in Θ(dn) time. This algorithm is called 'radix' sort(and Exercise 4-11 asks you to implement it). 

Another somewhat similar linear-time sorting algorithm is 'bucket sort'. It assumes that your values are evenly (uniformly) distributed in an interval, for example, real numbers in the interval [0,1), and uses n buckets, or subintervals, that you can put your values into directly. In a way, you’re hashing each value into its proper slot, and the average (expected) size of each bucket is Θ(1).  Because the buckets are in order, you can go through them and have your sorting in Θ(n) time, in the average case, for random data. 

......................celebrity search
#every one knows celebrity, but celebrity knows no one.
# only incoming links, and from everyone else, but no out going link
from random import randrange
def find_celeb(G):
    n=len(G)
    u,v = 0,1
    for c in range(2,n+1):
        if G[u][v] : u =c #if node u has outgoing link, change it
        else: v =c # u does not know v, change it
    if u==n: c=v
    else : c=u
    for v in range(n):
        if c==v: continue
        if G[c][v]: break
        if not G[v][c]: break
    else:
        return c
    return None
if __name__=='__main__':
    n=20
    G=[[randrange(2) for i in range(n)] for i in range(n)]
    cele=14  # to make it celebrity
    for i in range(n):
        G[i][cele] = 1
        G[cele][i] = 0
    for g in G:
        print g
    print find_celeb(G)


...........Topological Sorting 
dependencies are (as mentioned in Chapter 2) easily represented as a directed acyclic graph (DAG), and finding an ordering that respect the dependencies (so that all the edges point forward in the ordering) is called "topological sorting".
It is like finding celebrity above.?


|------------------------|
|	 |---------|		 |
|	 |		   v		 v
a -> b -> c -> d -> e -> f
     |         |         ^
     |         |---------|
     |-------------------|

from random import randrange
n=6
#G=[[randrange(2) for i in range(n)] for i in range(n)]
#list
#matrix
#G=[[0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0], [0, 0, 1, 0, 0, 1], [0, 1, 1, 0, 0, 0]]
G=[[4,5],[],[1],[0,1],[2,5],[1,2]]  
for g in G:
	print g
# find the one not in demand, remove it and remove its dependnecy
# to make the next node not in demand
def toposort(G,o=None):
	if o==None: o=[]
	n=len(G)
	for i in range(n):
		if i not in o and sum(G[j][i] for j in range(n)) == 0 : # 0 target to i, means no dependency on i 
			o.append(i)   # add to the collection
			for k in range(n): # remove its dependency
				G[i][k] = 0
	if len(o) < n:
		return toposort(G,o)
	else : return o

def toposort_list(G,o=None):
	if o==None: o=[]
	n=len(G)
	for i in range(n):
		flg=True
		for j in range(n): # 0 target to i, means no dependency on i 
			if i in G[j]: 
				flg=False
				break 
		if i not in o  and flg==True : 
			o.append(i)   # add to the collection
			G[i] = [] # remove its dependency
			
	if len(o) < n:
		return toposort_list(G,o)
	else : return o
print toposort_list(G)


Listing 4-10.Topological Sorted of a Directed, Acyclic Graph 
def topsort(G): 
	count = dict((u, 0) for u in G) # The in-degree for each node 
	for u in G: 
		for v in G[u]: 
			count[v] += 1 # Count every in-edge 
	Q = [u for u in G if count[u] == 0] # Valid initial nodes 
	S = [] # The result 
	while Q: # While we have start nodes... 
		u = Q.pop() # Pick one 
		S.append(u) # Use it as first of the rest 
		for v in G[u]: 
			count[v] -= 1 # "Uncount" its out-edges 
			if count[v] == 0: # New valid start nodes? 
				Q.append(v) # Deal with them next 
	return S 

......................tree balance factor
Balance factor is the depth on the left branch - right branch.
h=lg(n)
n: number of leaf
internal node = n -1
   -         n1  ------------
   |         /\             |
   |        /  \           n-1
 h=lg(n)   /    \           |
   |     n2      n3  --------
   |    / \      / \
   |   /   \    /   \
   -  1     2  3     4
     |--- n = 2^h ----|
......................Invariants and Correctness 
A 'loop invariant' is something that is true after each iteration of a loop, given some preconditions (it’s called an invariant because it doesn’t vary—it’s true from beginning to end).
 If we want to use this invariant to prove correctness, we need to do the following: 

1.  Use induction to show that it is, in fact, true after each iteration. 
2.  Show that we’ll get the correct answer if the algorithm terminates. 
3.  Show that the algorithm terminates.

......................Relaxation and Gradual Improvement 
Give it a loose estimate and gradually imporve the result.

Reduction + Contraposition = Hardness Proof 

•  If you can (easily) reduce A to B, thenB is at least as hard as A. 
•  If you want to show that X is hard and you know that Y is hard, reduce Y to X. 

.................................Problem Solving Advice
•  Make sure you really understand the problem.What is the input? The output?  What’s the precise relationship between the two? Try to represent the problem instances as familiar structures, such as sequences or graphs. A direct, brute-force solution can sometimes help clarify exactly what the problem is. 

•  Look for a reduction.Can you transform the input so it works as input for another problem that you can solve? Can you transform the resulting output so that you can use it? Can you reduce an instance if size nto an instance of size k< nand extend the recursive solution (inductive hypothesis) back to n?  

•  Are there extra assumptions you can exploit?Integers in a fixed value range can be sorted more efficiently than arbitrary values. Finding the shortest path in a DAG is easier than in an arbitrary graph, and using only non-negative edge weights is often easier than arbitrary edge weights.


A graph that you can draw in the plane without any edges crossing each other is called planar. Such a drawing will have a number of regions, areas bounded by the edges of the graph, as well as the (infinitely large) area aroundthe graph. If the graph has V, E, and Fnodes, edges, and regions, respectively, "Euler’s formula" for connected planar graphs says that V– E+ F= 2. 
......................
#Listing 5-5.Iterative Depth-First Search 
def iter_dfs(G, s): 
	S, Q = set(), [] # Visited-set and queue 
	Q.append(s) # We plan on visiting s 
	while Q: # Planned nodes left? 
		u = Q.pop() # Get one 
		if u in S: continue # Already visited? Skip it 
		S.add(u) # We've visited it now 
		Q.extend(G[u]) # Schedule all neighbors 
		yield u # Report u as visited

>>> list(iter_dfs(G, 0)) # getting generater result with list
[0, 5, 7, 6, 2, 3, 4, 1] 

For a directed graph, DFS can only traverse the whole graph from certain starting point.

To find all component in a directed graph, 
1. consider the link undirected
2. add reverse link
3. construct the undirected graph, same as 1? In G matrix, if G[u][v]==1: G[v][u]=1  

#Listing 5-6.A General Graph Traversal Function 
def traverse(G, s, qtype=set): 
	S, Q = set(), qtype() 
	Q.add(s) 
	while Q:  # stack queue, first in, last out, to make DFS
		u = Q.pop()   # if it is FIFO queue, could be BFS
		if u in S: continue 
		S.add(u) 
		for v in G[u]: 
			Q.add(v) 
		yield u 

class stack(list): # if defining a stack class to use
	add = list.append 

>>> list(traverse(G, 0, stack)) 
[0, 5, 7, 6, 2, 3, 4, 1] 

Remembering and avoiding previously visited nodes is what keeps us from going in circles (or, rather, cycles), and a traversal without cycles naturally forms a tree. Such "traversal trees" have different names based on how they were constructed; for DFS, they are aptly named "depth-first trees" (or "DFS trees").
The thing that is particular to DFS trees is that all descendants of a node u are processed in the time interval from when u is discovered to when we backtrack through it.

The DFS property then states that 
(1) every node is discovered beforeits descendants in the DFS tree, and 
(2) every node is finished afterits descendants in the DFS. 

#Listing 5-6.Depth-First Search with Timestamps 
def dfs(G, s, d, f, S=None, t=0): # d and s are dictionary
	if S is None: S = set() # Initialize the history 
	d[s] = t; t += 1 # Set discover time 
	S.add(s) # We've visited s 
	for u in G[s]: # Explore neighbors 
		if u in S: continue # Already visited. Skip 
		t = dfs(G, u, d, f, S, t) # Recurse; update timestamp 
	f[s] = t; t += 1 # Set finish time for s, all s' descendents are all finished
	return t # Return timestamp

#Listing 5-7.Topological Sorting Based on Depth-First Search 
def dfs_topsort(G): 
	S, res = set(), [] # History and result 
	def recurse(u): # Traversal subroutine 
		if u in S: return # Ignore visited nodes 
		S.add(u) # Otherwise: Add to history 
		for v in G[u]: 
			recurse(v) # Recurse through neighbors 
		res.append(u) # Finished with u: Append it 
	for u in G:  #the most targeted node always show up first 
		recurse(u) # Cover entire graph 
	res.reverse() # It's all backward so far 
	return res 

"The order in which DFS backtracks over nodes" (that is, the order of their finish times) is called "postorder", while "the order in which it visits them in the first place" is called "preorder". Processing at these times is called preorder or postorder processing. 

Note that you can classify the edges without actually using any explicit color labeling. Let the time span of a node be the interval from its discover time to its finish time. A descendant will then have a time span contained in its ancestor’s, while nodes unrelated by ancestry will have nonoverlapping intervals. 

By check if there is a cycle, you can use DFS to check whether a graph is a DAG (or, for undirected graphs, a tree).

......................Infinite Mazes and Shortest (Unweighted) Paths

"iterative deepening depth-first search", or "IDDFS", and it simply consists of running a depth-constrained DFS with an iteratively incremented depth limit.

#Listing 5-8.Iterative Deepening Depth-First Search 
def iddfs(G, s): 
	yielded = set() # Visited for the first time 
	def recurse(G, s, d, S=None): # Depth-limited DFS 
		if s not in yielded: 
			yield s 
			yielded.add(s) 
		if d == 0: return # Max depth zero: Backtrack 
		if S is None: S = set() 
		S.add(s) 
		for u in G[s]: 
			if u in S: continue 
			for v in recurse(G, u, d-1, S): # Recurse with depth-1 
				yield v 
		n = len(G) 
		for d in range(n): # Try all depths 0..V-1 
			if len(yielded) == n: break # All nodes seen? 
			for u in recurse(G, s, d): 
				yield u 

#Listing 5-9.Breadth-First Search 
def bfs(G, s): 
	P, Q = {s: None}, deque([s]) # Parents and FIFO queue 
	while Q: 
		u = Q.popleft() # Constant-time for deque 
		for v in G[u]: 
			if v in P: continue # Already has parent 
				P[v] = u # Reached from u: u is parent 
				Q.append(v) 
	return P 

#To extract a path to a node u, you can simply “walk backward” in P: 
path = [u] 
while P[u] is not None: 
	path.append(P[u]) 
	u = P[u] 
path.reverse() 

.................................
Python has a deque class in the collectionsmodule in the standard library. In
addition to methods such as append, extend, and pop, which are performed on
the rightside, it has leftequivalents, called appendleft, extendleft, and
popleft.  


......................Strongly Connected Components 
strongly connected components(SCCs) are the maximal subgraphs where there is a
directed path from any node to any other.

1.  Run dfs_topsort on the graph, resulting in a sequence seq.
2.  Reverse all edges. 
3.  Run a full traversal, selecting starting points (in order) from seq.

Kosaraju¡¯s algorithm, involves first findingthe finish-times for all nodes 
and then running a traversal in the transposedgraph (the graph with all edges
reversed), 
using descending finish-times to select starting points. 

    - > A -->  B     
    |          |
    |          |   
      ---  C <-

reverse edge direction, becomes:

    A <--  B <--   
    |          |
    |          |   
      -->  C --

#Listing 5-10.Kosaraju¡¯s Algorithm for Finding Strongly Connected Components 
def tr(G): # Transpose (rev. edges of) G 
	GT = {} 
	for u in G: GT[u] = set() # Get all the nodes in there 
	for u in G: 
		for v in G[u]: 
			GT[v].add(u) # Add all reverse edges 
	return GT 
def scc(G): 
	GT = tr(G) # Get the transposed graph 
	sccs, seen = [], set() 
	for u in dfs_topsort(G): # DFS starting points 
		if u in seen: continue # Ignore covered nodes 
		C = walk(GT, u, seen) # Don't go "backward" (seen) 
		seen.update(C) # We've now seen C 
		sccs.append(C) # Another SCC found 
	return sccs 

.................................quicksort
Both quicksort and mergesort are using divide and conquer. quicksort work
mostly duirng divid, mergesort put most work during the merge.
def partition(seq):
    pi,seq=seq[0],seq[1:]
    lo,hi=[],[]
    for s in seq:
        if s <= pi: lo.append(s)
        else: hi.append(s)
    return lo, pi, hi
def quicksort(seq):
    if len(seq) <=1: return seq
    lo,pi,hi=partition(seq)
    return quicksort(lo) + [pi] + quicksort(hi)

def selection(seq,k): #to find kth smallest item, k items in front of it
    lo,pi,hi= partition(seq)
    m= len(lo)
    if m==k: return pi
    if m < k: pi=selection(hi,k -m -1)
    if m > k: pi=selection(lo, k)
    return pi

...................... binary tree balancing
AA Tree
http://www.eternallyconfuzzled.com/tuts/datastructures/jsw_tut_andersson.aspx

the rules for an Andersson tree:

1) Every path contains the same number of pseudo-nodes.
2) A left child may not have the same level as its parent.
3) Two right children with the same level as the parent are not allowed.

A "skew" removes left horizontal links by rotating right at the parent. No changes are needed to the levels after a skew because the operation simply turns a left horizontal link into a right horizontal link:

         d,2               b,2
        /   \             /   \
     b,2     e,1  -->  a,1     d,2
    /   \                     /   \
 a,1     c,1               c,1     e,1

A "split" removes consecutive horizontal links by rotating left and increasing the level of the parent. A split needs to change the level of a single node because if a skew is made first, a split will negate the changes made by doing the inverse of a skew. Therefore, a proper split will force the new parent to a higher level:

     b,2                     d,3
    /   \                   /   \
 a,1     d,2     -->     b,2     e,2
        /   \           /   \
     c,1     e,2     a,1     c,1

Inserting in AA tree:

 0,1            1,2
  \            /   \
   1,1   -> 0,1     2,1
    \
     2,1

     1,2                    1,2
    /   \                  /   \
 0,1     2,1            0,1     3,2
            \        ->        /   \
             3,1            2,1     4,1
                \
                 4,1
     1,2                        1,2
    /   \                      /   \
 0,1     3,2                0,1     3,2
        /   \                      /   \
     2,1     4,1         ->     2,1     5,2
                \                      /   \
                 5,1                4,1     6,1
                    \
                     6,1

     1,2                           3,3
    /   \                      /         \
 0,1     3,2                1,2           5,2
        /   \        ->    /   \         /   \
     2,1     5,2        0,1     2,1   4,1     6,1
            /   \
         4,1     6,1
In this example there was never a skew because we were only inserting values in ascending order.

let''s look at another degenerate case that will force a skew, descending order insertion. We start with adding 6 to an empty tree, and nothing happens, then 5 is inserted. At this point we have a left horizontal link because both 6 and 5 have a level of 1, so a skew is required to turn the left horizontal link into a right horizontal link: 
'On the right link, it can have 2 nodes the same level, but not on the left link.'
     6,1    5,1
    /    ->    \
 5,1            6,1

Now we insert 4, which creates another horizontal left link, so another skew is required. But the skew creates consecutive right horizontal links, so a split needs to fix that. This is why skew is called before split, because a skew can create a case that requires a split:
'cause only split will add a level'

     5,1        4,1                5,2
    /   \    ->    \        ->    /   \
 4,1     6,1        5,1        4,1     6,1
                       \
                        6,1

         5,2            5,2
        /   \    ->    /   \
     4,1     6,1    3,1     6,1
    /                  \
 3,1                    4,1

         5,2            5,2                5,2
        /   \          /   \              /   \
     3,1     6,1 -> 2,1     6,1 ->     3,2     6,1
    /   \              \              /   \
 2,1     4,1            3,1        2,1     4,1
                           \
                            4,1

         5,2            3,2
        /   \          /   \
     3,2     6,1 -> 2,1     5,2
    /   \                  /   \
 2,1     4,1            4,1     6,1

...........AA tree deletion
            3,3
        /         \
     1,2           5,2
    /   \         /   \
 0,1     2,1   4,1     6,1
Removing 0 will cause a break in the levels between 1 and nil, so the level of 1 is decreased to 1. Then the break is between 1 and 3, so the level of 3 is decreased to 2. 
        3,2
    /         \
 1,1           5,2
    \         /   \
     2,1   4,1     6,1
Now we will remove 3:

        2,2
    /         \
 1,1           5,2
              /   \
           4,1     6,1
3 is replaced with its inorder predecessor 2, and the node for 2 is replaced with nil.
 However, when we remove 1, this creates a break between 2 and nil. The level of 2 is decreased to 1 along with the level of 5:

 2,2            2,1
    \              \
     5,2     ->     5,1
    /   \          /   \
 4,1     6,1    4,1     6,1

 2,1            2,1                    4,2
    \              \                  /   \
     5,1     ->     4,1         -> 2,1     5,1
    /   \              \                      \
 4,1     6,1            5,1                    6,1
                           \
                            6,1
To delete node in BS tree
 First you find it the node; let’s call it v. If it’s a leaf, just remove it.  If it’san internal node with a single child, just replace it with its child. If the node has twochildren, find the largest (rightmost) node in the left subtree, or the smallest (leftmost) in the right subtree—your choice. Now replace the key and value in vwith those of this descendant, and then delete the descendant. (To avoid making the tree unnecessarily unbalanced, you should switch between the left and right versions.) 

#Listing 6-6.The Binary Search Tree, Now with AA-Tree Balancing 
class Node: 
	lft = None 
	rgt = None 
	lvl = 1 # We've added a level... 
	def __init__(self, key, val): 
		self.key = key 
		self.val = val 
def skew(node): # Basically a right rotation 
	if None in [node, node.lft]: return node # No need for a skew 
	if node.lft.lvl != node.lvl: return node # Still no need 
	lft = node.lft # The 3 steps of the rotation 
	node.lft = lft.rgt 
	lft.rgt = node 
	return lft # Switch pointer from parent 
def split(node): # Left rotation & level incr. 
	if None in [node, node.rgt, node.rgt.rgt]: return node 
	if node.rgt.rgt.lvl != node.lvl: return node 
	rgt = node.rgt 
	node.rgt = rgt.lft 
	rgt.lft = node 
	rgt.lvl += 1 # This has moved up 
	return rgt # This should be pointed to 
def insert(node, key, val): 
	if node is None: return Node(key, val) 
	if node.key == key: node.val = val 
	elif key < node.key: 
		node.lft = insert(node.lft, key, val) 
	else: 
		node.rgt = insert(node.rgt, key, val) 
	node = skew(node) # In case it's backward 
	node = split(node) # In case it's overfull 
	return node 

.................................binary heap
There are many ways of implementing a priority queue, but probably the most common data structure used for this purpose is the binary heap.
"Binary heaps are complete binary trees". That means that they are as balanced as they can get, with each level of the tree filled up, except (possibly) the lowest level, which is filled as far as possible from the left.  Arguably the most important aspect of their structure, though, is the so-called "heap property": the value of every parent is smaller than those of both children. (This holds for a minimum heap; for a maximum heap, each parent is greater.)

The heapqmodule contains a very efficient heap implementation that represents its heaps in lists, using a common “encoding”: if ais a heap, the children of a[i]are found in a[2*i+1]and a[2*i+2]. This means that the root (the smallest element) is always found in a[0].

>>> from heapq import heappush, heappop 
>>> from random import randrange 
>>> Q = [] 
>>> for i in range(10): 
... heappush(Q, randrange(100)) 
... 
>>> Q 
[15, 20, 56, 21, 62, 87, 67, 74, 50, 74] 
>>> [heappop(Q) for i in range(10)] 
[15, 20, 21, 50, 56, 62, 67, 74, 74, 87] 
.................................linked list and recursive calls
class LL_node():
	def __init__(self, d=None, l=None):
		self.data=d
		self.link=l
	def append(self, n):
		N=self
		node=LL_node(n)
		while N.link: 
			N=N.link
		N.link=node
n1=LL_node(1)
n1.append(3)
n1.append(2)
n1.append(5)
n1.append(3)
n1.append(9)
n1.append(1)
n1.append(8)
n1.append(8)
n1.append(7)
N=n1
while N:
	print N.data,
	N=N.link
print
def nth_last_ele_recu(nth,N,c=1):
	if N.link == None: return  c #2. until reach the end, always return c, cause c need to add up
	c=nth_last_ele_recu(nth,N.link,c)#1. all rounds stop here, keep c's value for next round
	c+=1
	if c == nth: 
		print N.data, c # 4.
		return c # do not return N or N.data, cause need N to automatically roll back 
	return c #3.
nth_last_ele_recu(2,n1)

# execution sequence, 1,1,1..., reach the end, return c to line 1., then roll
# back start, till reach 3, then, back to 1 with the new c value, when c==nth,
# get the value N.data, then the roll back continue till reach the head, from
# 1 run to 3, and finish
############################################
in-order successor

In Binary Search Tree, Inorder Successor of an input node can also be defined as the node with the smallest key greater than the key of input node. 

Method 1 (Uses Parent Pointer) 
In this method, we assume that every node has parent pointer.

The Algorithm is divided into two cases on the basis of right subtree of the input node being empty or not.

Input: node, root // node is the node whose Inorder successor is needed.
output: succ // succ is Inorder successor of node.

1) If right subtree of node is not NULL, then succ lies in right subtree. Do following.  Go to right subtree and return the node with minimum key value in right subtree.

2) If right sbtree of node is NULL, then succ is one of the ancestors. Do following.  Travel up using the parent pointer until you see a node which is left child of it’s parent. The parent of such a node is the succ.


Method 2 (Search from root) 
Parent pointer is NOT needed in this algorithm. The Algorithm is divided into
two cases on the basis of right subtree of the input node being empty or not.

Input: node, root // node is the node whose Inorder successor is needed.  output: succ // succ is Inorder successor of node.

1) If right subtree of node is not NULL, then succ lies in right subtree. Do following.  Go to right subtree and return the node with minimum key value in right subtree.

2) If right sbtree of node is NULL, then start from root and us search like technique. Do following.  Travel down the tree, if a node’s data is greater than root’s data then go right side, otherwise go to left side.
############################################
To determin a system is big endian or little endian
>>> import sys
>>> sys.byteorder
'little'
>>> sys.maxint
2147483647

############################################
json pretty print
curl -u 'zhibwang@cisco.com:mar999%!&' https://us1.rallydev.com/slm/webservice/v2.0/defect/|python -m json.tool

