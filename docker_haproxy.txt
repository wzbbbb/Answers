To install:
apt-get install docker.io

To list all docker containers
docker ps -a

docker ps -h

To launch a redis server with 1G memeory
docker run -d -m 1g redis	----

To ssh in Boot2Docker tinycore linux:
localhost:2022

To access an running container:
docker attach 	----

To access logs:
docker logs -f			----

To list process in docker:
docker top  container_name 		----

To save a container into an image, -p is to pause the container during commit
docker commit -p de232737c619 docker_dev

To build an openstack_cli container: -

create a Dockerfile: 		--
FROM ubuntu
MAINTAINER  ZHI BING WANG

RUN apt-get update && \
 apt-get install -y python-keystoneclient && \
 apt-get install -y python-swiftclient && \
 apt-get install -y python-novaclient && \
 apt-get install -y python-troveclient && \
 apt-get install -y python-glanceclient && \
 apt-get install -y python-cinderclient && \
 apt-get install -y python-heatclient && \
 apt-get install -y python-ceilometerclient && \
 apt-get install -y python-neutronclient

docker build -t openstack_cli .					----

To create a databox container:  -
FROM busybox
RUM mkdir /data
VOLUME /data

To launch the databox with a directory from host:
 docker run -it --rm -v /home:/mnt databox sh								----

To access the databox from another container:
 docker run -it --rm --volumes-from 7a69a35b0493 --name userbox busybox sh				----


Container are store in : /var/lib/docker/containers:

To bind a particular interface:
docker run -d -p 127.0.0.1:80:80 --name static_web jamtur01/static_web  nginx -g "daemon off;"		----		----		----		----

To a random port
docker run -d -p 127.0.0.1::80 --name static_web jamtur01/static_web  nginx -g "daemon off;" 	----

Bypassing the Dockerfile build cache with --no-cache
docker builds images in layers, if the previous layers are the say, docker will use cache
docker build --no-cache -t="jamtur01/static_web" . 			----

In Dockerfile, RUN use by default:
/bin/sh -c
To avoid using shell
RUN [ "apt-get", "install", "-y", "nginx"  ]		----


Bind UDP ports by adding the suffix /udp to the port binding. ---

-P, that allows us to expose all ports we've specified via EXPOSE instructions in our Dockerfile.	---
docker run -d -P --name static_web jamtur01/static_web  nginx -g "daemon off;" 		----

CMD to specify command to run after the container launched.	---
CMD ["/bin/bash", "-l"]		----
"docker run" will override the CMD in Dockerfile


You can only specify one CMD instruction in a Dockerfile. If more than one is specified, then the last CMD instruction will be used. If you need to run multiple processes or commands as part of starting a container you should use a service management tool like Supervisor.

Entrypoint:
With ENTRYPOINT, all arguments specified in docker run will be passed as argument for ENTRYPOINT
ENTRYPOINT ["/usr/sbin/nginx"]			----


This allows us to build in a default command to execute when our container is run combined with overridable options and flags on the docker run command line.
ENTRYPOINT ["/usr/sbin/nginx"]
CMD ["-h"]
If not argument specified in docker run, it will display help.

If required at runtime, you can override the ENTRYPOINT instruction using the docker run command with "--entrypoint" flag.

WORKDIR: specify directy to run a command 			---

override with docker run -w
docker run -ti -w /var/log ubuntu pwd		----


ENV: to set environment variable:
ENV RVM_PATH /home/rvm/
or -e ---
 docker run -ti -e "WEB_PORT=8080" ubuntu env	----


USER nginx in Dockerfile
or -u in docker run---
Defaule user is root

VOLUME
• Volumes can be shared and reused between containers.
• A container doesn't have to be running to share its volumes.
• Changes to a volume are made directly.
• Changes to a volume will not be included when you update an image.
• Volumes persist until no containers use them.

Specify 2 volumes in a container
VOLUME ["/opt/project", "/data" ]			----

ADD
The ADD instruction adds files and directories from our build environment into our image. The files have to be in the build directory
ADD will automatically unpack tar files.
if the destination doesn't exist, Docker will create the full path for us, including any directories. New files and directories will be created with a mode of 0755 and a UID and GID of 0.

ADD software.lic /opt/application/software.lic			----

COPY
Same as ADD, just without extraction and decompression 

ONBUILD
The ONBUILD instruction adds triggers to images. A trigger is executed when the image is used as the basis of another image

Running a registry from a container -
docker run -p 5000:5000 registry			----
find the container ID
docker images jamtur01/static_web			----
tag to the new registry
docker tag 22d47c8cb6e5 docker.example.com:5000/jamtur01/static_web		----
push to the new registry
docker push docker.example.com:5000/jamtur01/static_web				----

building container from local registry
docker run -t -i docker.example.com:5000/jamtur01/static_web /bin/bash		----

................................. -
To create samba share with docker
build databox with busybox:
...................... the Dockerfile
FROM busybox
RUM mkdir /data
VOLUME /data
......................

docker build -t databox . 		----
Then launch databox
docker run -it -v /home/data:/data databox sh ----
or without a directory from the host OS:
docker run databox  ----

docker run --rm -v $(which docker):/docker -v /var/run/docker.sock:/docker.sock svendowideit/samba da87db80f7eb 	----
Then the command will output how to mount the volume from Windows!
............................................ 	-
docker HAProxy 

git clone https://github.com/dockerfile/haproxy.git 		----
update haproxy.cfg						----
docker run -d -p 80:80 -v /home/ubuntu/dockerfile/haproxy:/haproxy-override dockerfile/haproxy 			----


 cat haproxy.cfg
global
  log 127.0.0.1 local0
  log 127.0.0.1 local1 notice
  maxconn 4000
  chroot /var/lib/haproxy
  user haproxy
  group haproxy
  # daemon

defaults
  log global
  mode http
  option httplog
  option dontlognull
  contimeout 5000
  clitimeout 50000
  srvtimeout 50000
  errorfile 400 /etc/haproxy/errors/400.http
  errorfile 403 /etc/haproxy/errors/403.http
  errorfile 408 /etc/haproxy/errors/408.http
  errorfile 500 /etc/haproxy/errors/500.http
  errorfile 502 /etc/haproxy/errors/502.http
  errorfile 503 /etc/haproxy/errors/503.http
  errorfile 504 /etc/haproxy/errors/504.http
  stats enable
  stats refresh 10s
  stats uri /stats

frontend http-in
    bind *:80
    option httpclose
    option forwardfor
    mode http
    default_backend http_server

backend http_server
  mode http
  balance roundrobin
  cookie SERVERID insert indirect nocache
  #server s1 10.100.0.10:80  check cookie s1
  #server s2 10.100.0.9:80  check cookie s2
  server s1 10.100.0.10:80  check
  server s2 10.100.0.9:80  check


#listen stats :80
#  stats enable
#  stats uri /stats

For SSL termination --
docker run -d -p 443:443 -v /home/ubuntu/dockerfile/haproxy:/haproxy-override dockerfile/haproxy 	----

only change the frontend:

frontend http-in
    bind *:443 ssl crt /haproxy-override/sever.pem
    mode http
    option httpclose
    option forwardfor
    reqadd X-Forwarded-Proto:\ https
    default_backend http_server

Pacemaker + CoroSync -

Pacemaker + CoroSync: for resource cluster, if a serice is down, start it on another node--
keeyalived plays with VIP, if one node is down, assign the IP to anther node --

http://www.sebastien-han.fr/blog/2012/04/15/active-passive-failover-cluster-on-a-mysql-galera-cluster-with-haproxy-lsb-agent/

after starting corosync and pacemaker:
# crm_mon -1
Last updated: Mon Sep 15 20:27:59 2014
Last change: Mon Sep 15 20:27:55 2014 via crmd on ub3
Stack: corosync
Current DC: ub3 (174325771) - partition with quorum
Version: 1.1.10-42f2063
1 Nodes configured
0 Resources configured


Online: [ ub3 ]

root@ub3:/var/log#


.................................
https://gist.github.com/ianunruh/2162add34d1c55e387b9
#!/bin/bash
BIND_NETWORK="192.168.5.0"
SHARED_VIP="192.168.5.30"
 
apt-get update			----
apt-get install -y pacemaker ntp		----
 
# Configure Corosync
echo "START=yes" > /etc/default/corosync
sed -i "s/bindnetaddr: 127.0.0.1/bindnetaddr: $BIND_NETWORK/g" /etc/corosync/corosync.conf
 
# Start clustering software
service corosync start
service pacemaker start
 
crm configure <<EOF
  primitive virtual-ip ocf:heartbeat:IPaddr \
    params ip="$SHARED_VIP"
 
  property stonith-enabled=false
  commit
EOF
.................................
Redundant HAProxy , keepalived-
http://thejimmahknows.com/high-availability-using-haproxy-and-keepalived/		---

This one also include HA setting for other openstack components
http://behindtheracks.com/2014/04/redundant-load-balancers-haproxy-and-keepalived/ 	---

configure and start HAPorxy on both nodes --

keepalived.conf	---
............................................
MASTER
............................................
global_defs {
  router_id haproxy1 		#hostname
}
vrrp_script haproxy {
  script "killall -0 haproxy" 	#make sure haproxy is running
  interval 2 			#check every 2 seconds
  weight 2			#add weight if OK
}
vrrp_instance 50 {
  virtual_router_id 50
  advert_int 1
  priority 101			#priority higher than BACKUP 
  state MASTER
  interface eth0
  virtual_ipaddress {
    10.100.0.200 dev eth0  # virutal IP
  }
  track_script {
    haproxy
  }
}

............................................
BACKUP
............................................
global_defs {
  router_id haproxy2            #hostname
}
vrrp_script haproxy {
  script "killall -0 haproxy"   #make sure haproxy is running
  interval 2                    #check every 2 seconds
  weight 2                      #add weight if OK
}
vrrp_instance 50 {
  virtual_router_id 50
  advert_int 1
  priority 100                  #priority lower than MASTER
  state BACKUP 
  interface eth0
  virtual_ipaddress {
    10.100.0.200 dev eth0  # virutal IP
  }
  track_script {
    haproxy
  }
}

............................................
Config HA for HAProxy		-

install HAProxy on 2 servers 	--
install keepalived on 2 servers	--
In /etc/sysctl.conf of both server		--
net.ipv4.ip_nonlocal_bind=1

sysctl -p

create neutron port for VIP --
neutron port-create --fixed-ip ip_address=${vip_ip} --security-group $secgroup-vrrp $net_id 

assign a floating IP --
neutron floatingip-create --port-id=$(echo $vip_port) public-floating-601

update neutron ports on both haproxy nodes to allow VIP and vrrp mulitcast IP in 	--
neutron port-update $server1_port --allowed-address-pairs type=dict list=true ip_address=${vip_ip} ip_address=224.0.0.18 		
neutron port-update $server2_port --allowed-address-pairs type=dict list=true ip_address=${vip_ip} ip_address=224.0.0.18

To verify --
check vip is on which server, should NOT be on both server	---
ip a		----
check for vrrp multicast ---
tcpdump -v -i eth0 host 224.0.0.18		---
.................................
To install HAProxy with SSL compiled in:
add-apt-repository ppa:vbernat/haproxy-1.5
apt-get update
apt-get install haproxy

The defail haproxy in ubuntu does not include SSL. If starting haproxy with SSL configured in the /etc/haproxy/haproxy.cfg, it will complain:
[ALERT] 280/190040 (21687) : parsing [/etc/haproxy/haproxy.cfg:30] : 'bind' only supports the 'transparent', 'defer-accept', 'name', 'id', 'mss' and 'interface' options.
.................................
cat fig.yml
nginx:
  build: ./nginx/.
  #image: zhibing/nginx
  ports:
   - "80:80"
  links:
   - uwsgi
uwsgi:
  build: ./flask_uwsgi/.
  #image: zhibing/flask_uwsgi
  ports:
   - "3031:3031"
  volumes:
   - .:/src

To launch it:
fig up -d			----
fig logs uwsgi 		----
fig ps 			----

........... more fig.yml syntax
environment:
  DJANGO_SETTINGS_MODULE 'webapp_settings'
volumes_from:
  - another_node
links:
  - database
  - redis
  - search
